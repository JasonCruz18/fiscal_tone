{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b952faed",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"background:#634C44; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22631b14",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497e7d2",
   "metadata": {},
   "source": [
    "> **Author:** Jason Cruz  \n",
    "  **Last updated:** 11/16/2025  \n",
    "  **Python version:** 3.12  \n",
    "  **Project:** Fiscal Tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916311",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Summary\n",
    "Welcome to the **Fiscal Tone** Text Preprocessing notebook! This notebook will guide you through the **step-by-step process** of \n",
    "\n",
    "\n",
    "### What will this notebook help you achieve?\n",
    "1. **Downloading PDFs** from the BCRP Weekly Reports (WR).\n",
    "2. **Generating PDF inputs** by shortening them to focus on key pages containing GDP growth rate tables.\n",
    "3. **Cleaning-up extracted data** to ensure it's usable and building RTD.\n",
    "4. **Concatenating RTD** from different years and frequencies (monthly, quarterly, annual).\n",
    "5. **Updating metadata** for storing base years changes and other revisions-based information.\n",
    "6. **Converting RTD** to releases dataset for econometric analysis.\n",
    "\n",
    "üåê **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (üì∞ WR, from here on)  \n",
    "For any questions or issues, feel free to reach out via email: [Jason üì®](mailto:jj.cruza@up.edu.pe)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a909438",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### ‚öôÔ∏è Initial Set-up\n",
    "\n",
    "Before preprocessing the new GDP releases data, we need to perform some initial set-up steps:\n",
    "\n",
    "1. üß∞ **Import helper functions** from `gdp_rtd_pipeline.py` that are required for this notebook.\n",
    "2. üõ¢Ô∏è **Connect to the PostgreSQL database** that will contain GDP revisions datasets. _(This step is pending: direct access will be provided via ODBC or other methods, allowing users to connect from any software or programming language.)_\n",
    "3. üìÇ **Create necessary folders** to store inputs, outputs, logs, and screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f754c24",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "#from cf_mef_functions.py import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f17343",
   "metadata": {},
   "source": [
    "> üöß Although the second step (database connection) is pending, the notebook currently works using **flat files (CSV)**. These CSV files will **not be saved in GitHub** as they are included in the `.gitignore` to ensure no data is stored publicly. Users can be confident that no data will be stored on GitHub. The notebook **automatically generates the CSV files**, giving users direct access to the dataset on their own systems. The data is created on the fly and can be saved locally for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8c42b",
   "metadata": {},
   "source": [
    "### üß∞ Import helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d50a",
   "metadata": {},
   "source": [
    "This notebook relies on a set of helper functions found in the script `gdp_rtd_pipeline.py`. These functions will be used throughout the notebook, so please ensure you have them ready by running the line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6199",
   "metadata": {},
   "source": [
    "> üõ†Ô∏è **Libraries:** Before you begin, please ensure that you have the required libraries installed and imported. See all the libraries you need section by section in `gdp_rtd_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660bfd8",
   "metadata": {},
   "source": [
    "**Check out Python information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addf904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Information\n",
      "  Version  : 3.12.11\n",
      "  Compiler : MSC v.1929 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jun  5 2025 12:58:53')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"üêç Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ace86",
   "metadata": {},
   "source": [
    "### üìÇ Create necessary folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb2655",
   "metadata": {},
   "source": [
    "We will start by creating the necessary folders to store the data at various stages of processing. The following code ensures all required directories exist, and if not, it creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6916c1-e725-4eba-8199-4aa480f7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a78251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\Policy Brief\\GitHub\\peruvian_policy_brief\n",
      "üìÇ data created\n",
      "üìÇ data\\raw created\n",
      "üìÇ data\\input created\n",
      "üìÇ data\\output created\n",
      "üìÇ metadata created\n",
      "üìÇ record created\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # Importing Path module from pathlib to handle file and directory paths in a cross-platform way.\n",
    "\n",
    "# Get current working directory\n",
    "PROJECT_ROOT = Path.cwd()  # Get the current working directory where the notebook is being executed.\n",
    "\n",
    "# User input for folder location\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"  # Prompt user to input the folder path or use the default value \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()  # Combine the project root directory with user input to get the full target path.\n",
    "\n",
    "# Create the necessary directories if they don't already exist\n",
    "target_path.mkdir(parents=True, exist_ok=True)  # Creates the target folder and any necessary parent directories.\n",
    "print(f\"Using path: {target_path}\")  # Print out the path being used for confirmation.\n",
    "\n",
    "# Define paths for saving data and PDFs\n",
    "data_folder = 'data'  # This folder will store the new Weekly Reports (post-2013), which are in PDF format.\n",
    "raw_data_subfolder = os.path.join(data_folder, 'raw')  # Subfolder for saving the raw PDFs exactly as downloaded from the BCRP website.\n",
    "input_data_subfolder = os.path.join(data_folder, 'input')  # Subfolder for saving reduced PDFs that contain only the selected pages with GDP growth tables.\n",
    "output_data_subfolder = os.path.join(data_folder, 'output')\n",
    "\n",
    "# Additional folders for metadata, records, and alert tracking\n",
    "metadata_folder = 'metadata'  # Folder for storing metadata files like wr_metadata.csv.\n",
    "record_folder = 'record'  # Folder for storing .txt files that track the files already processed to avoid reprocessing them.\n",
    "\n",
    "# Create additional required folders\n",
    "for folder in [data_folder, raw_data_subfolder, input_data_subfolder, output_data_subfolder, metadata_folder, record_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create the additional folders if they don't exist.\n",
    "    print(f\"üìÇ {folder} created\")  # Print confirmation for each of these additional folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc9c1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1f74",
   "metadata": {},
   "source": [
    "# 1. BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75589e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18cb1ce4",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "import re\n",
    "from time import time as timer\n",
    "\n",
    "\n",
    "# === üß† UTILITIES ===\n",
    "\n",
    "def clean_filename(s):\n",
    "    \"\"\"\n",
    "    Sanitize a string to make it safe for filenames:\n",
    "    - Removes non-alphanumeric symbols (except underscores and hyphens)\n",
    "    - Trims and replaces spaces with underscores\n",
    "\n",
    "    Parameters:\n",
    "        s (str): Original string.\n",
    "\n",
    "    Returns:\n",
    "        str: Safe filename string.\n",
    "    \"\"\"\n",
    "    s = re.sub(r'[^\\w\\s-]', '', s)\n",
    "    s = re.sub(r'\\s+', '_', s.strip())\n",
    "    return s[:100]  # Truncate to 100 characters max\n",
    "\n",
    "\n",
    "# === üåê SCRAPE CF WEBSITE ===\n",
    "\n",
    "def scrape_cf(url):\n",
    "    \"\"\"\n",
    "    Helper function to scrape document metadata from the Consejo Fiscal (CF) website.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL of the CF page containing document links.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing metadata of documents.\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    rows = soup.select('table.table tbody tr')\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            date = row.select_one('td.size100 p').text.strip()\n",
    "            link_tag = row.select_one('td a')\n",
    "            title = link_tag.text.strip()\n",
    "            doc_url = link_tag['href']\n",
    "\n",
    "            short_name = clean_filename(title) + \".pdf\"\n",
    "\n",
    "            data.append({\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'pdf_filename': short_name,\n",
    "                'url': doc_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error processing row:\", e)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"‚åõ scrape_cf executed in {t1 - t0:.2f} seconds.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# === üì• DOWNLOAD CF PDFs (MAIN SCRIPT) ===\n",
    "\n",
    "def download_cf_pdfs(cf_urls, raw_pdf_folder, download_record_folder, download_record_txt):\n",
    "    \"\"\"\n",
    "    Downloads PDF documents from the Consejo Fiscal website using metadata from scraping.\n",
    "    Downloads both 'informes' and 'comunicados' at the same time and stores URLs in a unified CSV.\n",
    "\n",
    "    Args:\n",
    "        cf_urls (list): List of URLs for the reports and announcements sections.\n",
    "        raw_pdf_folder (str): Folder to store downloaded PDFs.\n",
    "        download_record_folder (str): Folder to store the record text file.\n",
    "        download_record_txt (str): Text file to track the downloaded PDFs.\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "\n",
    "    # Ensure folders exist\n",
    "    os.makedirs(raw_pdf_folder, exist_ok=True)\n",
    "    os.makedirs(download_record_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize headers for requests\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "        'Accept': 'application/pdf',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "\n",
    "    # Load the already downloaded records from the text file\n",
    "    record_path = os.path.join(download_record_folder, download_record_txt)\n",
    "    downloaded_files = set()\n",
    "    if os.path.exists(record_path):\n",
    "        with open(record_path, 'r', encoding='utf-8') as f:\n",
    "            downloaded_files = set(f.read().splitlines())\n",
    "\n",
    "    # Collect all document metadata from both 'informes' and 'comunicados'\n",
    "    all_data = []\n",
    "    for url in cf_urls:\n",
    "        print(f\"üåê Scraping documents from {url}\")\n",
    "        data = scrape_cf(url)\n",
    "        all_data.extend(data)\n",
    "\n",
    "    # Reverse the order to start from the oldest first (we assume the list is from newest to oldest)\n",
    "    all_data.reverse()\n",
    "\n",
    "    # Convert scraped data to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Process each document for downloading\n",
    "    errors = []\n",
    "    skipped_files = []  # List of skipped files\n",
    "    new_counter = 0  # Count of new files downloaded\n",
    "    for i, row in df.iterrows():\n",
    "        # Check if file has already been downloaded\n",
    "        if row['pdf_filename'] in downloaded_files:\n",
    "            skipped_files.append(row['pdf_filename'])\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{i + 1}/{len(df)}] Processing: {row['title']}\")\n",
    "        try:\n",
    "            # Try to fetch the document\n",
    "            response = requests.get(row['url'], headers=headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            pdf_url = None\n",
    "\n",
    "            # 1. Try <a> with .pdf\n",
    "            pdf_link_tag = soup.find('a', href=lambda x: x and '.pdf' in x.lower())\n",
    "            if pdf_link_tag:\n",
    "                temp_pdf_url = pdf_link_tag['href']\n",
    "                try:\n",
    "                    head_response = requests.head(temp_pdf_url, headers=headers, allow_redirects=True, timeout=10)\n",
    "                    if head_response.status_code == 200:\n",
    "                        pdf_url = temp_pdf_url\n",
    "                        print(\"üîó Found working <a> link.\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è <a> link returned {head_response.status_code}. Trying <iframe>.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error validating <a> link: {e}. Trying <iframe>.\")\n",
    "\n",
    "            # 2. If <a> fails, try <iframe>\n",
    "            if not pdf_url:\n",
    "                iframe_tag = soup.find('iframe', src=lambda x: x and ('.pdf' in x.lower() or 'docs.google.com/viewer' in x.lower()))\n",
    "                if iframe_tag:\n",
    "                    iframe_src = iframe_tag['src']\n",
    "                    if iframe_src.startswith('//'):\n",
    "                        iframe_src = 'https:' + iframe_src\n",
    "\n",
    "                    if 'docs.google.com/viewer' in iframe_src:\n",
    "                        parsed = urlparse(iframe_src)\n",
    "                        query = parse_qs(parsed.query)\n",
    "                        if 'url' in query:\n",
    "                            pdf_url = unquote(query['url'][0])\n",
    "                            print(\"üîó Extracted PDF URL from Google Docs Viewer.\")\n",
    "                        else:\n",
    "                            print(\"‚ö†Ô∏è Google Docs Viewer iframe found but no 'url' parameter.\")\n",
    "                    else:\n",
    "                        pdf_url = iframe_src\n",
    "                        print(\"üîó Found PDF in <iframe>.\")\n",
    "\n",
    "            # 3. Download the PDF (with retry logic)\n",
    "            if pdf_url:\n",
    "                df.at[i, 'final_pdf_url'] = pdf_url\n",
    "                filename = row['pdf_filename']\n",
    "                filepath = os.path.join(raw_pdf_folder, filename)\n",
    "\n",
    "                # Retry download up to 3 times\n",
    "                for attempt in range(3):  # Retry up to 3 times\n",
    "                    try:\n",
    "                        pdf_response = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            f.write(pdf_response.content)\n",
    "                        print(f\"‚úÖ Saved: {filename}\")\n",
    "                        downloaded_files.add(filename)  # Add to the set of downloaded files\n",
    "\n",
    "                        # Immediately record the filename in the text file\n",
    "                        with open(record_path, 'a', encoding='utf-8') as f:\n",
    "                            f.write(f\"{filename}\\n\")\n",
    "\n",
    "                        new_counter += 1  # Increment newly downloaded counter\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Attempt {attempt+1}/3 failed to download {pdf_url}: {e}\")\n",
    "                        if attempt < 2:\n",
    "                            time.sleep(2)\n",
    "                        else:\n",
    "                            raise e\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è PDF not found on page: {row['url']}\")\n",
    "                errors.append(row['url'])\n",
    "\n",
    "                debug_path = f'debug_{i+1}.html'\n",
    "                with open(debug_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(soup.prettify())\n",
    "                print(f\"üìù Saved HTML for debugging: {debug_path}\")\n",
    "\n",
    "            time.sleep(1)  # polite pause between downloads\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {row['url']}: {e}\")\n",
    "            errors.append(row['url'])\n",
    "\n",
    "    # === SAVE UPDATED CSV ===\n",
    "    df.to_csv('cf_documents.csv', index=False)\n",
    "    print(f\"\\nüìù Updated CSV saved with final_pdf_url column: cf_documents.csv\")\n",
    "\n",
    "    # === FINAL SUMMARY ===\n",
    "    elapsed_time = round(time.time() - t0)  # Seconds elapsed\n",
    "    total_links = len(df)  # Total links processed\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"\\nüîó Total links processed: {total_links}\")\n",
    "    if skipped_files:\n",
    "        print(f\"üóÇÔ∏è {len(skipped_files)} already downloaded PDFs were skipped.\")\n",
    "    print(f\"‚ûï Newly downloaded: {new_counter}\")\n",
    "    print(f\"‚è±Ô∏è {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786a386c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Scraping documents from https://cf.gob.pe/p/informes/\n",
      "‚åõ scrape_cf executed in 0.76 seconds.\n",
      "üåê Scraping documents from https://cf.gob.pe/p/comunicados/\n",
      "‚åõ scrape_cf executed in 0.79 seconds.\n",
      "\n",
      "[28/79] Processing: Comunicado N¬∞ 06-2025-CF ‚Äì El Consejo Fiscal alerta sobre los crecientes riesgos por la proliferaci√≥n de leyes con impacto fiscal adverso e insta a revertir con urgencia esta situaci√≥n\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Comunicado_N_06-2025-CF_El_Consejo_Fiscal_alerta_sobre_los_crecientes_riesgos_por_la_proliferaci√≥n_d.pdf\n",
      "\n",
      "[71/79] Processing: Informe N¬∞ 01-2024-CF. Opini√≥n del Consejo Fiscal sobre el Informe de Actualizaci√≥n de Proyecciones Macroecon√≥micas 2024-2027\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_01-2024-CF_Opini√≥n_del_Consejo_Fiscal_sobre_el_Informe_de_Actualizaci√≥n_de_Proyecciones_Ma.pdf\n",
      "\n",
      "[72/79] Processing: Informe N¬∞ 02-2024-CF ‚Äì Opini√≥n del Consejo Fiscal acerca de la evoluci√≥n de las finanzas p√∫blicas y del  cumplimiento de las reglas fiscales subnacionales en 2023\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_02-2024-CF_Opini√≥n_del_Consejo_Fiscal_acerca_de_la_evoluci√≥n_de_las_finanzas_p√∫blicas_y_de.pdf\n",
      "\n",
      "[73/79] Processing: Informe N¬∞ 03-2024-CF ‚Äì Opini√≥n del Consejo Fiscal acerca de la evoluci√≥n de las finanzas p√∫blicas y del  cumplimiento de las reglas macrofiscales durante el ejercicio fiscal 2023\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_03-2024-CF_Opini√≥n_del_Consejo_Fiscal_acerca_de_la_evoluci√≥n_de_las_finanzas_p√∫blicas_y_de.pdf\n",
      "\n",
      "[74/79] Processing: Informe N¬∞ 04-2024-CF ‚Äì Opini√≥n del Consejo Fiscal acerca de la modificaci√≥n de las reglas fiscales  dispuesta por el Decreto Legislativo N¬∞ 1621\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_04-2024-CF_Opini√≥n_del_Consejo_Fiscal_acerca_de_la_modificaci√≥n_de_las_reglas_fiscales_dis.pdf\n",
      "\n",
      "[75/79] Processing: Informe N¬∞ 05-2024-CF ‚Äì Opini√≥n del Consejo Fiscal sobre el proyecto de Marco Macroecon√≥mico  Multianual 2025-2028\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_05-2024-CF_Opini√≥n_del_Consejo_Fiscal_sobre_el_proyecto_de_Marco_Macroecon√≥mico_Multianual.pdf\n",
      "\n",
      "[76/79] Processing: Informe N¬∞ 01-2025-CF ‚Äì Opini√≥n del Consejo Fiscal sobre el Informe de Actualizaci√≥n de Proyecciones Macroecon√≥micas 2025-2028\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_01-2025-CF_Opini√≥n_del_Consejo_Fiscal_sobre_el_Informe_de_Actualizaci√≥n_de_Proyecciones_Ma.pdf\n",
      "\n",
      "[77/79] Processing: Informe N¬∞ 02-2025-CF ‚Äì Opini√≥n del Consejo Fiscal sobre la situaci√≥n de las finanzas p√∫blicas subnacionales\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_02-2025-CF_Opini√≥n_del_Consejo_Fiscal_sobre_la_situaci√≥n_de_las_finanzas_p√∫blicas_subnacio.pdf\n",
      "\n",
      "[78/79] Processing: Informe N¬∞ 03-2025-CF ‚Äì Opini√≥n del Consejo Fiscal sobre la evoluci√≥n de las finanzas p√∫blicas y el  cumplimiento de las reglas macrofiscales en el ejercicio fiscal 2024\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_03-2025-CF_Opini√≥n_del_Consejo_Fiscal_sobre_la_evoluci√≥n_de_las_finanzas_p√∫blicas_y_el_cum.pdf\n",
      "\n",
      "[79/79] Processing: Informe N¬∞ 04-2025-CF ‚Äì Opini√≥n del Consejo Fiscal sobre el proyecto de Marco Macroecon√≥mico Multianual 2026-2029\n",
      "üîó Found working <a> link.\n",
      "‚úÖ Saved: Informe_N_04-2025-CF_Opini√≥n_del_Consejo_Fiscal_sobre_el_proyecto_de_Marco_Macroecon√≥mico_Multianual.pdf\n",
      "\n",
      "üìù Updated CSV saved with final_pdf_url column: cf_documents.csv\n",
      "\n",
      "üìä Summary:\n",
      "\n",
      "üîó Total links processed: 79\n",
      "üóÇÔ∏è 69 already downloaded PDFs were skipped.\n",
      "‚ûï Newly downloaded: 10\n",
      "‚è±Ô∏è 33 seconds\n"
     ]
    }
   ],
   "source": [
    "# URLs for both informes and comunicados\n",
    "cf_urls = ['https://cf.gob.pe/p/informes/', 'https://cf.gob.pe/p/comunicados/']\n",
    "\n",
    "# Specify folder paths\n",
    "raw_pdf_folder = raw_data_subfolder  # Replace with actual folder\n",
    "download_record_folder = record_folder  # Replace with actual folder\n",
    "download_record_txt = 'downloaded_pdfs.txt'\n",
    "\n",
    "# Run the PDF downloader\n",
    "download_cf_pdfs(\n",
    "    cf_urls=cf_urls,\n",
    "    raw_pdf_folder=raw_pdf_folder,\n",
    "    download_record_folder=download_record_folder,\n",
    "    download_record_txt=download_record_txt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c597d21",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Delete A/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254f2a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Ruta a la carpeta de reports\n",
    "reports_folder = \"pdf_reports\"\n",
    "\n",
    "# Lista de archivos a eliminar\n",
    "pdfs_to_remove = [\n",
    "    \"Informe_N_001-2019-CF.pdf\",\n",
    "    \"Informe_N_002-2018.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafdf3a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_pdfs(folder_path, filenames_to_remove):\n",
    "    \"\"\"\n",
    "    Deletes specific unwanted PDF files from a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, the directory containing the PDFs\n",
    "    - filenames_to_remove: list of str, filenames to delete\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    removed_count = 0\n",
    "\n",
    "    print(f\"üóëÔ∏è Starting cleanup in: {folder_path}\")\n",
    "    for filename in filenames_to_remove:\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            os.remove(full_path)\n",
    "            print(f\"‚úÖ Deleted: {filename}\")\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\nüßπ Cleanup complete. Total files removed: {removed_count}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca1ebf",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "remove_unwanted_pdfs(reports_folder, pdfs_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986531c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10799107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2433e30b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Clasificando entre scanned and editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a037",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Please, compruebe que la carpeta \"\" contenga scaneados y que la carpeta \"\" ediatables. En caso de que no se haya clasificado correctamente los pdfs, agreguelos manualemnte. Esto es vital para los pr√≥ximos c√≥digos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ef1b9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF, used to extract text from PDFs\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def is_editable_pdf(file_path, min_text_length=20):\n",
    "    \"\"\"\n",
    "    Checks whether a PDF contains extractable text (i.e., is editable).\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the PDF file\n",
    "    - min_text_length: int, minimum number of characters required to be considered editable\n",
    "\n",
    "    Returns:\n",
    "    - True if editable, False if likely scanned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            total_text = \"\".join(page.get_text() for page in doc)\n",
    "        return len(total_text.strip()) >= min_text_length\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def classify_pdfs_by_type(source_dirs, output_dir_scanned=\"scanned_pdf\", output_dir_editable=\"editable_pdf\"):\n",
    "    \"\"\"\n",
    "    Classifies PDF files from given directories into 'editable' and 'scanned' folders.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dirs: list of str, folders containing PDF files\n",
    "    - output_dir_scanned: str, target directory for scanned PDFs\n",
    "    - output_dir_editable: str, target directory for editable PDFs\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    os.makedirs(output_dir_scanned, exist_ok=True)\n",
    "    os.makedirs(output_dir_editable, exist_ok=True)\n",
    "\n",
    "    total_files = 0\n",
    "    scanned_count = 0\n",
    "    editable_count = 0\n",
    "\n",
    "    print(\"üîç Starting PDF classification...\\n\")\n",
    "\n",
    "    for folder in source_dirs:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                total_files += 1\n",
    "                pdf_path = os.path.join(folder, filename)\n",
    "\n",
    "                if is_editable_pdf(pdf_path):\n",
    "                    shutil.copy(pdf_path, os.path.join(output_dir_editable, filename))\n",
    "                    editable_count += 1\n",
    "                    print(f\"üíª Editable: {filename}\")\n",
    "                else:\n",
    "                    shutil.copy(pdf_path, os.path.join(output_dir_scanned, filename))\n",
    "                    scanned_count += 1\n",
    "                    print(f\"üñ®Ô∏è Scanned: {filename}\")\n",
    "\n",
    "    t1 = timer()\n",
    "    print(\"\\n‚úÖ Classification complete!\")\n",
    "    print(f\"üìÑ Total PDFs processed: {total_files}\")\n",
    "    print(f\"üíª Editable PDFs: {editable_count}\")\n",
    "    print(f\"üñ®Ô∏è Scanned PDFs: {scanned_count}\")\n",
    "    print(f\"üìÅ Saved editable PDFs in: '{output_dir_editable}'\")\n",
    "    print(f\"üìÅ Saved scanned PDFs in:  '{output_dir_scanned}'\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d2041",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "source_folders = [\"pdf_announcements\", \"pdf_reports\"]\n",
    "classify_pdfs_by_type(source_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dd08d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-2\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.2 Metadata\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c308e",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c27811",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def combine_metadata(csv_reports=\"cf_reports.csv\", csv_announcements=\"cf_announcements.csv\", output_csv=\"cf_pdfs_metadata.csv\"):\n",
    "    \"\"\"\n",
    "    Combines metadata from two separate CSV files (reports and announcements) into one.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_reports: str, path to the reports metadata CSV\n",
    "    - csv_announcements: str, path to the announcements metadata CSV\n",
    "    - output_csv: str, path for the combined output CSV\n",
    "\n",
    "    Saves:\n",
    "    - A single CSV file containing all metadata under the name specified in output_csv\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    print(\"üì• Loading metadata...\")\n",
    "\n",
    "    df_reports = pd.read_csv(csv_reports)\n",
    "    df_announcements = pd.read_csv(csv_announcements)\n",
    "\n",
    "    df_combined = pd.concat([df_reports, df_announcements], ignore_index=True)\n",
    "    df_combined.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"‚úÖ Combined metadata saved as '{output_csv}'\")\n",
    "    print(f\"üßæ Total records: {len(df_combined)}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ba3d9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "combine_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293849d3",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-3\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.3 Split into paragraphs\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17329117",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Editable PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b607c22",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def process_editable_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", output_csv=\"raw_editable_pdfs_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Processes editable PDF documents, extracting structured paragraphs and saving to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, path to the folder containing editable PDFs\n",
    "    - metadata_csv_path: str, path to the CSV file with document metadata\n",
    "    - output_csv: str, output CSV filename to save extracted paragraphs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted structured text and metadata\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # --- Extract doc_type, doc_id, year from title ---\n",
    "    def extract_doc_info(row):\n",
    "        title = row[\"title\"]\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "\n",
    "    print(\"üß† Metadata enriched. Starting paragraph extraction...\\n\")\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÑ Processing: {idx}. {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                paragraph_counter = 1\n",
    "                anexo_found = False\n",
    "\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    if anexo_found:\n",
    "                        break\n",
    "\n",
    "                    # Extract words with their size and vertical position\n",
    "                    words = page.extract_words(extra_attrs=[\"size\", \"top\"])\n",
    "                    FONT_MIN = 11.0\n",
    "                    FONT_MAX = 11.9\n",
    "                    clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX]\n",
    "                    if not clean_words:\n",
    "                        continue\n",
    "\n",
    "                    # Group words by their vertical position to form lines\n",
    "                    lines_dict = {}\n",
    "                    for word in clean_words:\n",
    "                        line_top = round(word[\"top\"], 1)\n",
    "                        lines_dict.setdefault(line_top, []).append(word[\"text\"])\n",
    "\n",
    "                    lines = [\n",
    "                        \" \".join(words).strip()\n",
    "                        for _, words in sorted(lines_dict.items())\n",
    "                        if words\n",
    "                    ]\n",
    "\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    page_text = \"\\n\".join(lines)\n",
    "\n",
    "                    # üö´ Stop extraction at \"Anexo\"\n",
    "                    match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                    if match:\n",
    "                        page_text = page_text[:match.start()].strip()\n",
    "                        anexo_found = True\n",
    "                        print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                    # Paragraph segmentation\n",
    "                    lines = page_text.strip().split(\"\\n\")\n",
    "                    lines = [line.strip() for line in lines if line.strip()]\n",
    "                    paragraph_lines = []\n",
    "\n",
    "                    for i, line in enumerate(lines):\n",
    "                        is_new_paragraph = (\n",
    "                            line.startswith(\"‚Ä¢\")\n",
    "                            or line.startswith(\"‚û¢\")\n",
    "                            or (i > 0 and lines[i - 1].strip().endswith(\".\"))\n",
    "                            or (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                        )\n",
    "\n",
    "                        if is_new_paragraph:\n",
    "                            if paragraph_lines:\n",
    "                                all_records.append({\n",
    "                                    \"filename\": filename,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"paragraph_id\": paragraph_counter,\n",
    "                                    \"text\": \" \".join(paragraph_lines).strip()\n",
    "                                })\n",
    "                                paragraph_counter += 1\n",
    "                            paragraph_lines = [line]\n",
    "                        else:\n",
    "                            paragraph_lines.append(line)\n",
    "\n",
    "                    if paragraph_lines:\n",
    "                        all_records.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"page\": page_num,\n",
    "                            \"paragraph_id\": paragraph_counter,\n",
    "                            \"text\": \" \".join(paragraph_lines).strip()\n",
    "                        })\n",
    "                        paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    # Merge with metadata\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\n‚úÖ Extraction complete. Total paragraphs: {len(df)}\")\n",
    "    print(f\"üíæ Saved to: {output_csv}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a354e5d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable = process_editable_pdfs(\"editable_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f3936",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbd2f",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af8df4a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd8030",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "\"\"\"\n",
    "Procesa PDFs escaneados usando OCR para extraer p√°rrafos por p√°gina y metadatos.\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Ruta a la carpeta con archivos PDF escaneados.\n",
    "    dpi (int): Resoluci√≥n al convertir PDF a imagen.\n",
    "    lang (str): Idioma para el OCR ('spa' para espa√±ol, 'eng' para ingl√©s, etc.).\n",
    "\n",
    "Returns:\n",
    "    pd.DataFrame: DataFrame con columnas:\n",
    "        ['filename', 'year', 'date', 'announcement', 'page', 'paragraph_id', 'text']\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eec680",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Puede tardar un poco m√°s. Se excluye pies de pagina con detecci√≥n visual de lineas que marcan el inicio. Se p√°ginas de anexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e58f5",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.2, y_range=(0.5, 0.85), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line that likely marks the beginning of the footer in a scanned PDF page.\n",
    "    \n",
    "    Parameters:\n",
    "        image (PIL.Image): Page image to analyze.\n",
    "        min_length_ratio (float): Minimum line length relative to image width.\n",
    "        y_range (tuple): Vertical range (as a proportion of image height) where footer lines are expected.\n",
    "        debug_path (str): Optional path to save debug image with the detected line.\n",
    "    \n",
    "    Returns:\n",
    "        int: Y-coordinate to crop the image above the footer line, or image height if no line is found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=100,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 3 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # Default: no footer line detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c36df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a635301",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Major function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac228",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.17, y_range=(0.55, 0.90), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line likely indicating the beginning of the footer in scanned PDFs.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Page image.\n",
    "        min_length_ratio (float): Minimum length of line relative to image width.\n",
    "        y_range (tuple): Vertical range to search (proportional to height).\n",
    "        debug_path (str): Optional file path to save a debug image with detected line.\n",
    "\n",
    "    Returns:\n",
    "        int: Y-coordinate of the detected line, or image height if none found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=80,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 5 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # No line detected ‚Üí return full height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a24cd2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Processing for Scanned PDFs ===\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from scanned PDFs using OCR, excluding footers and annexes.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Folder with scanned PDFs.\n",
    "        metadata_csv_path (str): Path to metadata CSV file.\n",
    "        dpi (int): Resolution used to convert PDFs to images.\n",
    "        lang (str): OCR language code.\n",
    "        debug (bool): Whether to save debug images with detected lines.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Paragraph-level extracted data.\n",
    "    \"\"\"\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                debug_path = None\n",
    "                if debug:\n",
    "                    os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                    debug_path = f\"debug_lines/{filename}_page_{page_num}.png\"\n",
    "\n",
    "                cut_y = detect_cut_line_y(image, debug_path=debug_path)\n",
    "                cropped_img = image.crop((0, 0, image.width, cut_y))\n",
    "\n",
    "                page_text = pytesseract.image_to_string(cropped_img, lang=lang)\n",
    "\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # Stop at 'Anexo'\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51a39",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === Run and Save ===\n",
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"scanned_pdf\",\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deef5c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93432a87",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# NUEVA UTILIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6963",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def filter_text_body_dynamic(\n",
    "    image,\n",
    "    size_threshold_percentile=40,\n",
    "    merge_dist=20,\n",
    "    expand_margin=10,\n",
    "    debug_path=None,\n",
    "    audit_csv_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Filtra regiones de texto fuera del cuerpo principal del documento mediante detecci√≥n din√°mica.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Imagen de entrada.\n",
    "        size_threshold_percentile (int): Percentil m√≠nimo de altura para considerar caja v√°lida.\n",
    "        merge_dist (int): Distancia m√°xima entre cajas para considerar agrupaci√≥n.\n",
    "        expand_margin (int): Expansi√≥n del marco del cuerpo principal.\n",
    "        debug_path (str): Ruta para guardar imagen de depuraci√≥n.\n",
    "        audit_csv_path (str): Ruta para guardar CSV con info de cajas.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Imagen filtrada.\n",
    "    \"\"\"\n",
    "    img_rgb = np.array(image.convert(\"RGB\"))\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    if not boxes:\n",
    "        return image\n",
    "\n",
    "    heights = [h for (_, _, _, h) in boxes]\n",
    "    height_threshold = np.percentile(heights, size_threshold_percentile)\n",
    "\n",
    "    filtered_boxes = [(x, y, w, h) for (x, y, w, h) in boxes if h >= height_threshold]\n",
    "    if not filtered_boxes:\n",
    "        return image\n",
    "\n",
    "    centers = np.array([[x + w//2, y + h//2] for (x, y, w, h) in filtered_boxes])\n",
    "    clustering = DBSCAN(eps=merge_dist, min_samples=3).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    largest_label = pd.Series(labels).value_counts().idxmax()\n",
    "    main_group_boxes = [box for i, box in enumerate(filtered_boxes) if labels[i] == largest_label]\n",
    "\n",
    "    x_min = max(0, min(x for x, _, _, _ in main_group_boxes) - expand_margin)\n",
    "    x_max = max(x + w for x, _, w, _ in main_group_boxes) + expand_margin\n",
    "    y_min = max(0, min(y for _, y, _, _ in main_group_boxes) - expand_margin)\n",
    "    y_max = max(y + h for _, y, _, h in main_group_boxes) + expand_margin\n",
    "\n",
    "    mask = np.zeros_like(gray)\n",
    "    accepted, rejected = [], []\n",
    "\n",
    "    for x, y, w, h in boxes:\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        if x_min <= cx <= x_max and y_min <= cy <= y_max:\n",
    "            accepted.append((x, y, w, h))\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "        else:\n",
    "            rejected.append((x, y, w, h))\n",
    "\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "    filtered_image = Image.fromarray(result)\n",
    "\n",
    "    if debug_path:\n",
    "        debug_img = img_rgb.copy()\n",
    "        for x, y, w, h in accepted:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        for x, y, w, h in rejected:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "        cv2.rectangle(debug_img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "        cv2.imwrite(debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if audit_csv_path:\n",
    "        records = []\n",
    "        for x, y, w, h in accepted:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"accepted\"})\n",
    "        for x, y, w, h in rejected:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"rejected\"})\n",
    "        pd.DataFrame(records).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                debug_path = f\"debug_lines/{filename}_page_{page_num}.png\" if debug else None\n",
    "                audit_csv_path = f\"debug_lines/{filename}_page_{page_num}_boxes.csv\" if debug else None\n",
    "\n",
    "                # üß† NUEVA funci√≥n din√°mica\n",
    "                filtered_img = filter_text_body_dynamic(\n",
    "                    image,\n",
    "                    size_threshold_percentile=40,\n",
    "                    merge_dist=20,\n",
    "                    expand_margin=10,\n",
    "                    debug_path=debug_path,\n",
    "                    audit_csv_path=audit_csv_path\n",
    "                )\n",
    "\n",
    "                page_text = pytesseract.image_to_string(filtered_img, lang=lang)\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # üõë Stop at Anexos\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"sc\",  # Cambia por tu carpeta de PDFs\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d6f369",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11bf8b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b583",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732a622",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenar los dos dataframes\n",
    "text_df = pd.concat([df_editable, df_scanned], ignore_index=True)\n",
    "\n",
    "# Ordenar por la columna 'date'\n",
    "text_df = text_df.sort_values(by='date')\n",
    "\n",
    "# Si deseas resetear los √≠ndices despu√©s de la concatenaci√≥n\n",
    "text_df = text_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "text_df.to_csv(\"raw_text_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f7447",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f091eb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905873",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas donde \"text\" empieza con una letra min√∫scula\n",
    "text_df[text_df[\"text\"].str.match(r\"^[a-z]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a964d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99986295",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2a300",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4495a5e",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-4\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.4 Noise reduction\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad352b2c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# BOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4509a5",
   "metadata": {},
   "source": [
    "\n",
    "# Testing on scanned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1 = df_editable.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb181b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1 = df_scanned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327efd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474c213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87487",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a35c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_noise(df, filter_keywords=True):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # === COMPILAR PATRONES DE DIRECCIONES A ELIMINAR ===\n",
    "    pattern_direccion_completa = re.compile(\n",
    "        r\"(?:Av\\.|Jr\\.|Calle)\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+\\d{1,5}\\s+‚Äî\\s+Oficina\\s+\\d{1,4}\\s+‚Äî\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+.*\",\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # === PATR√ìN DE DIRECCI√ìN ESCANEADA O CORRUPTA (completo o incrustado) ===\n",
    "    pattern_direccion_ruido = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "            \\b(?:Av|Jr|Calle|Psj|Prolongaci√≥n)\\.?\\s+            # V√≠a\n",
    "            [\\w√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±\\s\\.\\-]{3,30}                        # Nombre de calle\n",
    "            \\s+\\d{3,6}                                          # N√∫mero\n",
    "            (?:\\s+[^\\s]{3,15})?                                 # C√≥digo corrupto: \"OfIKi981\", etc.\n",
    "            \\s*[-‚Äì‚Äî]?\\s*\n",
    "            (?:San\\s+Isidro|Miraflores|Magdalena|Surco|Jesus\\s+Mar√≠a|Lince|Barranco)?\n",
    "            (?:\\s+e\\s+)?                                        # Posible \"e\"\n",
    "            (?:\\.pe|cf\\.gob\\.pe)?                               # Dominio\n",
    "        )\n",
    "        \"\"\",\n",
    "        flags=re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # === CONVERTIR A UNICODE SEGURO ===\n",
    "    def to_unicode(x):\n",
    "        try:\n",
    "            if isinstance(x, bytes):\n",
    "                return x.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            return str(x).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(to_unicode)\n",
    "    \n",
    "    # === ELIMINAR FILAS CON DIRECCIONES REPETITIVAS (ANTES DE LIMPIEZAS) ===\n",
    "    df_clean = df_clean[~df_clean[\"text\"].str.contains(pattern_direccion_completa)].reset_index(drop=True)\n",
    "    \n",
    "    # === LIMPIAR O ELIMINAR DIRECCIONES ESCANEADAS ===\n",
    "    def limpiar_o_eliminar_direccion(text):\n",
    "        if re.fullmatch(pattern_direccion_ruido, text.strip()):\n",
    "            return None  # Eliminar p√°rrafo completo\n",
    "        return re.sub(pattern_direccion_ruido, \"\", text)  # Reemplazar si est√° embebido\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(limpiar_o_eliminar_direccion)\n",
    "    df_clean = df_clean[df_clean[\"text\"].notnull() & df_clean[\"text\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "    # === LIMPIEZAS B√ÅSICAS VECTORIAL ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\b\\d{1,3}/\\d{1,3}\\b\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+([,;:.!?])\", r\"\\1\", regex=True)\n",
    "\n",
    "    # === LIMPIEZA DE URLS (normales y corruptas) ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"(https?://|htps://|htttp://|htpss://|www\\.|wwww\\.)[\\w\\-\\.]*\\.[a-z]{2,6}(\\/[\\w\\-\\.%]*)*\", \n",
    "        \"\", regex=True\n",
    "    )\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(htps?|htttp|htpss):\\/\\/[^\\s]*\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[‚Ä¢‚û¢√ò*ÔÄ™¬∞¬°!?¬ø\\\"]\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\":\\s*$\", \".\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"Lima[,]?\\s+\\d{1,2}\\s+de\\s+[a-z√°√©√≠√≥√∫]+\\s+de\\s+\\d{4}\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\"p.p.\", \"puntos porcentuales\")\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r'^\\s*((?:[ivxlcdm]+|[a-zA-Z]|\\d+)[\\.\\)]\\s*)+', '', regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(?:a|al|de|del|con|por|para|y|o|en|sin|sobre|ante|tras|entre|hacia|hasta|durante|mediante|excepto|salvo|seg√∫n)\\.$\",\n",
    "        lambda m: m.group(0)[:-1], regex=True)\n",
    "    \n",
    "    # === REEMPLAZAR EXPRESIONES CORRUPTAS ===\n",
    "    pattern_minist = re.compile(r\"MINIST\\s+ERIO[\\w%-¬∫|\\\"‚Äù]+|8N%IC%A\\s+Y\\s+FIN\\s+em.*?Usuaro\", flags=re.IGNORECASE)\n",
    "    pattern_garbage_symbols = re.compile(r\"[^\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]{5,}\")\n",
    "    \n",
    "    # Patr√≥n para capturar bloques muy corruptos (inicio o intermedios)\n",
    "    pattern_noise_block = re.compile(\n",
    "        r\"(?:[‚Äî\\[\\]_=\\]\\|%#@*<>~¬´¬ª]{2,}|[\\w]*‚Äî[\\w]*|‚Äî\\s*[A-Z]{1}\\s*‚Äî)+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_minist, \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_garbage_symbols, \"\", regex=True)\n",
    "    \n",
    "    # Aplica reemplazo a esos bloques basura\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_noise_block, \"\", regex=True)\n",
    "    \n",
    "    # Patr√≥n para frases de cortes√≠a y cierre\n",
    "    pattern_cortesia = re.compile(\n",
    "        r\"(aprovecho\\s+la\\s+oportunidad\\s+para\\s+expresar(le)?\\s*(a usted)?\\s*las?\\s*muestras?\\s+de\\s+mi\\s+especial\\s+consideraci√≥n( y estima)?[\\.]?)|\"  # Variante directa\n",
    "        r\"(le\\s+reitero\\s+(las?\\s+)?muestras?\\s+de\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)|\"  # Otra variante frecuente\n",
    "        r\"(hago\\s+propicia\\s+la\\s+ocasi√≥n\\s+para\\s+saludar(lo|la)?\\s*(muy)?\\s*atentamente[\\.]?)|\"  # Variante \"hago propicia...\"\n",
    "        r\"(sin\\s+otro\\s+particular.*?me\\s+despido[^\\n\\.]*[\\.]?)|\"  # Frase de cierre cl√°sica\n",
    "        r\"(con\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)\",  # Frase final t√≠pica\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Aplicar limpieza al texto\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_cortesia, \"\", regex=True)\n",
    "\n",
    "    # === FILTROS DE TEXTO V√ÅLIDO ===\n",
    "    def is_valid(text):\n",
    "        txt = text.strip()\n",
    "        letters = [c for c in txt if c.isalpha()]\n",
    "        if letters and sum(c.isupper() for c in letters) / len(letters) > 0.7:\n",
    "            return False\n",
    "        if txt.lower().startswith(\"fuente:\") or re.search(r\"\\b(Fuente:|Elaboraci√≥n:)\", txt, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        if re.search(r\"(?:\\b\\w\\s){3,}\\w\", txt):\n",
    "            return False\n",
    "        patterns = [\n",
    "            r'PCA\\s*(Inicial|1er\\s*Trim\\.|2do\\s*Trim\\.|3er\\s*Trim\\.)',\n",
    "            r'entre\\s+\\d{4}\\sy\\s+\\d{4}\\s*,?\\s*\\d+\\s*de\\s+cada\\s+100\\s*(leyes?|insistencia|implicancia\\s*fiscal)',\n",
    "            r'‚Äú[^‚Äù]*\\(p√°g\\.\\s*\\d+\\)[^‚Äù]*‚Äù|\\(p√°g\\.\\s*\\d+\\)',\n",
    "            r\"V√©ase informes\\s+(N¬∞\\s*\\d{2}-\\d{4}-CF[, y]*){2,}\",  # puedes mantenerla si gustas\n",
    "            r\"\\b(V√©ase|Ver|Consultar|Rem√≠tase|Rev√≠sese)\\s+(el\\s+)?(informe|art√≠culo|documento|reporte|an√°lisis|dictamen|comunicado\\s+oficial|nota)\\b.*?(N[¬∞¬∫]?\\s*\\d{1,3})?.*?\\b(del\\s+)?\\d{4}\",\n",
    "            r\"¬¢\\s*US?|US?\\s*¬¢\", r\"¬¢\"\n",
    "        ]\n",
    "        if any(re.search(p, txt, flags=re.IGNORECASE) for p in patterns):\n",
    "            return False\n",
    "        if len(txt.split()) < 6:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_clean = df_clean[df_clean[\"text\"].apply(is_valid)].reset_index(drop=True)\n",
    "\n",
    "    # === FUSI√ìN DE P√ÅRRAFOS CONSECUTIVOS ===\n",
    "    rows = []\n",
    "    i = 0\n",
    "    while i < len(df_clean):\n",
    "        current_row = df_clean.iloc[i].copy()\n",
    "        current_text = current_row[\"text\"]\n",
    "        current_title = current_row[\"title\"]\n",
    "\n",
    "        while i + 1 < len(df_clean):\n",
    "            next_row = df_clean.iloc[i + 1]\n",
    "            next_text = next_row[\"text\"]\n",
    "            next_title = next_row[\"title\"]\n",
    "\n",
    "            if next_title != current_title:\n",
    "                break\n",
    "\n",
    "            if not current_text.endswith(\".\") and next_text and next_text[0].isalpha() and next_text.endswith(\".\"):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{4}\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if re.match(r\"^[\\(,;:\\s]*[a-z\\(]\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{1,3}(?:,\\d+)?\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        current_row[\"text\"] = current_text\n",
    "        rows.append(current_row)\n",
    "        i += 1\n",
    "\n",
    "    df_result = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    df_result[\"paragraph_id\"] = df_result.groupby(\"title\").cumcount() + 1\n",
    "\n",
    "    # === LIMPIEZAS ADICIONALES ===\n",
    "    cleaned_rows = []\n",
    "    for idx, row in df_result.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        starts_with_special = re.match(r'^[*%\\\"\\'\\‚Äú\\\\\\[\\]\\{\\}\\(\\)\\^\\-+=<>~#@|]', text)\n",
    "        starts_with_number = re.match(r\"^\\d+\\s+\", text)\n",
    "        prev_text = df_result.iloc[idx - 1][\"text\"].strip() if idx > 0 else \"\"\n",
    "        prev_ends_with_dot = prev_text.endswith(\".\")\n",
    "\n",
    "        if starts_with_special:\n",
    "            continue\n",
    "        if starts_with_number and prev_ends_with_dot:\n",
    "            continue\n",
    "\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    df_result = pd.DataFrame(cleaned_rows).reset_index(drop=True)\n",
    "\n",
    "    # Eliminar n√∫meros al inicio del texto aunque est√©n pegados\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+\\s*\", \"\", regex=True)\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+(?=\\w)\", \"\", regex=True)\n",
    "    \n",
    "    # Eliminar observaciones con menos de 75 caracteres\n",
    "    df_result = df_result[df_result[\"text\"].str.len() >= 75].reset_index(drop=True)\n",
    "\n",
    "    # === FILTRO OPCIONAL CON KEYWORDS DE ALERTA FISCAL ===\n",
    "    if filter_keywords:\n",
    "        keywords = [\n",
    "            \"Incumplimiento de reglas fiscales\", \"Preocupaci√≥n\", \"Advertencia\", \"Alerta\",\n",
    "            \"Riesgos fiscales\", \"Desv√≠o del d√©ficit fiscal\", \"No cumplimiento\", \"Desviaciones significativas\",\n",
    "            \"Margen significativo\", \"Problema de credibilidad fiscal\", \"Credibilidad fiscal\",\n",
    "            \"Sostenibilidad fiscal\", \"Consolidaci√≥n fiscal\", \"Medidas correctivas\", \"Recomendaci√≥n\",\n",
    "            \"Necesidad de tomar medidas\", \"Control del gasto p√∫blico\", \"Presiones fiscales\",\n",
    "            \"Exceso de optimismo en proyecciones\", \"Exceso de gasto\", \"Aumento de gasto\",\n",
    "            \"Reducci√≥n de d√©ficit fiscal\", \"Incremento de ingresos permanentes\",\n",
    "            \"Falta de compromiso con la responsabilidad fiscal\", \"Medidas de consolidaci√≥n\",\n",
    "            \"Deficiencia en la ejecuci√≥n del gasto\", \"Aumento de la deuda p√∫blica\",\n",
    "            \"Iniciativas legislativas que afectan las finanzas p√∫blicas\", \"Incremento del gasto p√∫blico\",\n",
    "            \"Beneficios tributarios sin justificaci√≥n\", \"Tratamientos tributarios preferenciales\",\n",
    "            \"Erosi√≥n de la base tributaria\", \"Elusi√≥n y evasi√≥n tributaria\",\n",
    "            \"Aumento de gastos no previstos\", \"Aumento de gastos extraordinarios\",\n",
    "            \"Aumento de gastos en remuneraciones\", \"Crecimiento del gasto no financiero\",\n",
    "            \"Problema de sostenibilidad\", \"Riesgos de sostenibilidad fiscal\", \"Aumento de deuda neta\",\n",
    "            \"Desajuste fiscal\", \"Falta de transparencia en el gasto\", \"Riesgos de sobreendeudamiento\",\n",
    "            \"Excepciones a las reglas fiscales\", \"Riesgo de incumplimiento de metas fiscales\",\n",
    "            \"Aumento de los compromisos de deuda\", \"Riesgo de insolvencia\",\n",
    "            \"Falta de flexibilidad fiscal\", \"Desajuste entre el presupuesto y el MMM\",\n",
    "            \"Riesgo de incumplimiento debido a presiones de gasto\", \"Erosi√≥n de la capacidad recaudatoria\",\n",
    "            \"Incremento de la deuda p√∫blica\", \"Falta de control de gastos extraordinarios\",\n",
    "            \"Necesidad de ajustar el gasto\", \"Inestabilidad macroecon√≥mica\",\n",
    "            \"Problemas fiscales derivados de iniciativas legislativas\",\n",
    "            \"Riesgo de desajustes fiscales por reformas\",\n",
    "            \"Falta de capacidad de generar ingresos fiscales\", \"Riesgo de gasto excesivo\",\n",
    "            \"Incremento del gasto p√∫blico no controlado\", \"Medidas de ajuste fiscal\",\n",
    "            \"Inestabilidad presupuestaria\", \"Riesgo de inestabilidad fiscal\",\n",
    "            \"Falta de sostenibilidad de la deuda\", \"Compromiso con la disciplina fiscal\",\n",
    "            \"Necesidad de mejorar la disciplina fiscal\", \"Riesgos derivados de la crisis financiera\",\n",
    "            \"Emergencia fiscal\", \"No cumplimiento de los l√≠mites de deuda\",\n",
    "            \"Riesgo de presi√≥n sobre las finanzas p√∫blicas\", \"Riesgos de sostenibilidad a largo plazo\",\n",
    "            \"Inconsistencia en las proyecciones fiscales\", \"Proyecciones fiscales no realistas\",\n",
    "            \"Implicaciones fiscales de la situaci√≥n de Petroper√∫\",\n",
    "            \"Desajuste en las proyecciones fiscales\", \"Necesidad de consolidaci√≥n fiscal\",\n",
    "            \"Riesgos de desequilibrio fiscal\", \"Amenaza a la estabilidad fiscal\",\n",
    "            \"Inseguridad fiscal\", \"Inconsistencias fiscales\", \"Falta de previsi√≥n en el gasto\",\n",
    "            \"Riesgo de p√©rdida de control fiscal\", \"Impacto fiscal no anticipado\",\n",
    "            \"Presi√≥n de gastos adicionales\", \"Aumento en la presi√≥n fiscal\",\n",
    "            \"Erosi√≥n de las finanzas p√∫blicas\", \"Riesgo de d√©ficit fiscal no controlado\",\n",
    "            \"Aumento de la carga fiscal\", \"Riesgo de crisis fiscal\",\n",
    "            \"Propuestas legislativas que generan gasto\",\n",
    "            \"Propuestas que limitan la recaudaci√≥n fiscal\",\n",
    "            \"Iniciativas fiscales con implicaciones negativas\",\n",
    "            \"Aumento de los gastos sociales no previstos\",\n",
    "            \"Riesgo de incumplimiento de los l√≠mites fiscales\",\n",
    "            \"Propuestas legislativas que no cumplen con las reglas fiscales\",\n",
    "            \"Desviaciones fiscales no justificadas\",\n",
    "            \"Proyecciones de d√©ficit fiscal no alcanzables\",\n",
    "            \"Riesgos derivados de iniciativas legislativas excesivas\",\n",
    "            \"Crecimiento de la deuda p√∫blica sin control\",\n",
    "            \"Necesidad de pol√≠ticas fiscales m√°s estrictas\"\n",
    "        ]\n",
    "        keywords_lower = [k.lower() for k in keywords]\n",
    "        df_result = df_result[\n",
    "            df_result[\"text\"].str.lower().apply(lambda txt: any(k in txt for k in keywords_lower))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alertas = clean_noise(df, filter_keywords=True)   # Solo alertas fiscales\n",
    "df_editable_cleaned = clean_noise(df_copy_editable_1, filter_keywords=False) # Todo texto √∫til"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff0fb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_ed_2 = df_editable_cleaned[df_editable_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_ed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba3655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f97370",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1e3b6",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### The same abve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cc388",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned = clean_noise(df_copy_scanned_1, filter_keywords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129198df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a48a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_sc_2 = df_scanned_cleaned[df_scanned_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_sc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0b03f",
   "metadata": {},
   "source": [
    "# Limpieza global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset = pd.read_csv(\"raw_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = clean_noise(raw_text_dataset, filter_keywords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeba45",
   "metadata": {},
   "source": [
    "# Limpieza para estad√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea36f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "input_text_dataset = pd.read_csv(\"input_text_dataset_true.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68515fb",
   "metadata": {},
   "source": [
    "## Order by year and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_type\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_year_and_date(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'date' a datetime (si es necesario) y ordena el DataFrame por 'year' y 'date'.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame que contiene las columnas 'year' y 'date'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame ordenado correctamente por 'year' y 'date'.\n",
    "    \"\"\"\n",
    "    required_cols = {'year', 'date'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'year' y 'date'.\")\n",
    "\n",
    "    # Convertir 'date' a datetime si no lo es ya\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"date\"]):\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors='coerce')\n",
    "\n",
    "    return df.sort_values(by=[\"year\", \"date\", \"page\", \"paragraph_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = sort_by_year_and_date(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_doc_type_from_title(df):\n",
    "    \"\"\"\n",
    "    Completa los valores vac√≠os en la columna 'doc_type' a partir del contenido de la columna 'title'.\n",
    "    Solo extrae 'Comunicado' o 'Informe' si est√°n al inicio del t√≠tulo.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame con las columnas 'title' y 'doc_type'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_type' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas requeridas existan\n",
    "    required_cols = {'title', 'doc_type'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'title' y 'doc_type'.\")\n",
    "\n",
    "    # Crear m√°scara de filas donde doc_type est√° vac√≠o\n",
    "    mask = df[\"doc_type\"].isna()\n",
    "\n",
    "    # Expresi√≥n regular para capturar solo \"Comunicado\" o \"Informe\" al inicio del t√≠tulo\n",
    "    regex = r\"^(Comunicado|Informe)\\b\"\n",
    "\n",
    "    # Extraer y llenar solo en filas vac√≠as\n",
    "    df.loc[mask, \"doc_type\"] = df.loc[mask, \"title\"].str.extract(regex)[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7787bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_doc_type_from_title(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd665d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_id\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_doc_id(input_text_dataset):\n",
    "    \"\"\"\n",
    "    Llena los valores vac√≠os en 'doc_id' con n√∫meros flotantes secuenciales por a√±o, \n",
    "    basados en el orden de 'date', empezando en 1.0.\n",
    "\n",
    "    Par√°metros:\n",
    "        input_text_dataset (pd.DataFrame): DataFrame que debe contener las columnas 'doc_id', 'year', y 'date'.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_id' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas necesarias existan\n",
    "    required_cols = {'doc_id', 'year', 'date'}\n",
    "    if not required_cols.issubset(input_text_dataset.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'doc_id', 'year' y 'date'.\")\n",
    "\n",
    "    # Ordenar por year y date\n",
    "    df = input_text_dataset.sort_values(by=[\"year\", \"date\"]).copy()\n",
    "\n",
    "    # Crear m√°scara de valores vac√≠os en doc_id\n",
    "    mask = df[\"doc_id\"].isna()\n",
    "\n",
    "    # Para cada a√±o, generar un contador incremental para fechas √∫nicas\n",
    "    def assign_ids(group):\n",
    "        # Solo para filas con doc_id nulo\n",
    "        missing = group[\"doc_id\"].isna()\n",
    "        # Contador secuencial por fecha\n",
    "        group.loc[missing, \"doc_id\"] = (\n",
    "            group.loc[missing]\n",
    "                 .groupby(\"date\", sort=False)\n",
    "                 .ngroup() + 1\n",
    "        ).astype(float)\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(assign_ids)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_missing_doc_id(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368141b",
   "metadata": {},
   "source": [
    "# TO csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"llm_input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20813007",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Estad√≠sticos de p√°rrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c872a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "# input_text_dataset = pd.read_csv(\"input_text_dataset.csv\")\n",
    "\n",
    "# === Total de documentos √∫nicos, por tipo ===\n",
    "total_docs = input_text_dataset[['title', 'doc_id', 'date']].apply(tuple, axis=1).nunique()\n",
    "docs_por_tipo = input_text_dataset.drop_duplicates('title')['doc_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_por_tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c849ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A√±os con m√°s y menos opiniones ===\n",
    "opiniones_por_a√±o = input_text_dataset.groupby('year')['doc_id'].nunique()\n",
    "a√±o_mas_opiniones = opiniones_por_a√±o.idxmax()\n",
    "a√±o_menos_opiniones = opiniones_por_a√±o.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiniones_por_a√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_mas_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_menos_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Documento con m√°s/menos p√°rrafos ===\n",
    "parrafos_por_doc = input_text_dataset.groupby('title')['paragraph_id'].nunique()\n",
    "doc_mas_parrafos = parrafos_por_doc.idxmax()\n",
    "doc_menos_parrafos = parrafos_por_doc.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mas_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_menos_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5266a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmin()]\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmax()]\n",
    "tamano_promedio_parrafo = input_text_dataset[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a418",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4689",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a89ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968737a",
   "metadata": {},
   "source": [
    "# palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Conteo de palabras por p√°rrafo ===\n",
    "input_text_dataset[\"word_count\"] = input_text_dataset[\"text\"].str.split().str.len()\n",
    "\n",
    "# P√°rrafo con menor cantidad de palabras\n",
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmin()]\n",
    "\n",
    "# P√°rrafo con mayor cantidad de palabras\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmax()]\n",
    "\n",
    "# Tama√±o promedio de p√°rrafo en n√∫mero de palabras\n",
    "tamano_promedio_parrafo = input_text_dataset[\"word_count\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8886fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d15d9a2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc27b1",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9207",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "policy_brief",
   "language": "python",
   "name": "policy_brief"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
