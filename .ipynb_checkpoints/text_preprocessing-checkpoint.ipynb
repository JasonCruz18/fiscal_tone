{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b952faed",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"background:#634C44; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22631b14",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea16164-f75c-4b20-9d2b-71fb36b5cf9b",
   "metadata": {},
   "source": [
    "## LLM using Gemini (langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497e7d2",
   "metadata": {},
   "source": [
    "> **Author:** Jason Cruz  \n",
    "  **Last updated:** 11/16/2025  \n",
    "  **Python version:** 3.12  \n",
    "  **Project:** Fiscal Tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916311",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Summary\n",
    "Welcome to the **Fiscal Tone** Text Preprocessing notebook! This notebook will guide you through the **step-by-step process** of \n",
    "\n",
    "\n",
    "### What will this notebook help you achieve?\n",
    "1. **Downloading PDFs** from the BCRP Weekly Reports (WR).\n",
    "2. **Generating PDF inputs** by shortening them to focus on key pages containing GDP growth rate tables.\n",
    "3. **Cleaning-up extracted data** to ensure it's usable and building RTD.\n",
    "4. **Concatenating RTD** from different years and frequencies (monthly, quarterly, annual).\n",
    "5. **Updating metadata** for storing base years changes and other revisions-based information.\n",
    "6. **Converting RTD** to releases dataset for econometric analysis.\n",
    "\n",
    "üåê **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (üì∞ WR, from here on)  \n",
    "For any questions or issues, feel free to reach out via email: [Jason üì®](mailto:jj.cruza@up.edu.pe)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a909438",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### ‚öôÔ∏è Initial Set-up\n",
    "\n",
    "Before preprocessing the new GDP releases data, we need to perform some initial set-up steps:\n",
    "\n",
    "1. üß∞ **Import helper functions** from `gdp_rtd_pipeline.py` that are required for this notebook.\n",
    "2. üõ¢Ô∏è **Connect to the PostgreSQL database** that will contain GDP revisions datasets. _(This step is pending: direct access will be provided via ODBC or other methods, allowing users to connect from any software or programming language.)_\n",
    "3. üìÇ **Create necessary folders** to store inputs, outputs, logs, and screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f754c24",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "#from cf_mef_functions.py import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f17343",
   "metadata": {},
   "source": [
    "> üöß Although the second step (database connection) is pending, the notebook currently works using **flat files (CSV)**. These CSV files will **not be saved in GitHub** as they are included in the `.gitignore` to ensure no data is stored publicly. Users can be confident that no data will be stored on GitHub. The notebook **automatically generates the CSV files**, giving users direct access to the dataset on their own systems. The data is created on the fly and can be saved locally for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8c42b",
   "metadata": {},
   "source": [
    "### üß∞ Import helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d50a",
   "metadata": {},
   "source": [
    "This notebook relies on a set of helper functions found in the script `gdp_rtd_pipeline.py`. These functions will be used throughout the notebook, so please ensure you have them ready by running the line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6199",
   "metadata": {},
   "source": [
    "> üõ†Ô∏è **Libraries:** Before you begin, please ensure that you have the required libraries installed and imported. See all the libraries you need section by section in `gdp_rtd_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660bfd8",
   "metadata": {},
   "source": [
    "**Check out Python information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "addf904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Information\n",
      "  Version  : 3.12.11\n",
      "  Compiler : MSC v.1929 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jun  5 2025 12:58:53')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"üêç Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ace86",
   "metadata": {},
   "source": [
    "### üìÇ Create necessary folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb2655",
   "metadata": {},
   "source": [
    "We will start by creating the necessary folders to store the data at various stages of processing. The following code ensures all required directories exist, and if not, it creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6916c1-e725-4eba-8199-4aa480f7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a78251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\n",
      "üìÇ data created\n",
      "üìÇ data\\raw created\n",
      "üìÇ data\\input created\n",
      "üìÇ data\\output created\n",
      "üìÇ metadata created\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # Importing Path module from pathlib to handle file and directory paths in a cross-platform way.\n",
    "\n",
    "# Get current working directory\n",
    "PROJECT_ROOT = Path.cwd()  # Get the current working directory where the notebook is being executed.\n",
    "\n",
    "# User input for folder location\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"  # Prompt user to input the folder path or use the default value \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()  # Combine the project root directory with user input to get the full target path.\n",
    "\n",
    "# Create the necessary directories if they don't already exist\n",
    "target_path.mkdir(parents=True, exist_ok=True)  # Creates the target folder and any necessary parent directories.\n",
    "print(f\"Using path: {target_path}\")  # Print out the path being used for confirmation.\n",
    "\n",
    "# Define paths for saving data and PDFs\n",
    "data_folder = 'data'  # This folder will store the new Weekly Reports (post-2013), which are in PDF format.\n",
    "raw_data_subfolder = os.path.join(data_folder, 'raw')  # Subfolder for saving the raw PDFs exactly as downloaded from the BCRP website.\n",
    "input_data_subfolder = os.path.join(data_folder, 'input')  # Subfolder for saving reduced PDFs that contain only the selected pages with GDP growth tables.\n",
    "output_data_subfolder = os.path.join(data_folder, 'output')\n",
    "\n",
    "# Additional folders for metadata, records, and alert tracking\n",
    "metadata_folder = 'metadata'  # Folder for storing metadata files like wr_metadata.csv.\n",
    "\n",
    "# Create additional required folders\n",
    "for folder in [data_folder, raw_data_subfolder, input_data_subfolder, output_data_subfolder, metadata_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create the additional folders if they don't exist.\n",
    "    print(f\"üìÇ {folder} created\")  # Print confirmation for each of these additional folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc9c1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1f74",
   "metadata": {},
   "source": [
    "## 1. Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75589e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f6410e-3fdf-4b83-864c-29b6da530698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: classify PPT / presentation documents\n",
    "# ======================================================\n",
    "\n",
    "def is_presentation_pdf(url_or_text):\n",
    "    if not url_or_text:\n",
    "        return False\n",
    "    s = url_or_text.lower()\n",
    "    ppt_keywords = [\n",
    "        \"ppt\", \"presentacion\", \"presentaci√≥n\", \"diapositiva\",\n",
    "        \"slides\", \"conferencia\", \"powerpoint\"\n",
    "    ]\n",
    "    return any(kw in s for kw in ppt_keywords)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: extract all PDF links\n",
    "# ======================================================\n",
    "\n",
    "def extract_pdf_links(dsoup):\n",
    "    pdf_links = []\n",
    "    for a in dsoup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \".pdf\" in href.lower():\n",
    "            txt = a.text.strip() if a.text else \"\"\n",
    "            pdf_links.append((href, txt))\n",
    "    return pdf_links\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: select the best PDF from multiple candidates\n",
    "# ======================================================\n",
    "\n",
    "def select_appropriate_pdf(pdf_links):\n",
    "    if not pdf_links:\n",
    "        return None\n",
    "\n",
    "    filtered = [\n",
    "        (href, txt) for href, txt in pdf_links\n",
    "        if not is_presentation_pdf(href) and not is_presentation_pdf(txt)\n",
    "    ]\n",
    "\n",
    "    candidates = filtered if filtered else pdf_links\n",
    "\n",
    "    priority_keywords = [\n",
    "        \"comunicado\", \"informe\", \"nota\", \"reporte\",\n",
    "        \"documento\", \"pronunciamiento\"\n",
    "    ]\n",
    "\n",
    "    def score(x):\n",
    "        href, _ = x\n",
    "        h = href.lower()\n",
    "        return sum(kw in h for kw in priority_keywords)\n",
    "\n",
    "    return max(candidates, key=score)[0]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# SCRAPER ‚Äì Incremental: only visits detail pages not seen before\n",
    "# ======================================================\n",
    "\n",
    "def scrape_cf(url, already_scraped_pages):\n",
    "    t0 = timer()\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    rows = soup.select(\"table.table tbody tr\")\n",
    "    new_records = []\n",
    "    list_records = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            date = row.select_one(\"td.size100 p\").text.strip()\n",
    "            link_tag = row.select_one(\"td a\")\n",
    "            doc_title = link_tag.text.strip()\n",
    "            page_url = link_tag[\"href\"]\n",
    "\n",
    "            list_records.append({\n",
    "                \"date\": date,\n",
    "                \"doc_title\": doc_title,\n",
    "                \"page_url\": page_url\n",
    "            })\n",
    "\n",
    "            if page_url in already_scraped_pages:\n",
    "                continue\n",
    "\n",
    "            detail = requests.get(page_url, timeout=15)\n",
    "            dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "\n",
    "            pdf_url = None\n",
    "\n",
    "            # A) <a> PDF\n",
    "            pdf_links = extract_pdf_links(dsoup)\n",
    "            if pdf_links:\n",
    "                pdf_url = select_appropriate_pdf(pdf_links)\n",
    "\n",
    "            # B) iframe\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if iframe:\n",
    "                    src = iframe[\"src\"]\n",
    "                    if src.startswith(\"//\"):\n",
    "                        src = \"https:\" + src\n",
    "                    pdf_url = src\n",
    "\n",
    "            # C) Google Docs viewer\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \"docs.google.com\" in x.lower())\n",
    "                if iframe:\n",
    "                    parsed = urlparse(iframe[\"src\"])\n",
    "                    q = parse_qs(parsed.query)\n",
    "                    if \"url\" in q:\n",
    "                        pdf_url = unquote(q[\"url\"][0])\n",
    "\n",
    "            # D) PDF viewer \"Guardar\"\n",
    "            if not pdf_url:\n",
    "                button = dsoup.find(\"button\", id=\"downloadButton\") or dsoup.find(\"span\", string=\"Guardar\")\n",
    "                if button:\n",
    "                    parent = button.find_parent(\"a\")\n",
    "                    if parent and parent.has_attr(\"href\"):\n",
    "                        pdf_url = parent[\"href\"]\n",
    "\n",
    "            filename = pdf_url.split(\"/\")[-1] if pdf_url else None\n",
    "\n",
    "            new_records.append({\n",
    "                \"date\": date,\n",
    "                \"doc_title\": doc_title,\n",
    "                \"page_url\": page_url,\n",
    "                \"pdf_url\": pdf_url,\n",
    "                \"pdf_filename\": filename\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing row: {e}\")\n",
    "\n",
    "    print(f\"‚åõ scrape_cf_expanded executed in {timer() - t0:.2f} sec\")\n",
    "    return list_records, new_records\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# DOWNLOADER ‚Äî with incremental CSV-safe saving (even if interrupted)\n",
    "# ======================================================\n",
    "\n",
    "def pdf_downloader(cf_urls, raw_pdf_folder, metadata_folder, metadata_csv):\n",
    "\n",
    "    t0 = timer()\n",
    "    os.makedirs(raw_pdf_folder, exist_ok=True)\n",
    "    os.makedirs(metadata_folder, exist_ok=True)\n",
    "\n",
    "    metadata_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load previous metadata safely\n",
    "    if os.path.exists(metadata_path):\n",
    "        old_df = pd.read_csv(metadata_path, dtype=str)\n",
    "        old_urls = set(old_df[\"pdf_url\"].dropna())\n",
    "        old_pages = set(old_df[\"page_url\"].dropna())\n",
    "    else:\n",
    "        old_df = pd.DataFrame()\n",
    "        old_urls = set()\n",
    "        old_pages = set()\n",
    "\n",
    "    all_new_records = []\n",
    "\n",
    "    # SCRAPE incremental\n",
    "    for url in cf_urls:\n",
    "        print(f\"\\nüåê Scraping list page: {url}\")\n",
    "        list_records, new_page_records = scrape_cf(\n",
    "            url, already_scraped_pages=old_pages\n",
    "        )\n",
    "        all_new_records.extend(new_page_records)\n",
    "\n",
    "    if not all_new_records:\n",
    "        print(\"\\nüîé No new pages: skipping download.\")\n",
    "        print(f\"üìù Metadata unchanged: {metadata_path}\")\n",
    "        # ‚Üí return existing CSV as DataFrame\n",
    "        return pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "    new_df = pd.DataFrame(all_new_records).dropna(subset=[\"pdf_url\"])\n",
    "    mask_new = ~new_df[\"pdf_url\"].isin(old_urls)\n",
    "    df_to_download = new_df[mask_new].copy()\n",
    "\n",
    "    # Sort oldest ‚Üí newest\n",
    "    df_to_download[\"date\"] = pd.to_datetime(df_to_download[\"date\"], dayfirst=True)\n",
    "    df_to_download = df_to_download.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nüîé Found {len(df_to_download)} new PDFs to download\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/pdf\"}\n",
    "\n",
    "    # ---- Incremental download + incremental CSV writing ----\n",
    "    temp_df = old_df.copy()\n",
    "\n",
    "    for i, row in df_to_download.iterrows():\n",
    "        pdf_url = row[\"pdf_url\"]\n",
    "        filename = row[\"pdf_filename\"]\n",
    "        page_url = row[\"page_url\"]\n",
    "        filepath = os.path.join(raw_pdf_folder, filename)\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(df_to_download)}] üìÑ {filename}\")\n",
    "        print(f\"üîó {pdf_url}\")\n",
    "        \n",
    "        success = False\n",
    "\n",
    "        # Primary download attempt\n",
    "        try:\n",
    "            r = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"‚úÖ Saved {filename}\")\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Primary failed: {e}\")\n",
    "        \n",
    "        # Fallback attempt\n",
    "        if not success:\n",
    "            try:\n",
    "                print(\"üîÅ Trying extended fallback‚Ä¶\")\n",
    "                detail = requests.get(page_url, timeout=15)\n",
    "                dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "        \n",
    "                iframe_url = None\n",
    "        \n",
    "                # 1) <embed src=\"...pdf\">\n",
    "                embed = dsoup.find(\"embed\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if embed:\n",
    "                    iframe_url = embed[\"src\"]\n",
    "        \n",
    "                # 2) <div data-pdf-src=\"...pdf\">\n",
    "                if not iframe_url:\n",
    "                    divpdf = dsoup.find(\"div\", attrs={\"data-pdf-src\": True})\n",
    "                    if divpdf and \".pdf\" in divpdf[\"data-pdf-src\"].lower():\n",
    "                        iframe_url = divpdf[\"data-pdf-src\"]\n",
    "        \n",
    "                # Normalize URL\n",
    "                if iframe_url and iframe_url.startswith(\"//\"):\n",
    "                    iframe_url = \"https:\" + iframe_url\n",
    "        \n",
    "                if iframe_url:\n",
    "                    print(f\"   ‚á¢ fallback PDF URL: {iframe_url}\")\n",
    "        \n",
    "                    r2 = requests.get(iframe_url, headers=headers, timeout=20)\n",
    "                    r2.raise_for_status()\n",
    "        \n",
    "                    if \"pdf\" not in r2.headers.get(\"Content-Type\", \"\").lower():\n",
    "                        raise ValueError(\"Server returned HTML instead of PDF\")\n",
    "        \n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        f.write(r2.content)\n",
    "        \n",
    "                    print(f\"‚úÖ Saved via embed/data-pdf-src fallback: {filename}\")\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(\"‚ùå No embed/data-pdf-src found\")\n",
    "        \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Extended fallback failed: {e2}\")\n",
    "\n",
    "        # === Incremental update to metadata (even if interrupted later) ===\n",
    "        temp_df = pd.concat([temp_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        temp_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "        # avoid rate limit\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"üìù Metadata saved incrementally: {metadata_path}\")\n",
    "    print(f\"‚è±Ô∏è Done in {round(timer() - t0, 2)} sec\")\n",
    "\n",
    "    # === RETURN THE CSV AS DATAFRAME ===\n",
    "    final_df = pd.read_csv(metadata_path, dtype=str)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0a6dff3-a6a6-44b6-99a9-87f3be0a3fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Scraping list page: https://cf.gob.pe/p/informes/\n",
      "‚åõ scrape_cf_expanded executed in 1.07 sec\n",
      "\n",
      "üåê Scraping list page: https://cf.gob.pe/p/comunicados/\n",
      "‚åõ scrape_cf_expanded executed in 0.93 sec\n",
      "\n",
      "üîé No new pages: skipping download.\n",
      "üìù Metadata unchanged: metadata\\cf_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "cf_urls = [\n",
    "    \"https://cf.gob.pe/p/informes/\",\n",
    "    \"https://cf.gob.pe/p/comunicados/\"\n",
    "]\n",
    "\n",
    "metadata_df = pdf_downloader(\n",
    "    cf_urls=cf_urls,\n",
    "    raw_pdf_folder=raw_data_subfolder,\n",
    "    metadata_folder=metadata_folder,\n",
    "    metadata_csv=\"cf_metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d174521-b611-4a90-b242-18e39551b61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>page_url</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>pdf_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>Informe_CF_N_001-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>Informe_CF_N_002-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>Informe_CF_N_003-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>Informe_CF_N_005-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>Informe_CF_N_004-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>Informe_CF_N_006-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>Informe_CF_N_007-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>Informe_CF_N_008-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_001-2017-CF.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_002-2017-CF.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2016-01-28  Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "1  2016-04-15  Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "2  2016-06-23  Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "3  2016-08-18  Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "4  2016-08-18  Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "5  2016-08-26  Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "6  2016-10-04  Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "7  2016-12-26  Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "8  2017-05-03  Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...   \n",
       "9  2017-05-09  Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...   \n",
       "\n",
       "                                            page_url  \\\n",
       "0  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "1  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "2  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "3  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "4  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "5  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "6  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "7  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "9  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "\n",
       "                                           pdf_url               pdf_filename  \n",
       "0  https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf  Informe_CF_N_001-2016.pdf  \n",
       "1  https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf  Informe_CF_N_002-2016.pdf  \n",
       "2  https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf  Informe_CF_N_003-2016.pdf  \n",
       "3  https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf  Informe_CF_N_005-2016.pdf  \n",
       "4  https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf  Informe_CF_N_004-2016.pdf  \n",
       "5  https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf  Informe_CF_N_006-2016.pdf  \n",
       "6  https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf  Informe_CF_N_007-2016.pdf  \n",
       "7  https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf  Informe_CF_N_008-2016.pdf  \n",
       "8  https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf  INFORME_N_001-2017-CF.pdf  \n",
       "9  https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf  INFORME_N_002-2017-CF.pdf  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c597d21",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### Delete selected documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7254f2a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Lista de archivos a eliminar\n",
    "pdfs_to_remove = [\n",
    "    \"Informe-anual-2017_CF_vf.pdf\", # This is a way too long document containing statistical analaysis. We're focus only on text as comunicados from CF.\n",
    "    \"Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bafdf3a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_pdfs(folder_path, filenames_to_remove):\n",
    "    \"\"\"\n",
    "    Deletes specific unwanted PDF files from a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, the directory containing the PDFs\n",
    "    - filenames_to_remove: list of str, filenames to delete\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    removed_count = 0\n",
    "\n",
    "    print(f\"üßπ Removing in: {folder_path}\")\n",
    "    for filename in filenames_to_remove:\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            os.remove(full_path)\n",
    "            print(f\"üóëÔ∏è Deleted: {filename}\")\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "\n",
    "    print(f\"\\nüßπ Cleanup complete. Total files removed: {removed_count}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caca1ebf",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Removing in: data\\raw\n",
      "üóëÔ∏è Deleted: Informe-anual-2017_CF_vf.pdf\n",
      "üóëÔ∏è Deleted: Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\n",
      "\n",
      "üìä Summary:\n",
      "\n",
      "üßπ Cleanup complete. Total files removed: 2\n",
      "‚è±Ô∏è Time taken: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "remove_unwanted_pdfs(raw_data_subfolder, pdfs_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433e30b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Clasificando entre scanned and editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a037",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Please, compruebe que la carpeta \"\" contenga scaneados y que la carpeta \"\" ediatables. En caso de que no se haya clasificado correctamente los pdfs, agreguelos manualemnte. Esto es vital para los pr√≥ximos c√≥digos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5ef1b9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF, used to extract text from PDFs\n",
    "\n",
    "def is_editable_pdf(file_path, min_text_length=20):\n",
    "    \"\"\"Check if a PDF contains extractable text (editable).\"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            return len(\"\".join(page.get_text() for page in doc).strip()) >= min_text_length\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def classify_pdfs_by_type(classification_folder):\n",
    "    \"\"\"\n",
    "    Classifies PDF files into 'editable' and 'scanned' subfolders.\n",
    "    This function now only focuses on classifying the type of raw data (editable or scanned) and moving files.\n",
    "    \n",
    "    Parameters:\n",
    "    - classification_folder: str, the directory where 'editable' and 'scanned' subfolders will be created.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ensure classification_folder is a list, even if a single folder is passed\n",
    "    if isinstance(classification_folder, str):\n",
    "        classification_folder = [classification_folder]\n",
    "\n",
    "    # Create the 'editable' and 'scanned' subfolders within the classification_folder\n",
    "    output_dir_editable = os.path.join(classification_folder[0], \"editable\")\n",
    "    output_dir_scanned = os.path.join(classification_folder[0], \"scanned\")\n",
    "    os.makedirs(output_dir_editable, exist_ok=True)\n",
    "    os.makedirs(output_dir_scanned, exist_ok=True)\n",
    "\n",
    "    total_files = 0\n",
    "    scanned_count = 0\n",
    "    editable_count = 0\n",
    "\n",
    "    t0 = timer()\n",
    "    print(\"üîç Starting PDF classification...\")\n",
    "\n",
    "    # Iterate through the provided folders\n",
    "    for folder in classification_folder:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                total_files += 1\n",
    "                pdf_path = os.path.join(folder, filename)\n",
    "\n",
    "                # Classify and move the PDF to the appropriate folder\n",
    "                if is_editable_pdf(pdf_path):\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_editable, filename))\n",
    "                    editable_count += 1\n",
    "                else:\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_scanned, filename))\n",
    "                    scanned_count += 1\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    # Print a summary\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"üìÑ Total PDFs processed: {total_files}\")\n",
    "    print(f\"üíª Editable PDFs: {editable_count}\")\n",
    "    print(f\"üñ®Ô∏è Scanned PDFs: {scanned_count}\")\n",
    "    print(f\"üìÅ Saved editable PDFs in: '{output_dir_editable}'\")\n",
    "    print(f\"üìÅ Saved scanned PDFs in: '{output_dir_scanned}'\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "508d2041",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting PDF classification...\n",
      "\n",
      "üìä Summary:\n",
      "üìÑ Total PDFs processed: 77\n",
      "üíª Editable PDFs: 64\n",
      "üñ®Ô∏è Scanned PDFs: 13\n",
      "üìÅ Saved editable PDFs in: 'data\\raw\\editable'\n",
      "üìÅ Saved scanned PDFs in: 'data\\raw\\scanned'\n",
      "‚è±Ô∏è Time taken: 2.24 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the folder paths\n",
    "classification_folder = raw_data_subfolder\n",
    "\n",
    "# Call the function to classify PDFs\n",
    "classify_pdfs_by_type(classification_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62017a05-bc5f-49cf-b12b-b66ae4b236e0",
   "metadata": {},
   "source": [
    "Metadata Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308f8ea-9d91-45d6-89c1-2d77ed9953b9",
   "metadata": {},
   "source": [
    "Adds metadata from a CSV file, enhancing the extracted text with additional document-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37a1d261-968c-488d-8ffb-e7051a93d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def metadata_enrichment(classification_folder, metadata_folder, metadata_csv=\"cf_metadata\"):\n",
    "    \"\"\"\n",
    "    Enriches metadata with information such as 'pdf_type', 'doc_type', 'doc_number', 'year', and 'month'.\n",
    "    \n",
    "    Parameters:\n",
    "    - classification_folder: str, the directory where 'editable' and 'scanned' subfolders exist.\n",
    "    - metadata_folder: str, the folder where the metadata CSV file is located.\n",
    "    - metadata_csv: str, the name of the CSV file containing metadata (without the '.csv' extension).\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df: DataFrame with the enriched metadata.\n",
    "    \"\"\"\n",
    "    # Add .csv extension to metadata_csv if not provided\n",
    "    metadata_csv_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load the metadata CSV file\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # Ensure 'pdf_type', 'doc_type', 'doc_number', 'year' and 'month' columns exist\n",
    "    if 'pdf_type' not in metadata_df.columns:\n",
    "        metadata_df['pdf_type'] = ''\n",
    "    if 'doc_type' not in metadata_df.columns:\n",
    "        metadata_df['doc_type'] = ''\n",
    "    if 'doc_number' not in metadata_df.columns:\n",
    "        metadata_df['doc_number'] = ''\n",
    "    if 'year' not in metadata_df.columns:\n",
    "        metadata_df['year'] = ''\n",
    "    if 'month' not in metadata_df.columns:\n",
    "        metadata_df['month'] = ''\n",
    "\n",
    "    # Function to extract 'doc_type', 'doc_number', 'year' from 'doc_title' column\n",
    "    def extract_doc_info(row):\n",
    "        doc_title = row[\"doc_title\"]\n",
    "        # Extract document type (Informe, Comunicado), document number, and year\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", doc_title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "\n",
    "        # Convert doc_number to integer (this removes leading zeros)\n",
    "        if doc_number:\n",
    "            doc_number = int(doc_number)\n",
    "\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    # Enrich metadata by applying doc_type, doc_number, and year extraction\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "\n",
    "    # Function to extract 'month' from 'date' column (assuming 'date' is in \"YYYY-MM-DD\" format)\n",
    "    def extract_month(row):\n",
    "        date_str = row[\"date\"]\n",
    "        if pd.notna(date_str):  # Check if the date is not NaN\n",
    "            # Extract month from the date string (assuming format is \"YYYY-MM-DD\")\n",
    "            try:\n",
    "                month = int(date_str.split('-')[1])  # Extract the month (second part of the date)\n",
    "                return month\n",
    "            except IndexError:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    # Add 'month' column by applying the extract_month function\n",
    "    metadata_df['month'] = metadata_df.apply(extract_month, axis=1)\n",
    "\n",
    "    # Ensure 'editable' and 'scanned' subfolders exist in the provided classification_folder\n",
    "    editable_folder = os.path.join(classification_folder, \"editable\")\n",
    "    scanned_folder = os.path.join(classification_folder, \"scanned\")\n",
    "\n",
    "    # Check if the 'editable' and 'scanned' folders exist\n",
    "    if not os.path.isdir(editable_folder):\n",
    "        print(f\"‚ùå 'editable' folder does not exist in '{classification_folder}'.\")\n",
    "    if not os.path.isdir(scanned_folder):\n",
    "        print(f\"‚ùå 'scanned' folder does not exist in '{classification_folder}'.\")\n",
    "\n",
    "    # Update 'pdf_type' based on folder classification\n",
    "    for folder, file_type in [(editable_folder, \"editable\"), (scanned_folder, \"scanned\")]:\n",
    "        if os.path.isdir(folder):\n",
    "            for filename in os.listdir(folder):\n",
    "                if filename.lower().endswith(\".pdf\"):\n",
    "                    metadata_df.loc[metadata_df['pdf_filename'] == filename, 'pdf_type'] = file_type\n",
    "\n",
    "    # Reorder columns as requested: 'date', 'year', 'month', 'page_url', 'pdf_url', 'pdf_filename', 'pdf_type', 'doc_title', 'doc_type', 'doc_number'\n",
    "    column_order = ['date', 'year', 'month', 'page_url', 'pdf_url', 'pdf_filename', 'pdf_type', 'doc_title', 'doc_type', 'doc_number']\n",
    "    \n",
    "    # Check if all columns exist before reordering\n",
    "    missing_cols = [col for col in column_order if col not in metadata_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Warning: Missing columns {missing_cols}. They will not be reordered.\")\n",
    "\n",
    "    # Reorder columns if they exist\n",
    "    metadata_df = metadata_df[column_order]\n",
    "\n",
    "    # Save the enriched metadata to a new CSV file\n",
    "    enriched_metadata_path = os.path.join(metadata_folder, f\"enriched_{metadata_csv}.csv\")\n",
    "    metadata_df.to_csv(enriched_metadata_path, index=False)\n",
    "\n",
    "    print(f\"üìë Metadata enriched and saved to: '{enriched_metadata_path}'\")\n",
    "    \n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a30ee3f-8b34-4bac-87ef-4e8e1c1b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "classification_folder = raw_data_subfolder\n",
    "metadata_folder = metadata_folder\n",
    "metadata_csv = \"cf_metadata\"  # Note: without the \".csv\" extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50003dca-70de-461b-95d6-4cc93f26beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìë Metadata enriched and saved to: 'metadata\\enriched_cf_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "updated_metadata_df = metadata_enrichment(classification_folder, metadata_folder, metadata_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50629e11-9424-4721-b548-3b35b4b00add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>page_url</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>pdf_filename</th>\n",
       "      <th>raw_data_type</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>Informe</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  year  month                                           page_url  \\\n",
       "0  2016-01-28  2016      1  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "1  2016-04-15  2016      4  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "2  2016-06-23  2016      6  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "3  2016-08-18  2016      8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "4  2016-08-18  2016      8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "5  2016-08-26  2016      8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "6  2016-10-04  2016     10  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "7  2016-12-26  2016     12  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "8  2017-05-03  2017      5  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "9  2017-05-09  2017      5  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "\n",
       "                                           pdf_url               pdf_filename  \\\n",
       "0  https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf  Informe_CF_N_001-2016.pdf   \n",
       "1  https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf  Informe_CF_N_002-2016.pdf   \n",
       "2  https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf  Informe_CF_N_003-2016.pdf   \n",
       "3  https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf  Informe_CF_N_005-2016.pdf   \n",
       "4  https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf  Informe_CF_N_004-2016.pdf   \n",
       "5  https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf  Informe_CF_N_006-2016.pdf   \n",
       "6  https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf  Informe_CF_N_007-2016.pdf   \n",
       "7  https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf  Informe_CF_N_008-2016.pdf   \n",
       "8  https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf  INFORME_N_001-2017-CF.pdf   \n",
       "9  https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf  INFORME_N_002-2017-CF.pdf   \n",
       "\n",
       "  raw_data_type                                              title doc_type  \\\n",
       "0       scanned  Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "1       scanned  Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "2       scanned  Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "3       scanned  Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "4       scanned  Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "5       scanned  Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "6       scanned  Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "7       scanned  Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "8       scanned  Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "9       scanned  Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...  Informe   \n",
       "\n",
       "   doc_number  \n",
       "0         1.0  \n",
       "1         2.0  \n",
       "2         3.0  \n",
       "3         5.0  \n",
       "4         4.0  \n",
       "5         6.0  \n",
       "6         7.0  \n",
       "7         8.0  \n",
       "8         1.0  \n",
       "9         2.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the updated DataFrame in Jupyter Notebook or JupyterLab\n",
    "updated_metadata_df.head(10)  # Display the first few rows of the updated CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a0469-4b91-4b9a-aab7-7054986addd1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328c08f-ed3d-425e-875c-c9207d4ce03d",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73aeab-af66-47bc-88f1-33400c1bf941",
   "metadata": {},
   "source": [
    "## 3. Cleaning & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb714-1ab9-4f90-a3b9-56c6f8a6c717",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359ad05-e4a9-471d-a50d-f6fff4713674",
   "metadata": {},
   "source": [
    "Why Preprocessing is Important\n",
    "1. Reduces noise: Cleaning removes unnecessary or irrelevant parts of the text, ensuring the model learns from meaningful content.\n",
    "2. Improves consistency: By standardizing punctuation and text formats, we ensure the model handles diverse text sources uniformly.\n",
    "3. Increases tokenization accuracy: Clean text is easier to tokenize, leading to more precise embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17329117",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c5c04-90af-4a5f-ae75-4ba3bb6dd14c",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4aba9a23-4b64-40b0-b793-a4932e1da0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Starting text extraction...\n",
      "\n",
      "üìÑ Processing: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\\data\\raw\\editable\\Comunicado-Congreso-vf.pdf\n",
      "\n",
      "üìÑ Page 1 of Comunicado-Congreso-vf.pdf:\n",
      "1/6 El Consejo Fiscal (CF), en el marco de su mandato de contribuir con una gesti√≥n fiscal responsable y sostenible, expresa su profunda preocupaci√≥n ante la continua promulgaci√≥n de leyes con impacto fiscal adverso que incrementan el gasto p√∫blico, sin una fuente de financiamiento identificable, o reducen la capacidad de recaudar ingresos fiscales. Esta pr√°ctica ha erosionado significativamente los fundamentos de la responsabilidad fiscal y hoy coloca al pa√≠s en una probable senda de insostenibilidad de la deuda p√∫blica que, de no corregirse urgentemente, generar√° una situaci√≥n de inestabilidad macroecon√≥mica, alzas en los costos de financiamiento para el Estado, las empresas y los hogares, y la necesidad de significativos ajustes fiscales futuros. El CF considera que esta situaci√≥n refleja la ausencia de compromiso de las autoridades con los principios b√°sicos de responsabilidad fiscal, poniendo en riesgo la fortaleza fiscal que ha sido determinante para el desempe√±o macroecon√≥mico del pa√≠s durante las √∫ltimas d√©cadas y sustento esencial del grado de inversi√≥n.\n",
      "\n",
      "De acuerdo con un reciente an√°lisis de la Direcci√≥n de Estudios Macrofiscales del Consejo Fiscal (DEM-CF) , durante el actual periodo parlamentario el Congreso ha promulgado un n√∫mero alarmante de normas con efectos negativos sobre las finanzas p√∫blicas. Dicho an√°lisis revela un sistem√°tico rechazo a las observaciones formuladas por los entes t√©cnicos del Estado y un uso cada vez m√°s frecuente del mecanismo de insistencia para la promulgaci√≥n de estos dispositivos legales. En el mismo sentido, destaca que las leyes promulgadas en los √∫ltimos a√±os conllevan costos fiscales cada vez m√°s elevados, siendo individualmente m√°s onerosas para el fisco que aquellas promulgadas en periodos legislativos anteriores. As√≠, entre agosto de 2021 y octubre de 2025 , se promulgaron 229 leyes que han reducido la capacidad de recaudar ingresos, incrementado el gasto p√∫blico o generado presiones adicionales y rigideces sobre las finanzas p√∫blicas, principalmente a trav√©s de mayores transferencias autom√°ticas a los gobiernos subnacionales (ver gr√°fico N¬∞ 1.A). Dicha cantidad de leyes con impacto fiscal adverso m√°s que triplica el n√∫mero de leyes de similar naturaleza que promulgaron los congresos anteriores entre 2006 y 2021 (68 leyes en promedio)\n",
      "\n",
      "Adem√°s, el an√°lisis de las 101 leyes promulgadas por insistencia en el presente per√≠odo legislativo refleja que estas tendr√≠an un costo fiscal anual conjunto mayor a los S/ 36 mil millones, un monto que es aproximadamente 65 veces superior al costo de las leyes promulgadas por insistencia entre 2006 y 2021 (ver gr√°fico N¬∞ 1.B). En opini√≥n del CF, estos patrones reflejan un preocupante desd√©n por la preservaci√≥n de cuentas fiscales sostenibles y un debilitamiento creciente de la institucionalidad fiscal.eee\n",
      "\n",
      "üìÑ Page 2 of Comunicado-Congreso-vf.pdf:\n",
      "2/6 Gr√°fico 1: Leyes con impacto fiscal adverso (n√∫mero de leyes)\n",
      "\n",
      "A) Leyes con impacto fiscal negativo B) Leyes con impacto fiscal por insistencia y costo fiscal\n",
      "\n",
      "El an√°lisis de la DEM-CF tambi√©n muestra que la producci√≥n legislativa reciente ha resultado singularmente nociva en t√©rminos fiscales. Solo desde agosto de 2024, las 5 leyes con mayor impacto sobre las finanzas p√∫blicas implican un costo fiscal anual cercano a S/ 22 mil millones (aproximadamente 1,8% del PBI ). Dichas normas son:\n",
      "\n",
      "1. La Ley N¬∞ 32387 que asigna mayores recursos a los gobiernos locales mediante el incremento de la asignaci√≥n al FONCOMUN, con un costo anual de S/ 8,5 mil millones . 2. La Ley N¬∞ 32335 que establece cursos de capacitaci√≥n como medida preventiva para las microempresas bajo la potestad sancionadora de SUNAT, con un costo fiscal de aproximadamente de S/ 5,3 mil millones. 3. La Ley N¬∞ 32201 que establece un r√©gimen excepcional del impuesto a la renta para promover la formalizaci√≥n de la econom√≠a y ampliar la base tributaria de contribuyentes respecto de rentas no declaradas, con un costo total de S/ 3,6 mil millones; 4. La Ley N¬∞ 32424, que establece la homologaci√≥n del incentivo CAFAE de los trabajadores del r√©gimen laboral 276 de gobiernos regionales, con un costo fiscal anual de S/ 2,6 mil millones. 5. La Ley N¬∞ 32216 que autoriza suscribir acuerdos con incidencia econ√≥mica en la negociaci√≥n colectiva a nivel descentralizada, con un costo anual estimado de S/ 2,3 mil millones en el primer a√±o y creciente en el tiempo, de acuerdo con los estimados del MEF .\n",
      "\n",
      "En opini√≥n del CF la plena implementaci√≥n de estas y otras normas no solo generar√° una presi√≥n significativa sobre el d√©ficit fiscal y la deuda p√∫blica en los pr√≥ximos a√±os, sino que adem√°s comprometer√≠a la capacidad del Estado para atender prioridades sociales y aumentar√≠a la vulnerabilidad del pa√≠s frente a choques econ√≥micos, poniendo en riesgo la estabilidad macroecon√≥mica.\n",
      "\n",
      "üìÑ Page 3 of Comunicado-Congreso-vf.pdf:\n",
      "3/6 Adicionalmente a las normas ya promulgadas, el CF advierte que el Congreso viene tramitando un elevado n√∫mero de iniciativas legislativas que, de promulgarse, profundizar√≠an las presiones sobre las finanzas p√∫blicas y consolidar√≠an una trayectoria de gasto incompatible no solo con las reglas fiscales vigentes sino, en un horizonte de mediano y largo plazo, con d√©ficits fiscales que permitan estabilizar la trayectoria creciente de la deuda p√∫blica bruta, deviniendo en una senda fiscal insostenible. A la fecha se han identificado 352 iniciativas legislativas en tr√°mite con potencial impacto fiscal. Si solo consideramos el impacto fiscal de las 10 iniciativas fiscalmente m√°s costosas, encontramos que estas concentran un costo anual superior a S/ 25 mil millones (aproximadamente 2,0% del PBI).\n",
      "\n",
      "Las presiones fiscales ya generadas por leyes promulgadas, pero a√∫n no incorporadas en presupuestos ni en el Marco Macroecon√≥mico Multianual (MMM), bajo supuestos conservadores, ya sugieren d√©ficits fiscales inconsistentes con la estabilizaci√≥n de la trayectoria de deuda p√∫blica, deviniendo en una senda fiscal insostenible (ver gr√°fico N¬∞ 2.A). Si a ello se suman las iniciativas con impacto fiscal actualmente en proceso de aprobaci√≥n en el Congreso de la Rep√∫blica, dicha senda se tornar√≠a explosiva.\n",
      "\n",
      "Dado que estas medidas generan impactos fiscales adversos con car√°cter permanente o de largo plazo, su impacto sobre la deuda p√∫blica es acumulativo en el tiempo, raz√≥n por la cual, cuando sean evidentes sus consecuencias agregadas, la senda fiscal ya podr√≠a encontrarse irreversiblemente comprometida, con la deuda p√∫blica adoptando una tendencia creciente e insostenible, seg√∫n simulaciones de la DEM-STCF . Hacia 2036, la deuda p√∫blica podr√≠a ser superior a la prevista en el MMM en m√°s de 40 puntos porcentuales del PBI (ver gr√°fico N¬∞ 2.B) . Esto evidencia que se est√° gestando un severo problema de insostenibilidad fiscal que tendr√° que ser enfrentado por las pr√≥ximas administraciones de gobierno, el cual se explica tanto por las leyes ya promulgadas, como por el riesgo latente de que el actual parlamento continue promulgando medidas que comprometan la estabilidad macroecon√≥mica y trasladen a la poblaci√≥n el costo de decisiones adoptadas sin un an√°lisis de capacidad presupuestaria.\n",
      "\n",
      "Entre las iniciativas legislativas a√∫n en tr√°mite, el CF identifica como especialmente preocupantes aquellas que reducen la capacidad de recaudar ingresos fiscales mediante la creaci√≥n de beneficios tributarios y las que incrementan de manera permanente y r√≠gida el gasto en personal y pensiones. La DEM-CF ha identificado al menos 37 dict√°menes que crean beneficios tributarios, entre ellos el que permitir√≠a deducir nuevos gastos de la base imponible del Impuesto a la Renta de personas naturales, con un costo anual estimado por el MEF de S/ 2 082 millones (equivalente al 11% de la recaudaci√≥n de este impuesto en 2024). Esta medida debilitar√≠a a√∫n m√°s el nivel de recaudaci√≥n de un tributo progresivo, que ya es bajo en comparaci√≥n con otros pa√≠ses de la regi√≥n y del mundo.\n",
      "\n",
      "üìÑ Page 4 of Comunicado-Congreso-vf.pdf:\n",
      "4/6 Gr√°fico 2: Impacto fiscal de la legislaci√≥n sobre las cuentas fiscales, 2026-2036 (% del PBI) A) D√©ficit fiscal B) Deuda p√∫blica\n",
      "\n",
      "Por el lado del gasto, destacan 2 iniciativas cuyo costo anual combinado bordear√≠a los S/ 9 mil millones. La primera es la aut√≥grafa observada por el Poder Ejecutivo que propone homologar las pensiones de los maestros cesantes y jubilados a la remuneraci√≥n √≠ntegra mensual de la primera escala magisterial de los docentes activos, lo que generar√≠a un gasto anual de S/ 5 670 millones y reabrir√≠a la c√©dula viva. La segunda es el dictamen que propone otorgar gratificaciones en julio y diciembre, as√≠ como el pago de Compensaci√≥n por Tiempo de Servicios (CTS) a los servidores del r√©gimen CAS, con un costo anual de S/ 3 050 millones. Ambas iniciativas consolidar√≠an compromisos permanentes pr√°cticamente imposibles de revertir, restringiendo a√∫n m√°s el espacio fiscal futuro.\n",
      "\n",
      "El CF recuerda que estas medidas incrementar√°n la rigidez presupuestaria y restar√°n espacio fiscal para la implementaci√≥n de pol√≠ticas p√∫blicas prioritarias . As√≠ por ejemplo, la plena implementaci√≥n de las leyes promulgadas desde agosto de 2024 y de las iniciativas legislativas en tr√°mite con impacto fiscal adverso sobrepasar√≠a en un solo a√±o todo el espacio de incremento del gasto no financiero del Gobierno General previsto en el horizonte de proyecci√≥n del Marco Macroecon√≥mico Multianual vigente . En este contexto, el CF resalta la importancia de que el presupuesto p√∫blico sea formulado y ejecutado bajo un enfoque de arriba hacia abajo (‚Äú top-down ‚Äù), en el cual ‚Äú el Ejecutivo primero determina las metas agregadas para las finanzas p√∫blicas (d√©ficits, gastos e ingresos) en funci√≥n de los objetivos de pol√≠tica fiscal de mediano y largo plazo y de las previsiones macroecon√≥micas. Luego, estos techos globales son distribuidos entre los distintos niveles de gobierno, sectores y pliegos, en funci√≥n de los compromisos existentes, prioridades pol√≠ticas y nuevas intervenciones ‚Äù .\n",
      "\n",
      "El CF recuerda tambi√©n que la promulgaci√≥n de normas con altos costos fiscales y sin evaluaci√≥n t√©cnica s√≥lida no constituye la √∫nica fuente de presi√≥n para las finanzas p√∫blicas, sino que a estas\n",
      "\n",
      "üìÑ Page 5 of Comunicado-Congreso-vf.pdf:\n",
      "5/6 se suman las presiones que generan los anuncios del Poder Ejecutivo de llevar a cabo de forma simult√°nea diversos megaproyectos de infraestructura con un alto componente de cofinanciamiento estatal. En este contexto de amplia exposici√≥n a riesgos, el CF reitera la importancia de priorizar el proceso de consolidaci√≥n fiscal, con el fin de recuperar activos l√≠quidos, estabilizar la trayectoria de endeudamiento p√∫blico y ce√±irse a las limitaciones del espacio fiscal disponible en el mediano y largo plazo consistentes con la sostenibilidad fiscal. De esta forma, el Estado estar√° en capacidad, como se ha hecho en el pasado, de enfrentar la materializaci√≥n de eventos con impactos negativos significativos, tales como desastres naturales, crisis internacionales, entre otros.\n",
      "\n",
      "El CF subraya que la responsabilidad por la proliferaci√≥n de leyes con impacto fiscal adverso no recae √∫nicamente en el Poder Legislativo. El Poder Ejecutivo tambi√©n tiene la obligaci√≥n de salvaguardar la sostenibilidad fiscal, formulando de manera oportuna observaciones t√©cnicas fundamentadas, transparentando los costos fiscales de las iniciativas y, cuando corresponda, de haber fundamentos constitucionales, presentando acciones de inconstitucionalidad ante el Tribunal Constitucional. La omisi√≥n de estas acciones genera presiones adicionales sobre las cuentas fiscales y debilita la disciplina presupuestaria. En repetidas ocasiones el Ejecutivo ha renunciado a ejercer esta oposici√≥n, tal es el caso que, en el presente periodo parlamentario, m√°s del 56% del total de leyes con impacto fiscal identificadas (128 leyes), no fueron observadas por el Poder Ejecutivo. Asimismo, a pesar del elevado n√∫mero de leyes que generan incrementos permanentes en el gasto p√∫blico, desde 2021, durante la pasada administraci√≥n de gobierno solo se presentaron 3 demandas de inconstitucionalidad (la √∫ltima en diciembre de 2023). Incluso normas con importantes costos fiscales fueron promulgadas sin objeci√≥n del Ejecutivo, entre las cuales se encuentran: (i) La Ley N¬∞ 31495 que ordena el pago del beneficio por preparaci√≥n de clases a los docentes, que implicar√≠a un gasto (transitorio) de aproximadamente S/ 40 mil millones; (ii) La ley N¬∞ 32123, modificada recientemente mediante la ley N¬∞ 32445, que aprob√≥ la reforma del sistema previsional, que brinda entre otros el beneficio de la pensi√≥n m√≠nima para los afiliados al Sistema Privado de Pensiones y sobre la cual el MEF estim√≥ que podr√≠a generar incrementos en el gasto p√∫blico de hasta 1,1% del PBI , y (iii) la Ley N¬∞ 32434, que otorga beneficios tributarios al sector agrario, pese a contar con opini√≥n en contra del MEF y contravenir principios elementales de pol√≠tica tributaria y la Norma VII del C√≥digo Tributario .\n",
      "\n",
      "De igual manera, el CF insta al Tribunal Constitucional (TC) a considerar c√≥mo garantizar el cumplimiento efectivo de la prohibici√≥n constitucional que impide al Congreso promulgar medidas que generen gasto p√∫blico. En ese sentido resulta necesario precisar la interpretaci√≥n que limita dicha prohibici√≥n √∫nicamente a los casos que afectan el equilibrio financiero del presupuesto vigente y reafirmar, como m√≠nimo, que para que una norma que genere gasto p√∫blico no vulnere la Constituci√≥n debe existir una coordinaci√≥n real y efectiva entre los poderes Ejecutivo y Legislativo . El Consejo Fiscal considera que el TC podr√≠a evaluar la incorporaci√≥n de criterios de equilibrio presupuestario y sostenibilidad fiscal en sus decisiones y por lo tanto expresa su disposici√≥n a\n",
      "\n",
      "üìÑ Page 6 of Comunicado-Congreso-vf.pdf:\n",
      "6/6 desempe√±ar el rol de ‚Äú amicus curiae ‚Äù en las causas en las que pueda ser invitado, conforme a sus funciones y a la legislaci√≥n correspondiente.\n",
      "\n",
      "Por estas razones, el CF reitera su exhortaci√≥n a todas las autoridades con capacidad de decisi√≥n en la emisi√≥n o interpretaci√≥n de normativa con incidencia fiscal ‚ÄîCongreso de la Rep√∫blica, Poder Ejecutivo y Tribunal Constitucional‚Äî a evitar la promulgaci√≥n de medidas que contin√∫en generando presiones sobre las cuentas p√∫blicas, y a reevaluar la conveniencia y constitucionalidad de m√∫ltiples leyes ya promulgadas, a fin de modificarlas o derogarlas en cuanto corresponda.\n",
      "\n",
      "M√°s a√∫n, el CF considera indispensable que se mitigue la presi√≥n ya existente sobre las cuentas fiscales mediante la interposici√≥n de acciones de inconstitucionalidad sobre aquellas leyes ya promulgadas con impacto fiscal significativo que presumiblemente vulneren la Constituci√≥n. Asimismo, en adelante, recomienda que toda norma con impacto fiscal cuente con una adecuada cuantificaci√≥n de su costo y que se precise de manera expl√≠cita la fuente de recursos con la que se financiar√°, ya sea mediante un aumento de impuestos o una reducci√≥n de gastos en otras √°reas. El CF recuerda que la situaci√≥n de las finanzas p√∫blicas se ha venido deteriorando de manera progresiva en los √∫ltimos a√±os, como lo evidencian los aumentos significativos de la deuda p√∫blica bruta y, sobre todo, la neta, el mayor peso del pago de intereses en el presupuesto, los incumplimientos de las reglas fiscales por dos a√±os consecutivos y las rebajas en las calificaciones crediticias de la deuda soberana. De continuar en esta trayectoria, el pa√≠s enfrentar√° crecientes riesgos de vulnerabilidad e insostenibilidad fiscal, con consecuencias que recaer√°n sobre la poblaci√≥n en su conjunto.\n",
      "\n",
      "Finalmente, el CF enfatiza que la disciplina fiscal constituye un pilar esencial para preservar la estabilidad macroecon√≥mica, atraer inversi√≥n privada, impulsar el crecimiento econ√≥mico, generar empleo, reducir la pobreza y cerrar brechas sociales. Es fundamental, adem√°s, para mantener la calificaci√≥n crediticia de grado de inversi√≥n del pa√≠s. Desatender este principio implicar√≠a retroceder d√©cadas de avances en estabilidad macroecon√≥mica, exponer al pa√≠s a una trayectoria creciente e insostenible de deuda p√∫blica, mayores costos de financiamiento y la necesidad de eventuales ajustes fiscales significativos que inevitablemente afectar√°n a la ciudadan√≠a, a trav√©s de una menor cobertura y calidad de bienes y servicios p√∫blicos, ya de por si insatisfactoria, y en la capacidad del Estado de cerrar progresivamente las brechas sociales y de infraestructura. Por ello, el CF nuevamente exhorta a los legisladores, a las autoridades del Poder Ejecutivo y a los int√©rpretes de la Constituci√≥n a asumir con responsabilidad y sentido de urgencia la tarea de proteger la sostenibilidad fiscal del pa√≠s, reevaluando normas ya promulgadas e iniciativas en curso, evitando de esta manera que decisiones de corto plazo comprometan el bienestar de largo plazo de los peruanos .\n",
      "\n",
      "Lima, 21 de octubre de 2025.\n",
      "\n",
      "CONSEJO FISCAL DEL PER√ö\n",
      "üìÇ Text saved to JSON file: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\\data\\raw\\editable\\Comunicado-Congreso-vf.json\n",
      "\n",
      "‚úÖ Extraction complete. Total pages processed: 6\n",
      "‚è±Ô∏è Time taken: 0.97 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import json\n",
    "from time import time as timer\n",
    "\n",
    "def extract_text_from_single_pdf(file_path, FONT_MIN=11.0, FONT_MAX=11.9, exclude_bold=True, vertical_threshold=10):\n",
    "    \"\"\"\n",
    "    Extracts raw text from a single editable PDF for testing purposes, recognizing new paragraphs based on vertical spacing.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the single PDF file to be processed\n",
    "    - FONT_MIN: float, minimum font size to consider (default 11.0)\n",
    "    - FONT_MAX: float, maximum font size to consider (default 11.9)\n",
    "    - exclude_bold: bool, whether to exclude bold text (default True)\n",
    "    - vertical_threshold: int, the minimum vertical space between lines to consider as a new paragraph (default 10)\n",
    "\n",
    "    Prints the extracted text from the PDF for inspection and saves it to a JSON file in the same folder.\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "\n",
    "    print(\"üß† Starting text extraction...\\n\")\n",
    "    all_records = []\n",
    "\n",
    "    try:\n",
    "        print(f\"üìÑ Processing: {file_path}\")\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                # Extract words with their size, vertical position, and fontname\n",
    "                words = page.extract_words(extra_attrs=[\"size\", \"top\", \"fontname\"])\n",
    "\n",
    "                # Filter out bold words if exclude_bold is True\n",
    "                clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX and (\"Bold\" not in w[\"fontname\"] if exclude_bold else True)]\n",
    "                if not clean_words:\n",
    "                    continue\n",
    "\n",
    "                # Initialize variables for paragraph detection\n",
    "                page_text = []\n",
    "                paragraph_lines = []\n",
    "                last_top = None  # Keeps track of the vertical position of the previous word\n",
    "\n",
    "                # Process each word and check vertical spacing between lines\n",
    "                for word in clean_words:\n",
    "                    line_text = word[\"text\"]\n",
    "                    top = word[\"top\"]  # Vertical position of the word\n",
    "\n",
    "                    # Detect if we have a new paragraph based on vertical spacing\n",
    "                    if last_top is not None and top - last_top > vertical_threshold:\n",
    "                        # New paragraph detected based on large vertical space\n",
    "                        if paragraph_lines:\n",
    "                            page_text.append(\" \".join(paragraph_lines))  # Add the previous paragraph\n",
    "                        paragraph_lines = [line_text]  # Start a new paragraph\n",
    "                    else:\n",
    "                        paragraph_lines.append(line_text)  # Continue adding to the current paragraph\n",
    "                    \n",
    "                    last_top = top  # Update the last vertical position\n",
    "\n",
    "                # Add the last paragraph if exists\n",
    "                if paragraph_lines:\n",
    "                    page_text.append(\" \".join(paragraph_lines))\n",
    "\n",
    "                # Combine all extracted text for this page into one string with '\\n\\n' separating paragraphs\n",
    "                full_page_text = \"\\n\\n\".join(page_text)\n",
    "\n",
    "                # üö´ Stop extraction at \"Anexo\"\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", full_page_text)\n",
    "                if match:\n",
    "                    full_page_text = full_page_text[:match.start()].strip()\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                all_records.append({\n",
    "                    \"filename\": os.path.basename(file_path),\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": full_page_text\n",
    "                })\n",
    "        \n",
    "        if not all_records:\n",
    "            print(\"‚ö†Ô∏è No text extracted from the PDF.\")\n",
    "            return\n",
    "\n",
    "        # Print extracted text for inspection\n",
    "        for record in all_records:\n",
    "            print(f\"\\nüìÑ Page {record['page']} of {record['filename']}:\")\n",
    "            print(record['text'])\n",
    "\n",
    "        # Save extracted text to JSON file in the same path as the PDF\n",
    "        json_filename = os.path.splitext(os.path.basename(file_path))[0] + \".json\"\n",
    "        json_file_path = os.path.join(os.path.dirname(file_path), json_filename)\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_records, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"üìÇ Text saved to JSON file: {json_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\n‚úÖ Extraction complete. Total pages processed: {len(all_records)}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "\n",
    "# Example usage: specify the PDF file path\n",
    "file_path = r\"C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\\data\\raw\\editable\\Comunicado-Congreso-vf.pdf\"\n",
    "extract_text_from_single_pdf(file_path, FONT_MIN=11.0, FONT_MAX=11.9, exclude_bold=False, vertical_threshold=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdb0fd-7482-4b5a-90da-bb24cfcae820",
   "metadata": {},
   "source": [
    "# ACTUAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c63f3400-a377-48e0-9eb2-72fcfb648529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from time import time as timer\n",
    "\n",
    "def extract_text_from_pdfs(editable_folder, FONT_MIN=11.0, FONT_MAX=11.9, raw_text_storage_folder=None):\n",
    "    \"\"\"\n",
    "    Extracts raw text from editable PDF documents and optionally saves it to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - editable_folder: str, path to the folder containing editable PDFs\n",
    "    - FONT_MIN: float, minimum font size to consider (default 11.0)\n",
    "    - FONT_MAX: float, maximum font size to consider (default 11.9)\n",
    "    - raw_text_storage_folder: str, folder to save the JSON file (optional)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted text from each PDF\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "\n",
    "    print(\"üß† Starting text extraction...\\n\")\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(editable_folder) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(editable_folder, filename)\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÑ Processing: {idx}. {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    # Extract words with their size and vertical position\n",
    "                    words = page.extract_words(extra_attrs=[\"size\", \"top\"])\n",
    "                    clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX]\n",
    "                    if not clean_words:\n",
    "                        continue\n",
    "\n",
    "                    # Group words by their vertical position to form lines\n",
    "                    lines_dict = {}\n",
    "                    for word in clean_words:\n",
    "                        line_top = round(word[\"top\"], 1)\n",
    "                        lines_dict.setdefault(line_top, []).append(word[\"text\"])\n",
    "\n",
    "                    lines = [\n",
    "                        \" \".join(words).strip()\n",
    "                        for _, words in sorted(lines_dict.items())\n",
    "                        if words\n",
    "                    ]\n",
    "\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    page_text = \"\\n\".join(lines)\n",
    "\n",
    "                    # üö´ Stop extraction at \"Anexo\"\n",
    "                    match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                    if match:\n",
    "                        page_text = page_text[:match.start()].strip()\n",
    "                        print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"text\": page_text\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\n‚úÖ Extraction complete. Total pages: {len(df)}\")\n",
    "    print(f\"üíæ Extraction complete.\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "    \n",
    "    # Optionally save to JSON\n",
    "    if raw_text_storage_folder:\n",
    "        if not os.path.exists(raw_text_storage_folder):\n",
    "            os.makedirs(raw_text_storage_folder)\n",
    "        json_file_path = os.path.join(raw_text_storage_folder, \"editable_pdf_extracted_text.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_records, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"üìÇ Text saved to JSON file: {json_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f62a0378-fb88-4a87-927f-7dd18a2384b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Starting text extraction...\n",
      "\n",
      "üìÑ Processing: 1. 2-ComunicadoCF-RetiroAFP-1.pdf\n",
      "üìÑ Processing: 2. CF-Informe-IAPM21-vF.pdf\n",
      "üìÑ Processing: 3. CF-Informe-IP21-vF.pdf\n",
      "üìÑ Processing: 4. CF-Informe-MMM2124-cNotaAclaratoria-28-de-agosto-VF-publicada.pdf\n",
      "üìÑ Processing: 5. CF-Informe-MMM2124-publicado-enviado-CF-VF.pdf\n",
      "üìÑ Processing: 6. CF-Pronunciamiento-DU-032-2019-VERSI√ìN-FINAL.pdf\n",
      "üìÑ Processing: 7. CF-Pronunciamiento-MMM-2019-2022_15_8_2018-enviada-al-MEF-1.pdf\n",
      "üìÑ Processing: 8. CF-Pronunciamiento-MMM-2020-2023-Versi√≥n-Final-9pm.pdf\n",
      "üìÑ Processing: 9. CF-Pronunciamiento.pdf\n",
      "üìÑ Processing: 10. Comunicado-01-2025-ReglasFiscales-vf-1.pdf\n",
      "üìÑ Processing: 11. Comunicado-012024-RiesgosFiscales-v4-1.pdf\n",
      "üìÑ Processing: 12. Comunicado-01_2023-TC-intereses-moratorios-vf.pdf\n",
      "üìÑ Processing: 13. Comunicado-02-InciativasLegislativas-vf.pdf\n",
      "üìÑ Processing: 14. Comunicado-022023-CF-v4.pdf\n",
      "üìÑ Processing: 15. Comunicado-022024-vf.pdf\n",
      "üìÑ Processing: 16. Comunicado-03-2024-Reformapensiones-vf-2.pdf\n",
      "üìÑ Processing: 17. Comunicado-03-IAPM-vf.pdf\n",
      "üìÑ Processing: 18. Comunicado-04Infraestructura-v6.pdf\n",
      "üìÑ Processing: 19. Comunicado-05-2022-Art79CPP-vf.pdf\n",
      "üìÑ Processing: 20. Comunicado-05-2025CF-vf.pdf\n",
      "üìÑ Processing: 21. Comunicado-3DeudaMagisterio-1.pdf\n",
      "üìÑ Processing: 22. Comunicado-CAS-vf.pdf\n",
      "üìÑ Processing: 23. Comunicado-CF-161220.pdf\n",
      "üìÑ Processing: 24. Comunicado-CSsxDUs-v9_.pdf\n",
      "üìÑ Processing: 25. Comunicado-CSsxDUs100-v4.pdf\n",
      "üìÑ Processing: 26. Comunicado-Congreso-vf.pdf\n",
      "üìÑ Processing: 27. Comunicado-DU-112.pdf\n",
      "üìÑ Processing: 28. Comunicado-LeyONP-VersioÃÅn-Final-Publicada-convertido.pdf\n",
      "üìÑ Processing: 29. Comunicado-N-03-2021-CF.pdf\n",
      "üìÑ Processing: 30. Comunicado-N¬∞-01-2022-CF.pdf\n",
      "üìÑ Processing: 31. Comunicado-PIPs-G2G-vf.pdf\n",
      "üìÑ Processing: 32. Comunicado-Presupuesto-y-Otros-vf-1.pdf\n",
      "üõë 'Anexo' detected on page 7. Truncating content.\n",
      "üìÑ Processing: 33. Comunicado-Responsabilida-Fiscal-VF.pdf\n",
      "üìÑ Processing: 34. Comunicado-del-Consejo-Fiscal-sobre-el-DU-023-2022-vf.pdf\n",
      "üìÑ Processing: 35. Comunicado042024-VF.pdf\n",
      "üìÑ Processing: 36. Informe-DCRF2023-vf.pdf\n",
      "üìÑ Processing: 37. Informe-DCRF2024-vf.pdf\n",
      "üìÑ Processing: 38. Informe-DL1621-vf.pdf\n",
      "üìÑ Processing: 39. Informe-Escenarios-_8.6.2020_FINAL.pdf\n",
      "üìÑ Processing: 40. Informe-GobSubnacionales-2021-vF.pdf\n",
      "üìÑ Processing: 41. Informe-N¬∞-005-2020-CF.pdf\n",
      "üìÑ Processing: 42. Informe-PLReglasFiscales-2022.pdf\n",
      "üìÑ Processing: 43. Informe-ReglasGRsLs-VF-publicado.pdf\n",
      "üìÑ Processing: 44. Informe-sobre-MMM-26-29-vf.pdf\n",
      "üìÑ Processing: 45. Informe01-OpinionCFIAPM2023.pdf\n",
      "üìÑ Processing: 46. InformeCF-IAPM-VF.pdf\n",
      "üìÑ Processing: 47. Opinion-MMM2023-2026-cNotaAclaratoria.pdf\n",
      "üìÑ Processing: 48. Pronunciamiento-CF-DCRF-2017-12julio-enviada-1.pdf\n",
      "üìÑ Processing: 49. Pronunciamiento-CF-DCRF-2018-VF-publicada.pdf\n",
      "üìÑ Processing: 50. Pronunciamiento-COVID-CF-VF.pdf\n",
      "üìÑ Processing: 51. Pronunciamiento-DCRF-2020-publicar.pdf\n",
      "üìÑ Processing: 52. Pronunciamiento-DU-031-subnacionalvf.pdf\n",
      "üìÑ Processing: 53. Pronunciamiento-DU-079-ReglasFiscales-2022-vf.pdf\n",
      "üìÑ Processing: 54. Pronunciamiento-FinanzasPublicas2022-vF.pdf\n",
      "üõë 'Anexo' detected on page 13. Truncating content.\n",
      "üìÑ Processing: 55. Pronunciamiento-IAPM2022-vf.pdf\n",
      "üìÑ Processing: 56. Pronunciamiento-IAPM24-27-vf.pdf\n",
      "üìÑ Processing: 57. Pronunciamiento-IAPM25-28-VF.pdf\n",
      "üìÑ Processing: 58. Pronunciamiento-MMM-24-27-VF-nota-aclaratoria-VF.pdf\n",
      "üìÑ Processing: 59. Pronunciamiento-MMM2022-vf.pdf\n",
      "üìÑ Processing: 60. Pronunciamiento-Subnacional2024-vf.pdf\n",
      "üõë 'Anexo' detected on page 8. Truncating content.\n",
      "üõë 'Anexo' detected on page 9. Truncating content.\n",
      "üìÑ Processing: 61. Pronunciamiento-pMMM-25-28-vf.pdf\n",
      "üìÑ Processing: 62. Pronunciamiento-suspension-de-reglas-fiscales-VF.pdf\n",
      "üìÑ Processing: 63. PronunciamientoDCRF-RFSN-2021-vf.pdf\n",
      "üìÑ Processing: 64. Pronunciamiento_Subnacional_2024_240624.pdf\n",
      "üõë 'Anexo' detected on page 7. Truncating content.\n",
      "üõë 'Anexo' detected on page 9. Truncating content.\n",
      "\n",
      "‚úÖ Extraction complete. Total pages: 424\n",
      "üíæ Extraction complete.\n",
      "‚è±Ô∏è Time taken: 67.63 seconds\n",
      "üìÇ Text saved to JSON file: data\\raw\\extracted_text.json\n"
     ]
    }
   ],
   "source": [
    "df = extract_text_from_pdfs(\n",
    "    editable_folder=f\"{raw_data_subfolder}/editable\",\n",
    "    FONT_MIN=11.0,\n",
    "    FONT_MAX=11.9,\n",
    "    raw_text_storage_folder=raw_data_subfolder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc4c7bfb-8f75-42d1-839c-881cc2ee01b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-ComunicadoCF-RetiroAFP-1.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Direcci√≥n de Estudios Macrofiscales ‚Äì STCF\\nCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-ComunicadoCF-RetiroAFP-1.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Direcci√≥n de Estudios Macrofiscales ‚Äì STCF\\nde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CF-Informe-IAPM21-vF.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>El presente documento contiene la opini√≥n cole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CF-Informe-IAPM21-vF.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>del Producto Bruto Interno (PBI), igual a lo p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CF-Informe-IAPM21-vF.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>ya ven√≠an enfrentando antes de la pandemia. Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Pronunciamiento_Subnacional_2024_240624.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>acerca de que el incremento de la deuda de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Pronunciamiento_Subnacional_2024_240624.pdf</td>\n",
       "      <td>6</td>\n",
       "      <td>4.\\nEn cuanto al cumplimiento de las reglas fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Pronunciamiento_Subnacional_2024_240624.pdf</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Pronunciamiento_Subnacional_2024_240624.pdf</td>\n",
       "      <td>8</td>\n",
       "      <td>Gobiernos locales que solo incumplen la regla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Pronunciamiento_Subnacional_2024_240624.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  page  \\\n",
       "0                 2-ComunicadoCF-RetiroAFP-1.pdf     1   \n",
       "1                 2-ComunicadoCF-RetiroAFP-1.pdf     2   \n",
       "2                       CF-Informe-IAPM21-vF.pdf     1   \n",
       "3                       CF-Informe-IAPM21-vF.pdf     2   \n",
       "4                       CF-Informe-IAPM21-vF.pdf     3   \n",
       "..                                           ...   ...   \n",
       "419  Pronunciamiento_Subnacional_2024_240624.pdf     5   \n",
       "420  Pronunciamiento_Subnacional_2024_240624.pdf     6   \n",
       "421  Pronunciamiento_Subnacional_2024_240624.pdf     7   \n",
       "422  Pronunciamiento_Subnacional_2024_240624.pdf     8   \n",
       "423  Pronunciamiento_Subnacional_2024_240624.pdf     9   \n",
       "\n",
       "                                                  text  \n",
       "0    Direcci√≥n de Estudios Macrofiscales ‚Äì STCF\\nCo...  \n",
       "1    Direcci√≥n de Estudios Macrofiscales ‚Äì STCF\\nde...  \n",
       "2    El presente documento contiene la opini√≥n cole...  \n",
       "3    del Producto Bruto Interno (PBI), igual a lo p...  \n",
       "4    ya ven√≠an enfrentando antes de la pandemia. Ad...  \n",
       "..                                                 ...  \n",
       "419  acerca de que el incremento de la deuda de la ...  \n",
       "420  4.\\nEn cuanto al cumplimiento de las reglas fi...  \n",
       "421                                                     \n",
       "422  Gobiernos locales que solo incumplen la regla ...  \n",
       "423                                                     \n",
       "\n",
       "[424 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c95c8-dc4b-472f-874c-7290bc4d471b",
   "metadata": {},
   "source": [
    "# segmentaci√≥n por sentido ling√º√≠stico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b607c22",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def process_editable_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", output_csv=\"raw_editable_pdfs_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Processes editable PDF documents, extracting structured paragraphs and saving to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - editable_folder: str, path to the folder containing editable PDFs\n",
    "    - metadata_csv_path: str, path to the CSV file with document metadata\n",
    "    - output_folder: str, output folder to save output csv\n",
    "    - output_csv: str, output CSV filename to save extracted paragraphs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted structured text and metadata\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "\n",
    "    print(\"üß† Starting paragraph extraction...\\n\")\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÑ Processing: {idx}. {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                paragraph_counter = 1\n",
    "                anexo_found = False\n",
    "\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    if anexo_found:\n",
    "                        break\n",
    "\n",
    "                    # Extract words with their size and vertical position\n",
    "                    words = page.extract_words(extra_attrs=[\"size\", \"top\"])\n",
    "                    FONT_MIN = 11.0\n",
    "                    FONT_MAX = 11.9\n",
    "                    clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX]\n",
    "                    if not clean_words:\n",
    "                        continue\n",
    "\n",
    "                    # Group words by their vertical position to form lines\n",
    "                    lines_dict = {}\n",
    "                    for word in clean_words:\n",
    "                        line_top = round(word[\"top\"], 1)\n",
    "                        lines_dict.setdefault(line_top, []).append(word[\"text\"])\n",
    "\n",
    "                    lines = [\n",
    "                        \" \".join(words).strip()\n",
    "                        for _, words in sorted(lines_dict.items())\n",
    "                        if words\n",
    "                    ]\n",
    "\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    page_text = \"\\n\".join(lines)\n",
    "\n",
    "                    # üö´ Stop extraction at \"Anexo\"\n",
    "                    match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                    if match:\n",
    "                        page_text = page_text[:match.start()].strip()\n",
    "                        anexo_found = True\n",
    "                        print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                    # Paragraph segmentation\n",
    "                    lines = page_text.strip().split(\"\\n\")\n",
    "                    lines = [line.strip() for line in lines if line.strip()]\n",
    "                    paragraph_lines = []\n",
    "\n",
    "                    for i, line in enumerate(lines):\n",
    "                        is_new_paragraph = (\n",
    "                            line.startswith(\"‚Ä¢\")\n",
    "                            or line.startswith(\"‚û¢\")\n",
    "                            or (i > 0 and lines[i - 1].strip().endswith(\".\"))\n",
    "                            or (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                        )\n",
    "\n",
    "                        if is_new_paragraph:\n",
    "                            if paragraph_lines:\n",
    "                                all_records.append({\n",
    "                                    \"filename\": filename,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"paragraph_id\": paragraph_counter,\n",
    "                                    \"text\": \" \".join(paragraph_lines).strip()\n",
    "                                })\n",
    "                                paragraph_counter += 1\n",
    "                            paragraph_lines = [line]\n",
    "                        else:\n",
    "                            paragraph_lines.append(line)\n",
    "\n",
    "                    if paragraph_lines:\n",
    "                        all_records.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"page\": page_num,\n",
    "                            \"paragraph_id\": paragraph_counter,\n",
    "                            \"text\": \" \".join(paragraph_lines).strip()\n",
    "                        })\n",
    "                        paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    # Merge with metadata\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df = df[[\"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\n‚úÖ Extraction complete. Total paragraphs: {len(df)}\")\n",
    "    print(f\"üíæ Saved to: {output_csv}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a354e5d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable = process_editable_pdfs(\"editable_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f3936",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbd2f",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af8df4a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd8030",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "\"\"\"\n",
    "Procesa PDFs escaneados usando OCR para extraer p√°rrafos por p√°gina y metadatos.\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Ruta a la carpeta con archivos PDF escaneados.\n",
    "    dpi (int): Resoluci√≥n al convertir PDF a imagen.\n",
    "    lang (str): Idioma para el OCR ('spa' para espa√±ol, 'eng' para ingl√©s, etc.).\n",
    "\n",
    "Returns:\n",
    "    pd.DataFrame: DataFrame con columnas:\n",
    "        ['filename', 'year', 'date', 'announcement', 'page', 'paragraph_id', 'text']\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eec680",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Puede tardar un poco m√°s. Se excluye pies de pagina con detecci√≥n visual de lineas que marcan el inicio. Se p√°ginas de anexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e58f5",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.2, y_range=(0.5, 0.85), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line that likely marks the beginning of the footer in a scanned PDF page.\n",
    "    \n",
    "    Parameters:\n",
    "        image (PIL.Image): Page image to analyze.\n",
    "        min_length_ratio (float): Minimum line length relative to image width.\n",
    "        y_range (tuple): Vertical range (as a proportion of image height) where footer lines are expected.\n",
    "        debug_path (str): Optional path to save debug image with the detected line.\n",
    "    \n",
    "    Returns:\n",
    "        int: Y-coordinate to crop the image above the footer line, or image height if no line is found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=100,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 3 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # Default: no footer line detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c36df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a635301",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Major function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac228",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.17, y_range=(0.55, 0.90), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line likely indicating the beginning of the footer in scanned PDFs.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Page image.\n",
    "        min_length_ratio (float): Minimum length of line relative to image width.\n",
    "        y_range (tuple): Vertical range to search (proportional to height).\n",
    "        debug_path (str): Optional file path to save a debug image with detected line.\n",
    "\n",
    "    Returns:\n",
    "        int: Y-coordinate of the detected line, or image height if none found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=80,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 5 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # No line detected ‚Üí return full height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a24cd2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Processing for Scanned PDFs ===\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from scanned PDFs using OCR, excluding footers and annexes.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Folder with scanned PDFs.\n",
    "        metadata_csv_path (str): Path to metadata CSV file.\n",
    "        dpi (int): Resolution used to convert PDFs to images.\n",
    "        lang (str): OCR language code.\n",
    "        debug (bool): Whether to save debug images with detected lines.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Paragraph-level extracted data.\n",
    "    \"\"\"\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        doc_title = row.get(\"doc_title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", doc_title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                debug_path = None\n",
    "                if debug:\n",
    "                    os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                    debug_path = f\"debug_lines/{filename}_page_{page_num}.png\"\n",
    "\n",
    "                cut_y = detect_cut_line_y(image, debug_path=debug_path)\n",
    "                cropped_img = image.crop((0, 0, image.width, cut_y))\n",
    "\n",
    "                page_text = pytesseract.image_to_string(cropped_img, lang=lang)\n",
    "\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # Stop at 'Anexo'\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51a39",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === Run and Save ===\n",
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"scanned_pdf\",\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deef5c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93432a87",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# NUEVA UTILIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6963",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def filter_text_body_dynamic(\n",
    "    image,\n",
    "    size_threshold_percentile=40,\n",
    "    merge_dist=20,\n",
    "    expand_margin=10,\n",
    "    debug_path=None,\n",
    "    audit_csv_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Filtra regiones de texto fuera del cuerpo principal del documento mediante detecci√≥n din√°mica.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Imagen de entrada.\n",
    "        size_threshold_percentile (int): Percentil m√≠nimo de altura para considerar caja v√°lida.\n",
    "        merge_dist (int): Distancia m√°xima entre cajas para considerar agrupaci√≥n.\n",
    "        expand_margin (int): Expansi√≥n del marco del cuerpo principal.\n",
    "        debug_path (str): Ruta para guardar imagen de depuraci√≥n.\n",
    "        audit_csv_path (str): Ruta para guardar CSV con info de cajas.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Imagen filtrada.\n",
    "    \"\"\"\n",
    "    img_rgb = np.array(image.convert(\"RGB\"))\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    if not boxes:\n",
    "        return image\n",
    "\n",
    "    heights = [h for (_, _, _, h) in boxes]\n",
    "    height_threshold = np.percentile(heights, size_threshold_percentile)\n",
    "\n",
    "    filtered_boxes = [(x, y, w, h) for (x, y, w, h) in boxes if h >= height_threshold]\n",
    "    if not filtered_boxes:\n",
    "        return image\n",
    "\n",
    "    centers = np.array([[x + w//2, y + h//2] for (x, y, w, h) in filtered_boxes])\n",
    "    clustering = DBSCAN(eps=merge_dist, min_samples=3).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    largest_label = pd.Series(labels).value_counts().idxmax()\n",
    "    main_group_boxes = [box for i, box in enumerate(filtered_boxes) if labels[i] == largest_label]\n",
    "\n",
    "    x_min = max(0, min(x for x, _, _, _ in main_group_boxes) - expand_margin)\n",
    "    x_max = max(x + w for x, _, w, _ in main_group_boxes) + expand_margin\n",
    "    y_min = max(0, min(y for _, y, _, _ in main_group_boxes) - expand_margin)\n",
    "    y_max = max(y + h for _, y, _, h in main_group_boxes) + expand_margin\n",
    "\n",
    "    mask = np.zeros_like(gray)\n",
    "    accepted, rejected = [], []\n",
    "\n",
    "    for x, y, w, h in boxes:\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        if x_min <= cx <= x_max and y_min <= cy <= y_max:\n",
    "            accepted.append((x, y, w, h))\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "        else:\n",
    "            rejected.append((x, y, w, h))\n",
    "\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "    filtered_image = Image.fromarray(result)\n",
    "\n",
    "    if debug_path:\n",
    "        debug_img = img_rgb.copy()\n",
    "        for x, y, w, h in accepted:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        for x, y, w, h in rejected:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "        cv2.rectangle(debug_img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "        cv2.imwrite(debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if audit_csv_path:\n",
    "        records = []\n",
    "        for x, y, w, h in accepted:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"accepted\"})\n",
    "        for x, y, w, h in rejected:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"rejected\"})\n",
    "        pd.DataFrame(records).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        doc_title = row.get(\"doc_title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", doc_title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                debug_path = f\"debug_lines/{filename}_page_{page_num}.png\" if debug else None\n",
    "                audit_csv_path = f\"debug_lines/{filename}_page_{page_num}_boxes.csv\" if debug else None\n",
    "\n",
    "                # üß† NUEVA funci√≥n din√°mica\n",
    "                filtered_img = filter_text_body_dynamic(\n",
    "                    image,\n",
    "                    size_threshold_percentile=40,\n",
    "                    merge_dist=20,\n",
    "                    expand_margin=10,\n",
    "                    debug_path=debug_path,\n",
    "                    audit_csv_path=audit_csv_path\n",
    "                )\n",
    "\n",
    "                page_text = pytesseract.image_to_string(filtered_img, lang=lang)\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # üõë Stop at Anexos\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"doc_title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"sc\",  # Cambia por tu carpeta de PDFs\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d6f369",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11bf8b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b583",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732a622",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenar los dos dataframes\n",
    "text_df = pd.concat([df_editable, df_scanned], ignore_index=True)\n",
    "\n",
    "# Ordenar por la columna 'date'\n",
    "text_df = text_df.sort_values(by='date')\n",
    "\n",
    "# Si deseas resetear los √≠ndices despu√©s de la concatenaci√≥n\n",
    "text_df = text_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "text_df.to_csv(\"raw_text_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f7447",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f091eb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905873",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas donde \"text\" empieza con una letra min√∫scula\n",
    "text_df[text_df[\"text\"].str.match(r\"^[a-z]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a964d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99986295",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2a300",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4495a5e",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-4\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.4 Noise reduction\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35008e-67ba-4748-bc28-3c4c565d557b",
   "metadata": {},
   "source": [
    "## 3. Quality Filtering (Heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad352b2c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# BOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4509a5",
   "metadata": {},
   "source": [
    "\n",
    "# Testing on scanned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1 = df_editable.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb181b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1 = df_scanned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327efd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474c213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87487",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a35c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_noise(df, filter_keywords=True):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # === COMPILAR PATRONES DE DIRECCIONES A ELIMINAR ===\n",
    "    pattern_direccion_completa = re.compile(\n",
    "        r\"(?:Av\\.|Jr\\.|Calle)\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+\\d{1,5}\\s+‚Äî\\s+Oficina\\s+\\d{1,4}\\s+‚Äî\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+.*\",\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # === PATR√ìN DE DIRECCI√ìN ESCANEADA O CORRUPTA (completo o incrustado) ===\n",
    "    pattern_direccion_ruido = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "            \\b(?:Av|Jr|Calle|Psj|Prolongaci√≥n)\\.?\\s+            # V√≠a\n",
    "            [\\w√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±\\s\\.\\-]{3,30}                        # Nombre de calle\n",
    "            \\s+\\d{3,6}                                          # N√∫mero\n",
    "            (?:\\s+[^\\s]{3,15})?                                 # C√≥digo corrupto: \"OfIKi981\", etc.\n",
    "            \\s*[-‚Äì‚Äî]?\\s*\n",
    "            (?:San\\s+Isidro|Miraflores|Magdalena|Surco|Jesus\\s+Mar√≠a|Lince|Barranco)?\n",
    "            (?:\\s+e\\s+)?                                        # Posible \"e\"\n",
    "            (?:\\.pe|cf\\.gob\\.pe)?                               # Dominio\n",
    "        )\n",
    "        \"\"\",\n",
    "        flags=re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # === CONVERTIR A UNICODE SEGURO ===\n",
    "    def to_unicode(x):\n",
    "        try:\n",
    "            if isinstance(x, bytes):\n",
    "                return x.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            return str(x).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(to_unicode)\n",
    "    \n",
    "    # === ELIMINAR FILAS CON DIRECCIONES REPETITIVAS (ANTES DE LIMPIEZAS) ===\n",
    "    df_clean = df_clean[~df_clean[\"text\"].str.contains(pattern_direccion_completa)].reset_index(drop=True)\n",
    "    \n",
    "    # === LIMPIAR O ELIMINAR DIRECCIONES ESCANEADAS ===\n",
    "    def limpiar_o_eliminar_direccion(text):\n",
    "        if re.fullmatch(pattern_direccion_ruido, text.strip()):\n",
    "            return None  # Eliminar p√°rrafo completo\n",
    "        return re.sub(pattern_direccion_ruido, \"\", text)  # Reemplazar si est√° embebido\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(limpiar_o_eliminar_direccion)\n",
    "    df_clean = df_clean[df_clean[\"text\"].notnull() & df_clean[\"text\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "    # === LIMPIEZAS B√ÅSICAS VECTORIAL ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\b\\d{1,3}/\\d{1,3}\\b\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+([,;:.!?])\", r\"\\1\", regex=True)\n",
    "\n",
    "    # === LIMPIEZA DE URLS (normales y corruptas) ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"(https?://|htps://|htttp://|htpss://|www\\.|wwww\\.)[\\w\\-\\.]*\\.[a-z]{2,6}(\\/[\\w\\-\\.%]*)*\", \n",
    "        \"\", regex=True\n",
    "    )\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(htps?|htttp|htpss):\\/\\/[^\\s]*\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[‚Ä¢‚û¢√ò*ÔÄ™¬∞¬°!?¬ø\\\"]\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\":\\s*$\", \".\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"Lima[,]?\\s+\\d{1,2}\\s+de\\s+[a-z√°√©√≠√≥√∫]+\\s+de\\s+\\d{4}\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\"p.p.\", \"puntos porcentuales\")\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r'^\\s*((?:[ivxlcdm]+|[a-zA-Z]|\\d+)[\\.\\)]\\s*)+', '', regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(?:a|al|de|del|con|por|para|y|o|en|sin|sobre|ante|tras|entre|hacia|hasta|durante|mediante|excepto|salvo|seg√∫n)\\.$\",\n",
    "        lambda m: m.group(0)[:-1], regex=True)\n",
    "    \n",
    "    # === REEMPLAZAR EXPRESIONES CORRUPTAS ===\n",
    "    pattern_minist = re.compile(r\"MINIST\\s+ERIO[\\w%-¬∫|\\\"‚Äù]+|8N%IC%A\\s+Y\\s+FIN\\s+em.*?Usuaro\", flags=re.IGNORECASE)\n",
    "    pattern_garbage_symbols = re.compile(r\"[^\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]{5,}\")\n",
    "    \n",
    "    # Patr√≥n para capturar bloques muy corruptos (inicio o intermedios)\n",
    "    pattern_noise_block = re.compile(\n",
    "        r\"(?:[‚Äî\\[\\]_=\\]\\|%#@*<>~¬´¬ª]{2,}|[\\w]*‚Äî[\\w]*|‚Äî\\s*[A-Z]{1}\\s*‚Äî)+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_minist, \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_garbage_symbols, \"\", regex=True)\n",
    "    \n",
    "    # Aplica reemplazo a esos bloques basura\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_noise_block, \"\", regex=True)\n",
    "    \n",
    "    # Patr√≥n para frases de cortes√≠a y cierre\n",
    "    pattern_cortesia = re.compile(\n",
    "        r\"(aprovecho\\s+la\\s+oportunidad\\s+para\\s+expresar(le)?\\s*(a usted)?\\s*las?\\s*muestras?\\s+de\\s+mi\\s+especial\\s+consideraci√≥n( y estima)?[\\.]?)|\"  # Variante directa\n",
    "        r\"(le\\s+reitero\\s+(las?\\s+)?muestras?\\s+de\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)|\"  # Otra variante frecuente\n",
    "        r\"(hago\\s+propicia\\s+la\\s+ocasi√≥n\\s+para\\s+saludar(lo|la)?\\s*(muy)?\\s*atentamente[\\.]?)|\"  # Variante \"hago propicia...\"\n",
    "        r\"(sin\\s+otro\\s+particular.*?me\\s+despido[^\\n\\.]*[\\.]?)|\"  # Frase de cierre cl√°sica\n",
    "        r\"(con\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)\",  # Frase final t√≠pica\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Aplicar limpieza al texto\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_cortesia, \"\", regex=True)\n",
    "\n",
    "    # === FILTROS DE TEXTO V√ÅLIDO ===\n",
    "    def is_valid(text):\n",
    "        txt = text.strip()\n",
    "        letters = [c for c in txt if c.isalpha()]\n",
    "        if letters and sum(c.isupper() for c in letters) / len(letters) > 0.7:\n",
    "            return False\n",
    "        if txt.lower().startswith(\"fuente:\") or re.search(r\"\\b(Fuente:|Elaboraci√≥n:)\", txt, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        if re.search(r\"(?:\\b\\w\\s){3,}\\w\", txt):\n",
    "            return False\n",
    "        patterns = [\n",
    "            r'PCA\\s*(Inicial|1er\\s*Trim\\.|2do\\s*Trim\\.|3er\\s*Trim\\.)',\n",
    "            r'entre\\s+\\d{4}\\sy\\s+\\d{4}\\s*,?\\s*\\d+\\s*de\\s+cada\\s+100\\s*(leyes?|insistencia|implicancia\\s*fiscal)',\n",
    "            r'‚Äú[^‚Äù]*\\(p√°g\\.\\s*\\d+\\)[^‚Äù]*‚Äù|\\(p√°g\\.\\s*\\d+\\)',\n",
    "            r\"V√©ase informes\\s+(N¬∞\\s*\\d{2}-\\d{4}-CF[, y]*){2,}\",  # puedes mantenerla si gustas\n",
    "            r\"\\b(V√©ase|Ver|Consultar|Rem√≠tase|Rev√≠sese)\\s+(el\\s+)?(informe|art√≠culo|documento|reporte|an√°lisis|dictamen|comunicado\\s+oficial|nota)\\b.*?(N[¬∞¬∫]?\\s*\\d{1,3})?.*?\\b(del\\s+)?\\d{4}\",\n",
    "            r\"¬¢\\s*US?|US?\\s*¬¢\", r\"¬¢\"\n",
    "        ]\n",
    "        if any(re.search(p, txt, flags=re.IGNORECASE) for p in patterns):\n",
    "            return False\n",
    "        if len(txt.split()) < 6:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_clean = df_clean[df_clean[\"text\"].apply(is_valid)].reset_index(drop=True)\n",
    "\n",
    "    # === FUSI√ìN DE P√ÅRRAFOS CONSECUTIVOS ===\n",
    "    rows = []\n",
    "    i = 0\n",
    "    while i < len(df_clean):\n",
    "        current_row = df_clean.iloc[i].copy()\n",
    "        current_text = current_row[\"text\"]\n",
    "        current_title = current_row[\"doc_title\"]\n",
    "\n",
    "        while i + 1 < len(df_clean):\n",
    "            next_row = df_clean.iloc[i + 1]\n",
    "            next_text = next_row[\"text\"]\n",
    "            next_title = next_row[\"doc_title\"]\n",
    "\n",
    "            if next_title != current_title:\n",
    "                break\n",
    "\n",
    "            if not current_text.endswith(\".\") and next_text and next_text[0].isalpha() and next_text.endswith(\".\"):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{4}\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if re.match(r\"^[\\(,;:\\s]*[a-z\\(]\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{1,3}(?:,\\d+)?\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        current_row[\"text\"] = current_text\n",
    "        rows.append(current_row)\n",
    "        i += 1\n",
    "\n",
    "    df_result = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    df_result[\"paragraph_id\"] = df_result.groupby(\"doc_title\").cumcount() + 1\n",
    "\n",
    "    # === LIMPIEZAS ADICIONALES ===\n",
    "    cleaned_rows = []\n",
    "    for idx, row in df_result.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        starts_with_special = re.match(r'^[*%\\\"\\'\\‚Äú\\\\\\[\\]\\{\\}\\(\\)\\^\\-+=<>~#@|]', text)\n",
    "        starts_with_number = re.match(r\"^\\d+\\s+\", text)\n",
    "        prev_text = df_result.iloc[idx - 1][\"text\"].strip() if idx > 0 else \"\"\n",
    "        prev_ends_with_dot = prev_text.endswith(\".\")\n",
    "\n",
    "        if starts_with_special:\n",
    "            continue\n",
    "        if starts_with_number and prev_ends_with_dot:\n",
    "            continue\n",
    "\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    df_result = pd.DataFrame(cleaned_rows).reset_index(drop=True)\n",
    "\n",
    "    # Eliminar n√∫meros al inicio del texto aunque est√©n pegados\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+\\s*\", \"\", regex=True)\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+(?=\\w)\", \"\", regex=True)\n",
    "    \n",
    "    # Eliminar observaciones con menos de 75 caracteres\n",
    "    df_result = df_result[df_result[\"text\"].str.len() >= 75].reset_index(drop=True)\n",
    "\n",
    "    # === FILTRO OPCIONAL CON KEYWORDS DE ALERTA FISCAL ===\n",
    "    if filter_keywords:\n",
    "        keywords = [\n",
    "            \"Incumplimiento de reglas fiscales\", \"Preocupaci√≥n\", \"Advertencia\", \"Alerta\",\n",
    "            \"Riesgos fiscales\", \"Desv√≠o del d√©ficit fiscal\", \"No cumplimiento\", \"Desviaciones significativas\",\n",
    "            \"Margen significativo\", \"Problema de credibilidad fiscal\", \"Credibilidad fiscal\",\n",
    "            \"Sostenibilidad fiscal\", \"Consolidaci√≥n fiscal\", \"Medidas correctivas\", \"Recomendaci√≥n\",\n",
    "            \"Necesidad de tomar medidas\", \"Control del gasto p√∫blico\", \"Presiones fiscales\",\n",
    "            \"Exceso de optimismo en proyecciones\", \"Exceso de gasto\", \"Aumento de gasto\",\n",
    "            \"Reducci√≥n de d√©ficit fiscal\", \"Incremento de ingresos permanentes\",\n",
    "            \"Falta de compromiso con la responsabilidad fiscal\", \"Medidas de consolidaci√≥n\",\n",
    "            \"Deficiencia en la ejecuci√≥n del gasto\", \"Aumento de la deuda p√∫blica\",\n",
    "            \"Iniciativas legislativas que afectan las finanzas p√∫blicas\", \"Incremento del gasto p√∫blico\",\n",
    "            \"Beneficios tributarios sin justificaci√≥n\", \"Tratamientos tributarios preferenciales\",\n",
    "            \"Erosi√≥n de la base tributaria\", \"Elusi√≥n y evasi√≥n tributaria\",\n",
    "            \"Aumento de gastos no previstos\", \"Aumento de gastos extraordinarios\",\n",
    "            \"Aumento de gastos en remuneraciones\", \"Crecimiento del gasto no financiero\",\n",
    "            \"Problema de sostenibilidad\", \"Riesgos de sostenibilidad fiscal\", \"Aumento de deuda neta\",\n",
    "            \"Desajuste fiscal\", \"Falta de transparencia en el gasto\", \"Riesgos de sobreendeudamiento\",\n",
    "            \"Excepciones a las reglas fiscales\", \"Riesgo de incumplimiento de metas fiscales\",\n",
    "            \"Aumento de los compromisos de deuda\", \"Riesgo de insolvencia\",\n",
    "            \"Falta de flexibilidad fiscal\", \"Desajuste entre el presupuesto y el MMM\",\n",
    "            \"Riesgo de incumplimiento debido a presiones de gasto\", \"Erosi√≥n de la capacidad recaudatoria\",\n",
    "            \"Incremento de la deuda p√∫blica\", \"Falta de control de gastos extraordinarios\",\n",
    "            \"Necesidad de ajustar el gasto\", \"Inestabilidad macroecon√≥mica\",\n",
    "            \"Problemas fiscales derivados de iniciativas legislativas\",\n",
    "            \"Riesgo de desajustes fiscales por reformas\",\n",
    "            \"Falta de capacidad de generar ingresos fiscales\", \"Riesgo de gasto excesivo\",\n",
    "            \"Incremento del gasto p√∫blico no controlado\", \"Medidas de ajuste fiscal\",\n",
    "            \"Inestabilidad presupuestaria\", \"Riesgo de inestabilidad fiscal\",\n",
    "            \"Falta de sostenibilidad de la deuda\", \"Compromiso con la disciplina fiscal\",\n",
    "            \"Necesidad de mejorar la disciplina fiscal\", \"Riesgos derivados de la crisis financiera\",\n",
    "            \"Emergencia fiscal\", \"No cumplimiento de los l√≠mites de deuda\",\n",
    "            \"Riesgo de presi√≥n sobre las finanzas p√∫blicas\", \"Riesgos de sostenibilidad a largo plazo\",\n",
    "            \"Inconsistencia en las proyecciones fiscales\", \"Proyecciones fiscales no realistas\",\n",
    "            \"Implicaciones fiscales de la situaci√≥n de Petroper√∫\",\n",
    "            \"Desajuste en las proyecciones fiscales\", \"Necesidad de consolidaci√≥n fiscal\",\n",
    "            \"Riesgos de desequilibrio fiscal\", \"Amenaza a la estabilidad fiscal\",\n",
    "            \"Inseguridad fiscal\", \"Inconsistencias fiscales\", \"Falta de previsi√≥n en el gasto\",\n",
    "            \"Riesgo de p√©rdida de control fiscal\", \"Impacto fiscal no anticipado\",\n",
    "            \"Presi√≥n de gastos adicionales\", \"Aumento en la presi√≥n fiscal\",\n",
    "            \"Erosi√≥n de las finanzas p√∫blicas\", \"Riesgo de d√©ficit fiscal no controlado\",\n",
    "            \"Aumento de la carga fiscal\", \"Riesgo de crisis fiscal\",\n",
    "            \"Propuestas legislativas que generan gasto\",\n",
    "            \"Propuestas que limitan la recaudaci√≥n fiscal\",\n",
    "            \"Iniciativas fiscales con implicaciones negativas\",\n",
    "            \"Aumento de los gastos sociales no previstos\",\n",
    "            \"Riesgo de incumplimiento de los l√≠mites fiscales\",\n",
    "            \"Propuestas legislativas que no cumplen con las reglas fiscales\",\n",
    "            \"Desviaciones fiscales no justificadas\",\n",
    "            \"Proyecciones de d√©ficit fiscal no alcanzables\",\n",
    "            \"Riesgos derivados de iniciativas legislativas excesivas\",\n",
    "            \"Crecimiento de la deuda p√∫blica sin control\",\n",
    "            \"Necesidad de pol√≠ticas fiscales m√°s estrictas\"\n",
    "        ]\n",
    "        keywords_lower = [k.lower() for k in keywords]\n",
    "        df_result = df_result[\n",
    "            df_result[\"text\"].str.lower().apply(lambda txt: any(k in txt for k in keywords_lower))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alertas = clean_noise(df, filter_keywords=True)   # Solo alertas fiscales\n",
    "df_editable_cleaned = clean_noise(df_copy_editable_1, filter_keywords=False) # Todo texto √∫til"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff0fb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_ed_2 = df_editable_cleaned[df_editable_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_ed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba3655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f97370",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1e3b6",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### The same abve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cc388",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned = clean_noise(df_copy_scanned_1, filter_keywords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129198df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a48a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_sc_2 = df_scanned_cleaned[df_scanned_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_sc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0b03f",
   "metadata": {},
   "source": [
    "# Limpieza global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset = pd.read_csv(\"raw_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = clean_noise(raw_text_dataset, filter_keywords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85e839-342c-48ff-92f7-7f163f272df5",
   "metadata": {},
   "source": [
    "## Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83de11-21f5-4602-b8c1-984e33cc7352",
   "metadata": {},
   "source": [
    "(exact, fuzzy, and semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87b60-edbb-4add-94dd-68aa32dca553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4e3de-00f6-45e6-9f98-e46bbe05e731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6bc72-7b82-4a6c-8a27-d4b0d3f4d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f0611-274f-4fa2-a51e-5a1dd0c3c708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d142ba-ab58-41fc-b795-01b6c22d92e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c281b-dcdb-44e1-ac54-178f4f09961a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d0f2ac4-b938-4804-9971-7c57a3bbf698",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeba45",
   "metadata": {},
   "source": [
    "# Limpieza para estad√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea36f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "input_text_dataset = pd.read_csv(\"input_text_dataset_true.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68515fb",
   "metadata": {},
   "source": [
    "## Order by year and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_type\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_year_and_date(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'date' a datetime (si es necesario) y ordena el DataFrame por 'year' y 'date'.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame que contiene las columnas 'year' y 'date'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame ordenado correctamente por 'year' y 'date'.\n",
    "    \"\"\"\n",
    "    required_cols = {'year', 'date'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'year' y 'date'.\")\n",
    "\n",
    "    # Convertir 'date' a datetime si no lo es ya\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"date\"]):\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors='coerce')\n",
    "\n",
    "    return df.sort_values(by=[\"year\", \"date\", \"page\", \"paragraph_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = sort_by_year_and_date(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_doc_type_from_title(df):\n",
    "    \"\"\"\n",
    "    Completa los valores vac√≠os en la columna 'doc_type' a partir del contenido de la columna 'doc_title'.\n",
    "    Solo extrae 'Comunicado' o 'Informe' si est√°n al inicio del t√≠tulo.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame con las columnas 'doc_title' y 'doc_type'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_type' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas requeridas existan\n",
    "    required_cols = {'doc_title', 'doc_type'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'doc_title' y 'doc_type'.\")\n",
    "\n",
    "    # Crear m√°scara de filas donde doc_type est√° vac√≠o\n",
    "    mask = df[\"doc_type\"].isna()\n",
    "\n",
    "    # Expresi√≥n regular para capturar solo \"Comunicado\" o \"Informe\" al inicio del t√≠tulo\n",
    "    regex = r\"^(Comunicado|Informe)\\b\"\n",
    "\n",
    "    # Extraer y llenar solo en filas vac√≠as\n",
    "    df.loc[mask, \"doc_type\"] = df.loc[mask, \"doc_title\"].str.extract(regex)[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7787bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_doc_type_from_title(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd665d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_number\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_doc_number(input_text_dataset):\n",
    "    \"\"\"\n",
    "    Llena los valores vac√≠os en 'doc_number' con n√∫meros flotantes secuenciales por a√±o, \n",
    "    basados en el orden de 'date', empezando en 1.0.\n",
    "\n",
    "    Par√°metros:\n",
    "        input_text_dataset (pd.DataFrame): DataFrame que debe contener las columnas 'doc_number', 'year', y 'date'.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_number' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas necesarias existan\n",
    "    required_cols = {'doc_number', 'year', 'date'}\n",
    "    if not required_cols.issubset(input_text_dataset.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'doc_number', 'year' y 'date'.\")\n",
    "\n",
    "    # Ordenar por year y date\n",
    "    df = input_text_dataset.sort_values(by=[\"year\", \"date\"]).copy()\n",
    "\n",
    "    # Crear m√°scara de valores vac√≠os en doc_number\n",
    "    mask = df[\"doc_number\"].isna()\n",
    "\n",
    "    # Para cada a√±o, generar un contador incremental para fechas √∫nicas\n",
    "    def assign_ids(group):\n",
    "        # Solo para filas con doc_number nulo\n",
    "        missing = group[\"doc_number\"].isna()\n",
    "        # Contador secuencial por fecha\n",
    "        group.loc[missing, \"doc_number\"] = (\n",
    "            group.loc[missing]\n",
    "                 .groupby(\"date\", sort=False)\n",
    "                 .ngroup() + 1\n",
    "        ).astype(float)\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(assign_ids)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_missing_doc_number(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368141b",
   "metadata": {},
   "source": [
    "# TO csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"llm_input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20813007",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Estad√≠sticos de p√°rrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c872a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "# input_text_dataset = pd.read_csv(\"input_text_dataset.csv\")\n",
    "\n",
    "# === Total de documentos √∫nicos, por tipo ===\n",
    "total_docs = input_text_dataset[['doc_title', 'doc_number', 'date']].apply(tuple, axis=1).nunique()\n",
    "docs_por_tipo = input_text_dataset.drop_duplicates('doc_title')['doc_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_por_tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c849ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A√±os con m√°s y menos opiniones ===\n",
    "opiniones_por_a√±o = input_text_dataset.groupby('year')['doc_number'].nunique()\n",
    "a√±o_mas_opiniones = opiniones_por_a√±o.idxmax()\n",
    "a√±o_menos_opiniones = opiniones_por_a√±o.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiniones_por_a√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_mas_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_menos_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Documento con m√°s/menos p√°rrafos ===\n",
    "parrafos_por_doc = input_text_dataset.groupby('doc_title')['paragraph_id'].nunique()\n",
    "doc_mas_parrafos = parrafos_por_doc.idxmax()\n",
    "doc_menos_parrafos = parrafos_por_doc.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mas_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_menos_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5266a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmin()]\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmax()]\n",
    "tamano_promedio_parrafo = input_text_dataset[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a418",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4689",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a89ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968737a",
   "metadata": {},
   "source": [
    "# palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Conteo de palabras por p√°rrafo ===\n",
    "input_text_dataset[\"word_count\"] = input_text_dataset[\"text\"].str.split().str.len()\n",
    "\n",
    "# P√°rrafo con menor cantidad de palabras\n",
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmin()]\n",
    "\n",
    "# P√°rrafo con mayor cantidad de palabras\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmax()]\n",
    "\n",
    "# Tama√±o promedio de p√°rrafo en n√∫mero de palabras\n",
    "tamano_promedio_parrafo = input_text_dataset[\"word_count\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8886fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d15d9a2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc27b1",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9207",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python (Fiscal_Tone)",
   "language": "python",
   "name": "fiscal_tone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
