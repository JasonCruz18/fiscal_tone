{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b952faed",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"background:#634C44; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22631b14",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea16164-f75c-4b20-9d2b-71fb36b5cf9b",
   "metadata": {},
   "source": [
    "## LLM using Gemini (langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497e7d2",
   "metadata": {},
   "source": [
    "> **Author:** Jason Cruz  \n",
    "  **Last updated:** 11/16/2025  \n",
    "  **Python version:** 3.12  \n",
    "  **Project:** Fiscal Tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916311",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Summary\n",
    "Welcome to the **Fiscal Tone** Text Preprocessing notebook! This notebook will guide you through the **step-by-step process** of \n",
    "\n",
    "\n",
    "### What will this notebook help you achieve?\n",
    "1. **Downloading PDFs** from the BCRP Weekly Reports (WR).\n",
    "2. **Generating PDF inputs** by shortening them to focus on key pages containing GDP growth rate tables.\n",
    "3. **Cleaning-up extracted data** to ensure it's usable and building RTD.\n",
    "4. **Concatenating RTD** from different years and frequencies (monthly, quarterly, annual).\n",
    "5. **Updating metadata** for storing base years changes and other revisions-based information.\n",
    "6. **Converting RTD** to releases dataset for econometric analysis.\n",
    "\n",
    "üåê **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (üì∞ WR, from here on)  \n",
    "For any questions or issues, feel free to reach out via email: [Jason üì®](mailto:jj.cruza@up.edu.pe)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a909438",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### ‚öôÔ∏è Initial Set-up\n",
    "\n",
    "Before preprocessing the new GDP releases data, we need to perform some initial set-up steps:\n",
    "\n",
    "1. üß∞ **Import helper functions** from `gdp_rtd_pipeline.py` that are required for this notebook.\n",
    "2. üõ¢Ô∏è **Connect to the PostgreSQL database** that will contain GDP revisions datasets. _(This step is pending: direct access will be provided via ODBC or other methods, allowing users to connect from any software or programming language.)_\n",
    "3. üìÇ **Create necessary folders** to store inputs, outputs, logs, and screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f754c24",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "#from cf_mef_functions.py import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f17343",
   "metadata": {},
   "source": [
    "> üöß Although the second step (database connection) is pending, the notebook currently works using **flat files (CSV)**. These CSV files will **not be saved in GitHub** as they are included in the `.gitignore` to ensure no data is stored publicly. Users can be confident that no data will be stored on GitHub. The notebook **automatically generates the CSV files**, giving users direct access to the dataset on their own systems. The data is created on the fly and can be saved locally for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8c42b",
   "metadata": {},
   "source": [
    "### üß∞ Import helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d50a",
   "metadata": {},
   "source": [
    "This notebook relies on a set of helper functions found in the script `gdp_rtd_pipeline.py`. These functions will be used throughout the notebook, so please ensure you have them ready by running the line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6199",
   "metadata": {},
   "source": [
    "> üõ†Ô∏è **Libraries:** Before you begin, please ensure that you have the required libraries installed and imported. See all the libraries you need section by section in `gdp_rtd_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660bfd8",
   "metadata": {},
   "source": [
    "**Check out Python information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "addf904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Information\n",
      "  Version  : 3.12.11\n",
      "  Compiler : MSC v.1929 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jun  5 2025 12:58:53')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"üêç Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ace86",
   "metadata": {},
   "source": [
    "### üìÇ Create necessary folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb2655",
   "metadata": {},
   "source": [
    "We will start by creating the necessary folders to store the data at various stages of processing. The following code ensures all required directories exist, and if not, it creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6916c1-e725-4eba-8199-4aa480f7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a78251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\n",
      "üìÇ data created\n",
      "üìÇ data\\raw created\n",
      "üìÇ data\\input created\n",
      "üìÇ data\\output created\n",
      "üìÇ metadata created\n",
      "üìÇ record created\n",
      "üìÇ _debugging created\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # Importing Path module from pathlib to handle file and directory paths in a cross-platform way.\n",
    "\n",
    "# Get current working directory\n",
    "PROJECT_ROOT = Path.cwd()  # Get the current working directory where the notebook is being executed.\n",
    "\n",
    "# User input for folder location\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"  # Prompt user to input the folder path or use the default value \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()  # Combine the project root directory with user input to get the full target path.\n",
    "\n",
    "# Create the necessary directories if they don't already exist\n",
    "target_path.mkdir(parents=True, exist_ok=True)  # Creates the target folder and any necessary parent directories.\n",
    "print(f\"Using path: {target_path}\")  # Print out the path being used for confirmation.\n",
    "\n",
    "# Define paths for saving data and PDFs\n",
    "data_folder = 'data'  # This folder will store the new Weekly Reports (post-2013), which are in PDF format.\n",
    "raw_data_subfolder = os.path.join(data_folder, 'raw')  # Subfolder for saving the raw PDFs exactly as downloaded from the BCRP website.\n",
    "input_data_subfolder = os.path.join(data_folder, 'input')  # Subfolder for saving reduced PDFs that contain only the selected pages with GDP growth tables.\n",
    "output_data_subfolder = os.path.join(data_folder, 'output')\n",
    "\n",
    "# Additional folders for metadata, records, and alert tracking\n",
    "metadata_folder = 'metadata'  # Folder for storing metadata files like wr_metadata.csv.\n",
    "record_folder = 'record'  # Folder for storing .txt files that track the files already processed to avoid reprocessing them.\n",
    "debugging_folder = '_debugging'  # Folder for storing .html files that perform debugging.\n",
    "\n",
    "# Create additional required folders\n",
    "for folder in [data_folder, raw_data_subfolder, input_data_subfolder, output_data_subfolder, metadata_folder, record_folder, debugging_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create the additional folders if they don't exist.\n",
    "    print(f\"üìÇ {folder} created\")  # Print confirmation for each of these additional folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc9c1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1f74",
   "metadata": {},
   "source": [
    "## 1. Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75589e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6410e-3fdf-4b83-864c-29b6da530698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: classify PPT / presentation documents\n",
    "# ======================================================\n",
    "\n",
    "def is_presentation_pdf(url_or_text):\n",
    "    if not url_or_text:\n",
    "        return False\n",
    "    s = url_or_text.lower()\n",
    "    ppt_keywords = [\n",
    "        \"ppt\", \"presentacion\", \"presentaci√≥n\", \"diapositiva\",\n",
    "        \"slides\", \"conferencia\", \"powerpoint\"\n",
    "    ]\n",
    "    return any(kw in s for kw in ppt_keywords)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: extract all PDF links\n",
    "# ======================================================\n",
    "\n",
    "def extract_pdf_links(dsoup):\n",
    "    pdf_links = []\n",
    "    for a in dsoup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \".pdf\" in href.lower():\n",
    "            txt = a.text.strip() if a.text else \"\"\n",
    "            pdf_links.append((href, txt))\n",
    "    return pdf_links\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: select the best PDF from multiple candidates\n",
    "# ======================================================\n",
    "\n",
    "def select_appropriate_pdf(pdf_links):\n",
    "    if not pdf_links:\n",
    "        return None\n",
    "\n",
    "    filtered = [\n",
    "        (href, txt) for href, txt in pdf_links\n",
    "        if not is_presentation_pdf(href) and not is_presentation_pdf(txt)\n",
    "    ]\n",
    "\n",
    "    candidates = filtered if filtered else pdf_links\n",
    "\n",
    "    priority_keywords = [\n",
    "        \"comunicado\", \"informe\", \"nota\", \"reporte\",\n",
    "        \"documento\", \"pronunciamiento\"\n",
    "    ]\n",
    "\n",
    "    def score(x):\n",
    "        href, _ = x\n",
    "        h = href.lower()\n",
    "        return sum(kw in h for kw in priority_keywords)\n",
    "\n",
    "    return max(candidates, key=score)[0]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# SCRAPER ‚Äì Incremental: only visits detail pages not seen before\n",
    "# ======================================================\n",
    "\n",
    "def scrape_cf(url, already_scraped_pages):\n",
    "    t0 = timer()\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    rows = soup.select(\"table.table tbody tr\")\n",
    "    new_records = []\n",
    "    list_records = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            date = row.select_one(\"td.size100 p\").text.strip()\n",
    "            link_tag = row.select_one(\"td a\")\n",
    "            title = link_tag.text.strip()\n",
    "            page_url = link_tag[\"href\"]\n",
    "\n",
    "            list_records.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"page_url\": page_url\n",
    "            })\n",
    "\n",
    "            if page_url in already_scraped_pages:\n",
    "                continue\n",
    "\n",
    "            detail = requests.get(page_url, timeout=15)\n",
    "            dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "\n",
    "            pdf_url = None\n",
    "\n",
    "            # A) <a> PDF\n",
    "            pdf_links = extract_pdf_links(dsoup)\n",
    "            if pdf_links:\n",
    "                pdf_url = select_appropriate_pdf(pdf_links)\n",
    "\n",
    "            # B) iframe\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if iframe:\n",
    "                    src = iframe[\"src\"]\n",
    "                    if src.startswith(\"//\"):\n",
    "                        src = \"https:\" + src\n",
    "                    pdf_url = src\n",
    "\n",
    "            # C) Google Docs viewer\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \"docs.google.com\" in x.lower())\n",
    "                if iframe:\n",
    "                    parsed = urlparse(iframe[\"src\"])\n",
    "                    q = parse_qs(parsed.query)\n",
    "                    if \"url\" in q:\n",
    "                        pdf_url = unquote(q[\"url\"][0])\n",
    "\n",
    "            # D) PDF viewer \"Guardar\"\n",
    "            if not pdf_url:\n",
    "                button = dsoup.find(\"button\", id=\"downloadButton\") or dsoup.find(\"span\", string=\"Guardar\")\n",
    "                if button:\n",
    "                    parent = button.find_parent(\"a\")\n",
    "                    if parent and parent.has_attr(\"href\"):\n",
    "                        pdf_url = parent[\"href\"]\n",
    "\n",
    "            filename = pdf_url.split(\"/\")[-1] if pdf_url else None\n",
    "\n",
    "            new_records.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"page_url\": page_url,\n",
    "                \"pdf_url\": pdf_url,\n",
    "                \"pdf_filename\": filename\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing row: {e}\")\n",
    "\n",
    "    print(f\"‚åõ scrape_cf_expanded executed in {timer() - t0:.2f} sec\")\n",
    "    return list_records, new_records\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# DOWNLOADER ‚Äî with incremental CSV-safe saving (even if interrupted)\n",
    "# ======================================================\n",
    "\n",
    "def pdf_downloader(cf_urls, raw_pdf_folder, metadata_folder, metadata_csv):\n",
    "\n",
    "    t0 = timer()\n",
    "    os.makedirs(raw_pdf_folder, exist_ok=True)\n",
    "    os.makedirs(metadata_folder, exist_ok=True)\n",
    "\n",
    "    metadata_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load previous metadata safely\n",
    "    if os.path.exists(metadata_path):\n",
    "        old_df = pd.read_csv(metadata_path, dtype=str)\n",
    "        old_urls = set(old_df[\"pdf_url\"].dropna())\n",
    "        old_pages = set(old_df[\"page_url\"].dropna())\n",
    "    else:\n",
    "        old_df = pd.DataFrame()\n",
    "        old_urls = set()\n",
    "        old_pages = set()\n",
    "\n",
    "    all_new_records = []\n",
    "\n",
    "    # SCRAPE incremental\n",
    "    for url in cf_urls:\n",
    "        print(f\"\\nüåê Scraping list page: {url}\")\n",
    "        list_records, new_page_records = scrape_cf(\n",
    "            url, already_scraped_pages=old_pages\n",
    "        )\n",
    "        all_new_records.extend(new_page_records)\n",
    "\n",
    "    if not all_new_records:\n",
    "        print(\"\\nüîé No new pages: skipping download.\")\n",
    "        print(f\"üìù Metadata unchanged: {metadata_path}\")\n",
    "        # ‚Üí return existing CSV as DataFrame\n",
    "        return pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "    new_df = pd.DataFrame(all_new_records).dropna(subset=[\"pdf_url\"])\n",
    "    mask_new = ~new_df[\"pdf_url\"].isin(old_urls)\n",
    "    df_to_download = new_df[mask_new].copy()\n",
    "\n",
    "    # Sort oldest ‚Üí newest\n",
    "    df_to_download[\"date\"] = pd.to_datetime(df_to_download[\"date\"], dayfirst=True)\n",
    "    df_to_download = df_to_download.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nüîé Found {len(df_to_download)} new PDFs to download\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/pdf\"}\n",
    "\n",
    "    # ---- Incremental download + incremental CSV writing ----\n",
    "    temp_df = old_df.copy()\n",
    "\n",
    "    for i, row in df_to_download.iterrows():\n",
    "        pdf_url = row[\"pdf_url\"]\n",
    "        filename = row[\"pdf_filename\"]\n",
    "        page_url = row[\"page_url\"]\n",
    "        filepath = os.path.join(raw_pdf_folder, filename)\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(df_to_download)}] üìÑ {filename}\")\n",
    "        print(f\"üîó {pdf_url}\")\n",
    "        \n",
    "        success = False\n",
    "\n",
    "        # Primary download attempt\n",
    "        try:\n",
    "            r = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"‚úÖ Saved {filename}\")\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Primary failed: {e}\")\n",
    "        \n",
    "        # Fallback attempt\n",
    "        if not success:\n",
    "            try:\n",
    "                print(\"üîÅ Trying extended fallback‚Ä¶\")\n",
    "                detail = requests.get(page_url, timeout=15)\n",
    "                dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "        \n",
    "                iframe_url = None\n",
    "        \n",
    "                # 1) <embed src=\"...pdf\">\n",
    "                embed = dsoup.find(\"embed\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if embed:\n",
    "                    iframe_url = embed[\"src\"]\n",
    "        \n",
    "                # 2) <div data-pdf-src=\"...pdf\">\n",
    "                if not iframe_url:\n",
    "                    divpdf = dsoup.find(\"div\", attrs={\"data-pdf-src\": True})\n",
    "                    if divpdf and \".pdf\" in divpdf[\"data-pdf-src\"].lower():\n",
    "                        iframe_url = divpdf[\"data-pdf-src\"]\n",
    "        \n",
    "                # Normalize URL\n",
    "                if iframe_url and iframe_url.startswith(\"//\"):\n",
    "                    iframe_url = \"https:\" + iframe_url\n",
    "        \n",
    "                if iframe_url:\n",
    "                    print(f\"   ‚á¢ fallback PDF URL: {iframe_url}\")\n",
    "        \n",
    "                    r2 = requests.get(iframe_url, headers=headers, timeout=20)\n",
    "                    r2.raise_for_status()\n",
    "        \n",
    "                    if \"pdf\" not in r2.headers.get(\"Content-Type\", \"\").lower():\n",
    "                        raise ValueError(\"Server returned HTML instead of PDF\")\n",
    "        \n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        f.write(r2.content)\n",
    "        \n",
    "                    print(f\"‚úÖ Saved via embed/data-pdf-src fallback: {filename}\")\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(\"‚ùå No embed/data-pdf-src found\")\n",
    "        \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Extended fallback failed: {e2}\")\n",
    "\n",
    "        # === Incremental update to metadata (even if interrupted later) ===\n",
    "        temp_df = pd.concat([temp_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        temp_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "        # avoid rate limit\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"üìù Metadata saved incrementally: {metadata_path}\")\n",
    "    print(f\"‚è±Ô∏è Done in {round(timer() - t0, 2)} sec\")\n",
    "\n",
    "    # === RETURN THE CSV AS DATAFRAME ===\n",
    "    final_df = pd.read_csv(metadata_path, dtype=str)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6dff3-a6a6-44b6-99a9-87f3be0a3fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf_urls = [\n",
    "    \"https://cf.gob.pe/p/informes/\",\n",
    "    \"https://cf.gob.pe/p/comunicados/\"\n",
    "]\n",
    "\n",
    "metadata_df = pdf_downloader(\n",
    "    cf_urls=cf_urls,\n",
    "    raw_pdf_folder=raw_data_subfolder,\n",
    "    metadata_folder=metadata_folder,\n",
    "    metadata_csv=\"cf_metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d174521-b611-4a90-b242-18e39551b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c597d21",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### Delete selected documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254f2a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Lista de archivos a eliminar\n",
    "pdfs_to_remove = [\n",
    "    \"Informe-anual-2017_CF_vf.pdf\", # This is a way too long document containing statistical analaysis. We're focus only on text as comunicados from CF.\n",
    "    \"Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafdf3a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_pdfs(folder_path, filenames_to_remove):\n",
    "    \"\"\"\n",
    "    Deletes specific unwanted PDF files from a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, the directory containing the PDFs\n",
    "    - filenames_to_remove: list of str, filenames to delete\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    removed_count = 0\n",
    "\n",
    "    print(f\"üßπ Removing in: {folder_path}\")\n",
    "    for filename in filenames_to_remove:\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            os.remove(full_path)\n",
    "            print(f\"üóëÔ∏è Deleted: {filename}\")\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "\n",
    "    print(f\"\\nüßπ Cleanup complete. Total files removed: {removed_count}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca1ebf",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "remove_unwanted_pdfs(raw_data_subfolder, pdfs_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433e30b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Clasificando entre scanned and editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a037",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Please, compruebe que la carpeta \"\" contenga scaneados y que la carpeta \"\" ediatables. En caso de que no se haya clasificado correctamente los pdfs, agreguelos manualemnte. Esto es vital para los pr√≥ximos c√≥digos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5ef1b9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF, used to extract text from PDFs\n",
    "\n",
    "def is_editable_pdf(file_path, min_text_length=20):\n",
    "    \"\"\"Check if a PDF contains extractable text (editable).\"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            return len(\"\".join(page.get_text() for page in doc).strip()) >= min_text_length\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def classify_pdfs_by_type(classification_folder):\n",
    "    \"\"\"\n",
    "    Classifies PDF files into 'editable' and 'scanned' subfolders.\n",
    "    This function now only focuses on classifying the type of raw data (editable or scanned) and moving files.\n",
    "    \n",
    "    Parameters:\n",
    "    - classification_folder: str, the directory where 'editable' and 'scanned' subfolders will be created.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ensure classification_folder is a list, even if a single folder is passed\n",
    "    if isinstance(classification_folder, str):\n",
    "        classification_folder = [classification_folder]\n",
    "\n",
    "    # Create the 'editable' and 'scanned' subfolders within the classification_folder\n",
    "    output_dir_editable = os.path.join(classification_folder[0], \"editable\")\n",
    "    output_dir_scanned = os.path.join(classification_folder[0], \"scanned\")\n",
    "    os.makedirs(output_dir_editable, exist_ok=True)\n",
    "    os.makedirs(output_dir_scanned, exist_ok=True)\n",
    "\n",
    "    total_files = 0\n",
    "    scanned_count = 0\n",
    "    editable_count = 0\n",
    "\n",
    "    t0 = timer()\n",
    "    print(\"üîç Starting PDF classification...\")\n",
    "\n",
    "    # Iterate through the provided folders\n",
    "    for folder in classification_folder:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                total_files += 1\n",
    "                pdf_path = os.path.join(folder, filename)\n",
    "\n",
    "                # Classify and move the PDF to the appropriate folder\n",
    "                if is_editable_pdf(pdf_path):\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_editable, filename))\n",
    "                    editable_count += 1\n",
    "                else:\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_scanned, filename))\n",
    "                    scanned_count += 1\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    # Print a summary\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"üìÑ Total PDFs processed: {total_files}\")\n",
    "    print(f\"üíª Editable PDFs: {editable_count}\")\n",
    "    print(f\"üñ®Ô∏è Scanned PDFs: {scanned_count}\")\n",
    "    print(f\"üìÅ Saved editable PDFs in: '{output_dir_editable}'\")\n",
    "    print(f\"üìÅ Saved scanned PDFs in: '{output_dir_scanned}'\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508d2041",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting PDF classification...\n",
      "\n",
      "\n",
      "üìä Summary:\n",
      "üìÑ Total PDFs processed: 77\n",
      "üíª Editable PDFs: 64\n",
      "üñ®Ô∏è Scanned PDFs: 13\n",
      "üìÅ Saved editable PDFs in: 'data\\raw\\editable'\n",
      "üìÅ Saved scanned PDFs in: 'data\\raw\\scanned'\n",
      "‚è±Ô∏è Time taken: 1.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the folder paths\n",
    "classification_folder = raw_data_subfolder\n",
    "\n",
    "# Call the function to classify PDFs\n",
    "classify_pdfs_by_type(classification_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62017a05-bc5f-49cf-b12b-b66ae4b236e0",
   "metadata": {},
   "source": [
    "Metadata Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308f8ea-9d91-45d6-89c1-2d77ed9953b9",
   "metadata": {},
   "source": [
    "Adds metadata from a CSV file, enhancing the extracted text with additional document-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37a1d261-968c-488d-8ffb-e7051a93d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def metadata_enrichment(classification_folder, metadata_folder, metadata_csv=\"cf_metadata\"):\n",
    "    \"\"\"\n",
    "    Enriches metadata with information such as 'raw_data_type', 'doc_type', 'doc_number', and 'year'.\n",
    "    \n",
    "    Parameters:\n",
    "    - classification_folder: str, the directory where 'editable' and 'scanned' subfolders exist.\n",
    "    - metadata_folder: str, the folder where the metadata CSV file is located.\n",
    "    - metadata_csv: str, the name of the CSV file containing metadata (without the '.csv' extension).\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df: DataFrame with the enriched metadata.\n",
    "    \"\"\"\n",
    "    # Add .csv extension to metadata_csv if not provided\n",
    "    metadata_csv_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load the metadata CSV file\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # Ensure 'raw_data_type', 'doc_type', 'doc_number', and 'year' columns exist\n",
    "    if 'raw_data_type' not in metadata_df.columns:\n",
    "        metadata_df['raw_data_type'] = ''\n",
    "    if 'doc_type' not in metadata_df.columns:\n",
    "        metadata_df['doc_type'] = ''\n",
    "    if 'doc_number' not in metadata_df.columns:\n",
    "        metadata_df['doc_number'] = ''\n",
    "    if 'year' not in metadata_df.columns:\n",
    "        metadata_df['year'] = ''\n",
    "\n",
    "    # Function to extract 'doc_type', 'doc_number', 'year' from 'title' column\n",
    "    def extract_doc_info(row):\n",
    "        title = row[\"title\"]\n",
    "        # Extract document type (Informe, Comunicado), document number, and year\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "\n",
    "        # Convert doc_number to integer (this removes leading zeros)\n",
    "        if doc_number:\n",
    "            doc_number = int(doc_number)\n",
    "\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    # Enrich metadata by applying doc_type, doc_number, and year extraction\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "\n",
    "    # Ensure 'editable' and 'scanned' subfolders exist in the provided classification_folder\n",
    "    editable_folder = os.path.join(classification_folder, \"editable\")\n",
    "    scanned_folder = os.path.join(classification_folder, \"scanned\")\n",
    "\n",
    "    # Check if the 'editable' and 'scanned' folders exist\n",
    "    if not os.path.isdir(editable_folder):\n",
    "        print(f\"‚ùå 'editable' folder does not exist in '{classification_folder}'.\")\n",
    "    if not os.path.isdir(scanned_folder):\n",
    "        print(f\"‚ùå 'scanned' folder does not exist in '{classification_folder}'.\")\n",
    "\n",
    "    # Update 'raw_data_type' based on folder classification\n",
    "    for folder, file_type in [(editable_folder, \"editable\"), (scanned_folder, \"scanned\")]:\n",
    "        if os.path.isdir(folder):\n",
    "            for filename in os.listdir(folder):\n",
    "                if filename.lower().endswith(\".pdf\"):\n",
    "                    metadata_df.loc[metadata_df['pdf_filename'] == filename, 'raw_data_type'] = file_type\n",
    "\n",
    "    # Save the enriched metadata to a new CSV file\n",
    "    enriched_metadata_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "    metadata_df.to_csv(enriched_metadata_path, index=False)\n",
    "\n",
    "    print(f\"üìë Metadata enriched and saved to: '{enriched_metadata_path}'\")\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a30ee3f-8b34-4bac-87ef-4e8e1c1b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "classification_folder = raw_data_subfolder\n",
    "metadata_folder = metadata_folder\n",
    "metadata_csv = \"cf_metadata\"  # Note: without the \".csv\" extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50003dca-70de-461b-95d6-4cc93f26beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìë Metadata enriched and saved to: 'metadata\\cf_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "updated_metadata_df = metadata_enrichment(classification_folder, metadata_folder, metadata_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50629e11-9424-4721-b548-3b35b4b00add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>page_url</th>\n",
       "      <th>final_pdf_url</th>\n",
       "      <th>final_pdf_filename</th>\n",
       "      <th>type</th>\n",
       "      <th>raw_data_type</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_number</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>scanned</td>\n",
       "      <td>scanned</td>\n",
       "      <td>Informe</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2016-01-28  Informe CF N¬∞ 001-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "1  2016-04-15  Informe CF N¬∞ 002-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "2  2016-06-23  Informe CF N¬∞ 003-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "3  2016-08-18  Informe CF N¬∞ 005-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "4  2016-08-18  Informe CF N¬∞ 004-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "5  2016-08-26  Informe CF N¬∞ 006-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "6  2016-10-04  Informe CF N¬∞ 007-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "7  2016-12-26  Informe CF N¬∞ 008-2016 ‚Äì Opini√≥n del Consejo F...   \n",
       "8  2017-05-03  Informe CF N¬∞ 001-2017 ‚Äì Opini√≥n del Consejo F...   \n",
       "9  2017-05-09  Informe CF N¬∞ 002-2017 ‚Äì Opini√≥n del Consejo F...   \n",
       "\n",
       "                                            page_url  \\\n",
       "0  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "1  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "2  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "3  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "4  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "5  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "6  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "7  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "9  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "\n",
       "                                     final_pdf_url         final_pdf_filename  \\\n",
       "0  https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf  Informe_CF_N_001-2016.pdf   \n",
       "1  https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf  Informe_CF_N_002-2016.pdf   \n",
       "2  https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf  Informe_CF_N_003-2016.pdf   \n",
       "3  https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf  Informe_CF_N_005-2016.pdf   \n",
       "4  https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf  Informe_CF_N_004-2016.pdf   \n",
       "5  https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf  Informe_CF_N_006-2016.pdf   \n",
       "6  https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf  Informe_CF_N_007-2016.pdf   \n",
       "7  https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf  Informe_CF_N_008-2016.pdf   \n",
       "8  https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf  INFORME_N_001-2017-CF.pdf   \n",
       "9  https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf  INFORME_N_002-2017-CF.pdf   \n",
       "\n",
       "      type raw_data_type doc_type  doc_number  year  \n",
       "0  scanned       scanned  Informe         1.0  2016  \n",
       "1  scanned       scanned  Informe         2.0  2016  \n",
       "2  scanned       scanned  Informe         3.0  2016  \n",
       "3  scanned       scanned  Informe         5.0  2016  \n",
       "4  scanned       scanned  Informe         4.0  2016  \n",
       "5  scanned       scanned  Informe         6.0  2016  \n",
       "6  scanned       scanned  Informe         7.0  2016  \n",
       "7  scanned       scanned  Informe         8.0  2016  \n",
       "8  scanned       scanned  Informe         1.0  2017  \n",
       "9  scanned       scanned  Informe         2.0  2017  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the updated DataFrame in Jupyter Notebook or JupyterLab\n",
    "updated_metadata_df.head(10)  # Display the first few rows of the updated CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a0469-4b91-4b9a-aab7-7054986addd1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328c08f-ed3d-425e-875c-c9207d4ce03d",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73aeab-af66-47bc-88f1-33400c1bf941",
   "metadata": {},
   "source": [
    "## 3. Cleaning & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb714-1ab9-4f90-a3b9-56c6f8a6c717",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359ad05-e4a9-471d-a50d-f6fff4713674",
   "metadata": {},
   "source": [
    "Why Preprocessing is Important\n",
    "1. Reduces noise: Cleaning removes unnecessary or irrelevant parts of the text, ensuring the model learns from meaningful content.\n",
    "2. Improves consistency: By standardizing punctuation and text formats, we ensure the model handles diverse text sources uniformly.\n",
    "3. Increases tokenization accuracy: Clean text is easier to tokenize, leading to more precise embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17329117",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c95c8-dc4b-472f-874c-7290bc4d471b",
   "metadata": {},
   "source": [
    "segmentaci√≥n por sentido ling√º√≠stico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b607c22",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def process_editable_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", output_csv=\"raw_editable_pdfs_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Processes editable PDF documents, extracting structured paragraphs and saving to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, path to the folder containing editable PDFs\n",
    "    - metadata_csv_path: str, path to the CSV file with document metadata\n",
    "    - output_csv: str, output CSV filename to save extracted paragraphs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted structured text and metadata\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # --- Extract doc_type, doc_number, year from title ---\n",
    "    def extract_doc_info(row):\n",
    "        title = row[\"title\"]\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "\n",
    "    print(\"üß† Starting paragraph extraction...\\n\")\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÑ Processing: {idx}. {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                paragraph_counter = 1\n",
    "                anexo_found = False\n",
    "\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    if anexo_found:\n",
    "                        break\n",
    "\n",
    "                    # Extract words with their size and vertical position\n",
    "                    words = page.extract_words(extra_attrs=[\"size\", \"top\"])\n",
    "                    FONT_MIN = 11.0\n",
    "                    FONT_MAX = 11.9\n",
    "                    clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX]\n",
    "                    if not clean_words:\n",
    "                        continue\n",
    "\n",
    "                    # Group words by their vertical position to form lines\n",
    "                    lines_dict = {}\n",
    "                    for word in clean_words:\n",
    "                        line_top = round(word[\"top\"], 1)\n",
    "                        lines_dict.setdefault(line_top, []).append(word[\"text\"])\n",
    "\n",
    "                    lines = [\n",
    "                        \" \".join(words).strip()\n",
    "                        for _, words in sorted(lines_dict.items())\n",
    "                        if words\n",
    "                    ]\n",
    "\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    page_text = \"\\n\".join(lines)\n",
    "\n",
    "                    # üö´ Stop extraction at \"Anexo\"\n",
    "                    match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                    if match:\n",
    "                        page_text = page_text[:match.start()].strip()\n",
    "                        anexo_found = True\n",
    "                        print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                    # Paragraph segmentation\n",
    "                    lines = page_text.strip().split(\"\\n\")\n",
    "                    lines = [line.strip() for line in lines if line.strip()]\n",
    "                    paragraph_lines = []\n",
    "\n",
    "                    for i, line in enumerate(lines):\n",
    "                        is_new_paragraph = (\n",
    "                            line.startswith(\"‚Ä¢\")\n",
    "                            or line.startswith(\"‚û¢\")\n",
    "                            or (i > 0 and lines[i - 1].strip().endswith(\".\"))\n",
    "                            or (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                        )\n",
    "\n",
    "                        if is_new_paragraph:\n",
    "                            if paragraph_lines:\n",
    "                                all_records.append({\n",
    "                                    \"filename\": filename,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"paragraph_id\": paragraph_counter,\n",
    "                                    \"text\": \" \".join(paragraph_lines).strip()\n",
    "                                })\n",
    "                                paragraph_counter += 1\n",
    "                            paragraph_lines = [line]\n",
    "                        else:\n",
    "                            paragraph_lines.append(line)\n",
    "\n",
    "                    if paragraph_lines:\n",
    "                        all_records.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"page\": page_num,\n",
    "                            \"paragraph_id\": paragraph_counter,\n",
    "                            \"text\": \" \".join(paragraph_lines).strip()\n",
    "                        })\n",
    "                        paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    # Merge with metadata\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\n‚úÖ Extraction complete. Total paragraphs: {len(df)}\")\n",
    "    print(f\"üíæ Saved to: {output_csv}\")\n",
    "    print(f\"‚è±Ô∏è Time taken: {t1 - t0:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a354e5d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable = process_editable_pdfs(\"editable_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f3936",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbd2f",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af8df4a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd8030",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "\"\"\"\n",
    "Procesa PDFs escaneados usando OCR para extraer p√°rrafos por p√°gina y metadatos.\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Ruta a la carpeta con archivos PDF escaneados.\n",
    "    dpi (int): Resoluci√≥n al convertir PDF a imagen.\n",
    "    lang (str): Idioma para el OCR ('spa' para espa√±ol, 'eng' para ingl√©s, etc.).\n",
    "\n",
    "Returns:\n",
    "    pd.DataFrame: DataFrame con columnas:\n",
    "        ['filename', 'year', 'date', 'announcement', 'page', 'paragraph_id', 'text']\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eec680",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Puede tardar un poco m√°s. Se excluye pies de pagina con detecci√≥n visual de lineas que marcan el inicio. Se p√°ginas de anexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e58f5",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.2, y_range=(0.5, 0.85), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line that likely marks the beginning of the footer in a scanned PDF page.\n",
    "    \n",
    "    Parameters:\n",
    "        image (PIL.Image): Page image to analyze.\n",
    "        min_length_ratio (float): Minimum line length relative to image width.\n",
    "        y_range (tuple): Vertical range (as a proportion of image height) where footer lines are expected.\n",
    "        debug_path (str): Optional path to save debug image with the detected line.\n",
    "    \n",
    "    Returns:\n",
    "        int: Y-coordinate to crop the image above the footer line, or image height if no line is found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=100,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 3 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # Default: no footer line detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c36df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a635301",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Major function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac228",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.17, y_range=(0.55, 0.90), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line likely indicating the beginning of the footer in scanned PDFs.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Page image.\n",
    "        min_length_ratio (float): Minimum length of line relative to image width.\n",
    "        y_range (tuple): Vertical range to search (proportional to height).\n",
    "        debug_path (str): Optional file path to save a debug image with detected line.\n",
    "\n",
    "    Returns:\n",
    "        int: Y-coordinate of the detected line, or image height if none found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=80,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 5 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # No line detected ‚Üí return full height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a24cd2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Processing for Scanned PDFs ===\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from scanned PDFs using OCR, excluding footers and annexes.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Folder with scanned PDFs.\n",
    "        metadata_csv_path (str): Path to metadata CSV file.\n",
    "        dpi (int): Resolution used to convert PDFs to images.\n",
    "        lang (str): OCR language code.\n",
    "        debug (bool): Whether to save debug images with detected lines.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Paragraph-level extracted data.\n",
    "    \"\"\"\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                debug_path = None\n",
    "                if debug:\n",
    "                    os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                    debug_path = f\"debug_lines/{filename}_page_{page_num}.png\"\n",
    "\n",
    "                cut_y = detect_cut_line_y(image, debug_path=debug_path)\n",
    "                cropped_img = image.crop((0, 0, image.width, cut_y))\n",
    "\n",
    "                page_text = pytesseract.image_to_string(cropped_img, lang=lang)\n",
    "\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # Stop at 'Anexo'\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51a39",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === Run and Save ===\n",
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"scanned_pdf\",\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deef5c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93432a87",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# NUEVA UTILIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6963",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def filter_text_body_dynamic(\n",
    "    image,\n",
    "    size_threshold_percentile=40,\n",
    "    merge_dist=20,\n",
    "    expand_margin=10,\n",
    "    debug_path=None,\n",
    "    audit_csv_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Filtra regiones de texto fuera del cuerpo principal del documento mediante detecci√≥n din√°mica.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Imagen de entrada.\n",
    "        size_threshold_percentile (int): Percentil m√≠nimo de altura para considerar caja v√°lida.\n",
    "        merge_dist (int): Distancia m√°xima entre cajas para considerar agrupaci√≥n.\n",
    "        expand_margin (int): Expansi√≥n del marco del cuerpo principal.\n",
    "        debug_path (str): Ruta para guardar imagen de depuraci√≥n.\n",
    "        audit_csv_path (str): Ruta para guardar CSV con info de cajas.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Imagen filtrada.\n",
    "    \"\"\"\n",
    "    img_rgb = np.array(image.convert(\"RGB\"))\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    if not boxes:\n",
    "        return image\n",
    "\n",
    "    heights = [h for (_, _, _, h) in boxes]\n",
    "    height_threshold = np.percentile(heights, size_threshold_percentile)\n",
    "\n",
    "    filtered_boxes = [(x, y, w, h) for (x, y, w, h) in boxes if h >= height_threshold]\n",
    "    if not filtered_boxes:\n",
    "        return image\n",
    "\n",
    "    centers = np.array([[x + w//2, y + h//2] for (x, y, w, h) in filtered_boxes])\n",
    "    clustering = DBSCAN(eps=merge_dist, min_samples=3).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    largest_label = pd.Series(labels).value_counts().idxmax()\n",
    "    main_group_boxes = [box for i, box in enumerate(filtered_boxes) if labels[i] == largest_label]\n",
    "\n",
    "    x_min = max(0, min(x for x, _, _, _ in main_group_boxes) - expand_margin)\n",
    "    x_max = max(x + w for x, _, w, _ in main_group_boxes) + expand_margin\n",
    "    y_min = max(0, min(y for _, y, _, _ in main_group_boxes) - expand_margin)\n",
    "    y_max = max(y + h for _, y, _, h in main_group_boxes) + expand_margin\n",
    "\n",
    "    mask = np.zeros_like(gray)\n",
    "    accepted, rejected = [], []\n",
    "\n",
    "    for x, y, w, h in boxes:\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        if x_min <= cx <= x_max and y_min <= cy <= y_max:\n",
    "            accepted.append((x, y, w, h))\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "        else:\n",
    "            rejected.append((x, y, w, h))\n",
    "\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "    filtered_image = Image.fromarray(result)\n",
    "\n",
    "    if debug_path:\n",
    "        debug_img = img_rgb.copy()\n",
    "        for x, y, w, h in accepted:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        for x, y, w, h in rejected:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "        cv2.rectangle(debug_img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "        cv2.imwrite(debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if audit_csv_path:\n",
    "        records = []\n",
    "        for x, y, w, h in accepted:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"accepted\"})\n",
    "        for x, y, w, h in rejected:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"rejected\"})\n",
    "        pd.DataFrame(records).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    print(\"üì• Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[¬∞¬∫o]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_number = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_number\": doc_number, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_number\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"üß† Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"üñ®Ô∏è OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                debug_path = f\"debug_lines/{filename}_page_{page_num}.png\" if debug else None\n",
    "                audit_csv_path = f\"debug_lines/{filename}_page_{page_num}_boxes.csv\" if debug else None\n",
    "\n",
    "                # üß† NUEVA funci√≥n din√°mica\n",
    "                filtered_img = filter_text_body_dynamic(\n",
    "                    image,\n",
    "                    size_threshold_percentile=40,\n",
    "                    merge_dist=20,\n",
    "                    expand_margin=10,\n",
    "                    debug_path=debug_path,\n",
    "                    audit_csv_path=audit_csv_path\n",
    "                )\n",
    "\n",
    "                page_text = pytesseract.image_to_string(filtered_img, lang=lang)\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # üõë Stop at Anexos\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"üõë 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"‚Ä¢\") or line.startswith(\"‚û¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_number\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_number\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\n‚úÖ OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"sc\",  # Cambia por tu carpeta de PDFs\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"üíæ Saved as 'raw_scanned_pdfs_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d6f369",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11bf8b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b583",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732a622",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenar los dos dataframes\n",
    "text_df = pd.concat([df_editable, df_scanned], ignore_index=True)\n",
    "\n",
    "# Ordenar por la columna 'date'\n",
    "text_df = text_df.sort_values(by='date')\n",
    "\n",
    "# Si deseas resetear los √≠ndices despu√©s de la concatenaci√≥n\n",
    "text_df = text_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "text_df.to_csv(\"raw_text_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f7447",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f091eb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905873",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas donde \"text\" empieza con una letra min√∫scula\n",
    "text_df[text_df[\"text\"].str.match(r\"^[a-z]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a964d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99986295",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2a300",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4495a5e",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-4\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.4 Noise reduction\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35008e-67ba-4748-bc28-3c4c565d557b",
   "metadata": {},
   "source": [
    "## 3. Quality Filtering (Heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad352b2c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# BOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4509a5",
   "metadata": {},
   "source": [
    "\n",
    "# Testing on scanned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1 = df_editable.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb181b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1 = df_scanned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327efd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474c213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87487",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a35c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_noise(df, filter_keywords=True):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # === COMPILAR PATRONES DE DIRECCIONES A ELIMINAR ===\n",
    "    pattern_direccion_completa = re.compile(\n",
    "        r\"(?:Av\\.|Jr\\.|Calle)\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+\\d{1,5}\\s+‚Äî\\s+Oficina\\s+\\d{1,4}\\s+‚Äî\\s+[\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫]+.*\",\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # === PATR√ìN DE DIRECCI√ìN ESCANEADA O CORRUPTA (completo o incrustado) ===\n",
    "    pattern_direccion_ruido = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "            \\b(?:Av|Jr|Calle|Psj|Prolongaci√≥n)\\.?\\s+            # V√≠a\n",
    "            [\\w√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±\\s\\.\\-]{3,30}                        # Nombre de calle\n",
    "            \\s+\\d{3,6}                                          # N√∫mero\n",
    "            (?:\\s+[^\\s]{3,15})?                                 # C√≥digo corrupto: \"OfIKi981\", etc.\n",
    "            \\s*[-‚Äì‚Äî]?\\s*\n",
    "            (?:San\\s+Isidro|Miraflores|Magdalena|Surco|Jesus\\s+Mar√≠a|Lince|Barranco)?\n",
    "            (?:\\s+e\\s+)?                                        # Posible \"e\"\n",
    "            (?:\\.pe|cf\\.gob\\.pe)?                               # Dominio\n",
    "        )\n",
    "        \"\"\",\n",
    "        flags=re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # === CONVERTIR A UNICODE SEGURO ===\n",
    "    def to_unicode(x):\n",
    "        try:\n",
    "            if isinstance(x, bytes):\n",
    "                return x.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            return str(x).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(to_unicode)\n",
    "    \n",
    "    # === ELIMINAR FILAS CON DIRECCIONES REPETITIVAS (ANTES DE LIMPIEZAS) ===\n",
    "    df_clean = df_clean[~df_clean[\"text\"].str.contains(pattern_direccion_completa)].reset_index(drop=True)\n",
    "    \n",
    "    # === LIMPIAR O ELIMINAR DIRECCIONES ESCANEADAS ===\n",
    "    def limpiar_o_eliminar_direccion(text):\n",
    "        if re.fullmatch(pattern_direccion_ruido, text.strip()):\n",
    "            return None  # Eliminar p√°rrafo completo\n",
    "        return re.sub(pattern_direccion_ruido, \"\", text)  # Reemplazar si est√° embebido\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(limpiar_o_eliminar_direccion)\n",
    "    df_clean = df_clean[df_clean[\"text\"].notnull() & df_clean[\"text\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "    # === LIMPIEZAS B√ÅSICAS VECTORIAL ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\b\\d{1,3}/\\d{1,3}\\b\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+([,;:.!?])\", r\"\\1\", regex=True)\n",
    "\n",
    "    # === LIMPIEZA DE URLS (normales y corruptas) ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"(https?://|htps://|htttp://|htpss://|www\\.|wwww\\.)[\\w\\-\\.]*\\.[a-z]{2,6}(\\/[\\w\\-\\.%]*)*\", \n",
    "        \"\", regex=True\n",
    "    )\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(htps?|htttp|htpss):\\/\\/[^\\s]*\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[‚Ä¢‚û¢√ò*ÔÄ™¬∞¬°!?¬ø\\\"]\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\":\\s*$\", \".\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"Lima[,]?\\s+\\d{1,2}\\s+de\\s+[a-z√°√©√≠√≥√∫]+\\s+de\\s+\\d{4}\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\"p.p.\", \"puntos porcentuales\")\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r'^\\s*((?:[ivxlcdm]+|[a-zA-Z]|\\d+)[\\.\\)]\\s*)+', '', regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(?:a|al|de|del|con|por|para|y|o|en|sin|sobre|ante|tras|entre|hacia|hasta|durante|mediante|excepto|salvo|seg√∫n)\\.$\",\n",
    "        lambda m: m.group(0)[:-1], regex=True)\n",
    "    \n",
    "    # === REEMPLAZAR EXPRESIONES CORRUPTAS ===\n",
    "    pattern_minist = re.compile(r\"MINIST\\s+ERIO[\\w%-¬∫|\\\"‚Äù]+|8N%IC%A\\s+Y\\s+FIN\\s+em.*?Usuaro\", flags=re.IGNORECASE)\n",
    "    pattern_garbage_symbols = re.compile(r\"[^\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]{5,}\")\n",
    "    \n",
    "    # Patr√≥n para capturar bloques muy corruptos (inicio o intermedios)\n",
    "    pattern_noise_block = re.compile(\n",
    "        r\"(?:[‚Äî\\[\\]_=\\]\\|%#@*<>~¬´¬ª]{2,}|[\\w]*‚Äî[\\w]*|‚Äî\\s*[A-Z]{1}\\s*‚Äî)+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_minist, \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_garbage_symbols, \"\", regex=True)\n",
    "    \n",
    "    # Aplica reemplazo a esos bloques basura\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_noise_block, \"\", regex=True)\n",
    "    \n",
    "    # Patr√≥n para frases de cortes√≠a y cierre\n",
    "    pattern_cortesia = re.compile(\n",
    "        r\"(aprovecho\\s+la\\s+oportunidad\\s+para\\s+expresar(le)?\\s*(a usted)?\\s*las?\\s*muestras?\\s+de\\s+mi\\s+especial\\s+consideraci√≥n( y estima)?[\\.]?)|\"  # Variante directa\n",
    "        r\"(le\\s+reitero\\s+(las?\\s+)?muestras?\\s+de\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)|\"  # Otra variante frecuente\n",
    "        r\"(hago\\s+propicia\\s+la\\s+ocasi√≥n\\s+para\\s+saludar(lo|la)?\\s*(muy)?\\s*atentamente[\\.]?)|\"  # Variante \"hago propicia...\"\n",
    "        r\"(sin\\s+otro\\s+particular.*?me\\s+despido[^\\n\\.]*[\\.]?)|\"  # Frase de cierre cl√°sica\n",
    "        r\"(con\\s+(mi\\s+)?especial\\s+consideraci√≥n[\\.]?)\",  # Frase final t√≠pica\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Aplicar limpieza al texto\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_cortesia, \"\", regex=True)\n",
    "\n",
    "    # === FILTROS DE TEXTO V√ÅLIDO ===\n",
    "    def is_valid(text):\n",
    "        txt = text.strip()\n",
    "        letters = [c for c in txt if c.isalpha()]\n",
    "        if letters and sum(c.isupper() for c in letters) / len(letters) > 0.7:\n",
    "            return False\n",
    "        if txt.lower().startswith(\"fuente:\") or re.search(r\"\\b(Fuente:|Elaboraci√≥n:)\", txt, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        if re.search(r\"(?:\\b\\w\\s){3,}\\w\", txt):\n",
    "            return False\n",
    "        patterns = [\n",
    "            r'PCA\\s*(Inicial|1er\\s*Trim\\.|2do\\s*Trim\\.|3er\\s*Trim\\.)',\n",
    "            r'entre\\s+\\d{4}\\sy\\s+\\d{4}\\s*,?\\s*\\d+\\s*de\\s+cada\\s+100\\s*(leyes?|insistencia|implicancia\\s*fiscal)',\n",
    "            r'‚Äú[^‚Äù]*\\(p√°g\\.\\s*\\d+\\)[^‚Äù]*‚Äù|\\(p√°g\\.\\s*\\d+\\)',\n",
    "            r\"V√©ase informes\\s+(N¬∞\\s*\\d{2}-\\d{4}-CF[, y]*){2,}\",  # puedes mantenerla si gustas\n",
    "            r\"\\b(V√©ase|Ver|Consultar|Rem√≠tase|Rev√≠sese)\\s+(el\\s+)?(informe|art√≠culo|documento|reporte|an√°lisis|dictamen|comunicado\\s+oficial|nota)\\b.*?(N[¬∞¬∫]?\\s*\\d{1,3})?.*?\\b(del\\s+)?\\d{4}\",\n",
    "            r\"¬¢\\s*US?|US?\\s*¬¢\", r\"¬¢\"\n",
    "        ]\n",
    "        if any(re.search(p, txt, flags=re.IGNORECASE) for p in patterns):\n",
    "            return False\n",
    "        if len(txt.split()) < 6:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_clean = df_clean[df_clean[\"text\"].apply(is_valid)].reset_index(drop=True)\n",
    "\n",
    "    # === FUSI√ìN DE P√ÅRRAFOS CONSECUTIVOS ===\n",
    "    rows = []\n",
    "    i = 0\n",
    "    while i < len(df_clean):\n",
    "        current_row = df_clean.iloc[i].copy()\n",
    "        current_text = current_row[\"text\"]\n",
    "        current_title = current_row[\"title\"]\n",
    "\n",
    "        while i + 1 < len(df_clean):\n",
    "            next_row = df_clean.iloc[i + 1]\n",
    "            next_text = next_row[\"text\"]\n",
    "            next_title = next_row[\"title\"]\n",
    "\n",
    "            if next_title != current_title:\n",
    "                break\n",
    "\n",
    "            if not current_text.endswith(\".\") and next_text and next_text[0].isalpha() and next_text.endswith(\".\"):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{4}\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if re.match(r\"^[\\(,;:\\s]*[a-z\\(]\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{1,3}(?:,\\d+)?\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        current_row[\"text\"] = current_text\n",
    "        rows.append(current_row)\n",
    "        i += 1\n",
    "\n",
    "    df_result = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    df_result[\"paragraph_id\"] = df_result.groupby(\"title\").cumcount() + 1\n",
    "\n",
    "    # === LIMPIEZAS ADICIONALES ===\n",
    "    cleaned_rows = []\n",
    "    for idx, row in df_result.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        starts_with_special = re.match(r'^[*%\\\"\\'\\‚Äú\\\\\\[\\]\\{\\}\\(\\)\\^\\-+=<>~#@|]', text)\n",
    "        starts_with_number = re.match(r\"^\\d+\\s+\", text)\n",
    "        prev_text = df_result.iloc[idx - 1][\"text\"].strip() if idx > 0 else \"\"\n",
    "        prev_ends_with_dot = prev_text.endswith(\".\")\n",
    "\n",
    "        if starts_with_special:\n",
    "            continue\n",
    "        if starts_with_number and prev_ends_with_dot:\n",
    "            continue\n",
    "\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    df_result = pd.DataFrame(cleaned_rows).reset_index(drop=True)\n",
    "\n",
    "    # Eliminar n√∫meros al inicio del texto aunque est√©n pegados\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+\\s*\", \"\", regex=True)\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+(?=\\w)\", \"\", regex=True)\n",
    "    \n",
    "    # Eliminar observaciones con menos de 75 caracteres\n",
    "    df_result = df_result[df_result[\"text\"].str.len() >= 75].reset_index(drop=True)\n",
    "\n",
    "    # === FILTRO OPCIONAL CON KEYWORDS DE ALERTA FISCAL ===\n",
    "    if filter_keywords:\n",
    "        keywords = [\n",
    "            \"Incumplimiento de reglas fiscales\", \"Preocupaci√≥n\", \"Advertencia\", \"Alerta\",\n",
    "            \"Riesgos fiscales\", \"Desv√≠o del d√©ficit fiscal\", \"No cumplimiento\", \"Desviaciones significativas\",\n",
    "            \"Margen significativo\", \"Problema de credibilidad fiscal\", \"Credibilidad fiscal\",\n",
    "            \"Sostenibilidad fiscal\", \"Consolidaci√≥n fiscal\", \"Medidas correctivas\", \"Recomendaci√≥n\",\n",
    "            \"Necesidad de tomar medidas\", \"Control del gasto p√∫blico\", \"Presiones fiscales\",\n",
    "            \"Exceso de optimismo en proyecciones\", \"Exceso de gasto\", \"Aumento de gasto\",\n",
    "            \"Reducci√≥n de d√©ficit fiscal\", \"Incremento de ingresos permanentes\",\n",
    "            \"Falta de compromiso con la responsabilidad fiscal\", \"Medidas de consolidaci√≥n\",\n",
    "            \"Deficiencia en la ejecuci√≥n del gasto\", \"Aumento de la deuda p√∫blica\",\n",
    "            \"Iniciativas legislativas que afectan las finanzas p√∫blicas\", \"Incremento del gasto p√∫blico\",\n",
    "            \"Beneficios tributarios sin justificaci√≥n\", \"Tratamientos tributarios preferenciales\",\n",
    "            \"Erosi√≥n de la base tributaria\", \"Elusi√≥n y evasi√≥n tributaria\",\n",
    "            \"Aumento de gastos no previstos\", \"Aumento de gastos extraordinarios\",\n",
    "            \"Aumento de gastos en remuneraciones\", \"Crecimiento del gasto no financiero\",\n",
    "            \"Problema de sostenibilidad\", \"Riesgos de sostenibilidad fiscal\", \"Aumento de deuda neta\",\n",
    "            \"Desajuste fiscal\", \"Falta de transparencia en el gasto\", \"Riesgos de sobreendeudamiento\",\n",
    "            \"Excepciones a las reglas fiscales\", \"Riesgo de incumplimiento de metas fiscales\",\n",
    "            \"Aumento de los compromisos de deuda\", \"Riesgo de insolvencia\",\n",
    "            \"Falta de flexibilidad fiscal\", \"Desajuste entre el presupuesto y el MMM\",\n",
    "            \"Riesgo de incumplimiento debido a presiones de gasto\", \"Erosi√≥n de la capacidad recaudatoria\",\n",
    "            \"Incremento de la deuda p√∫blica\", \"Falta de control de gastos extraordinarios\",\n",
    "            \"Necesidad de ajustar el gasto\", \"Inestabilidad macroecon√≥mica\",\n",
    "            \"Problemas fiscales derivados de iniciativas legislativas\",\n",
    "            \"Riesgo de desajustes fiscales por reformas\",\n",
    "            \"Falta de capacidad de generar ingresos fiscales\", \"Riesgo de gasto excesivo\",\n",
    "            \"Incremento del gasto p√∫blico no controlado\", \"Medidas de ajuste fiscal\",\n",
    "            \"Inestabilidad presupuestaria\", \"Riesgo de inestabilidad fiscal\",\n",
    "            \"Falta de sostenibilidad de la deuda\", \"Compromiso con la disciplina fiscal\",\n",
    "            \"Necesidad de mejorar la disciplina fiscal\", \"Riesgos derivados de la crisis financiera\",\n",
    "            \"Emergencia fiscal\", \"No cumplimiento de los l√≠mites de deuda\",\n",
    "            \"Riesgo de presi√≥n sobre las finanzas p√∫blicas\", \"Riesgos de sostenibilidad a largo plazo\",\n",
    "            \"Inconsistencia en las proyecciones fiscales\", \"Proyecciones fiscales no realistas\",\n",
    "            \"Implicaciones fiscales de la situaci√≥n de Petroper√∫\",\n",
    "            \"Desajuste en las proyecciones fiscales\", \"Necesidad de consolidaci√≥n fiscal\",\n",
    "            \"Riesgos de desequilibrio fiscal\", \"Amenaza a la estabilidad fiscal\",\n",
    "            \"Inseguridad fiscal\", \"Inconsistencias fiscales\", \"Falta de previsi√≥n en el gasto\",\n",
    "            \"Riesgo de p√©rdida de control fiscal\", \"Impacto fiscal no anticipado\",\n",
    "            \"Presi√≥n de gastos adicionales\", \"Aumento en la presi√≥n fiscal\",\n",
    "            \"Erosi√≥n de las finanzas p√∫blicas\", \"Riesgo de d√©ficit fiscal no controlado\",\n",
    "            \"Aumento de la carga fiscal\", \"Riesgo de crisis fiscal\",\n",
    "            \"Propuestas legislativas que generan gasto\",\n",
    "            \"Propuestas que limitan la recaudaci√≥n fiscal\",\n",
    "            \"Iniciativas fiscales con implicaciones negativas\",\n",
    "            \"Aumento de los gastos sociales no previstos\",\n",
    "            \"Riesgo de incumplimiento de los l√≠mites fiscales\",\n",
    "            \"Propuestas legislativas que no cumplen con las reglas fiscales\",\n",
    "            \"Desviaciones fiscales no justificadas\",\n",
    "            \"Proyecciones de d√©ficit fiscal no alcanzables\",\n",
    "            \"Riesgos derivados de iniciativas legislativas excesivas\",\n",
    "            \"Crecimiento de la deuda p√∫blica sin control\",\n",
    "            \"Necesidad de pol√≠ticas fiscales m√°s estrictas\"\n",
    "        ]\n",
    "        keywords_lower = [k.lower() for k in keywords]\n",
    "        df_result = df_result[\n",
    "            df_result[\"text\"].str.lower().apply(lambda txt: any(k in txt for k in keywords_lower))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alertas = clean_noise(df, filter_keywords=True)   # Solo alertas fiscales\n",
    "df_editable_cleaned = clean_noise(df_copy_editable_1, filter_keywords=False) # Todo texto √∫til"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff0fb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_ed_2 = df_editable_cleaned[df_editable_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_ed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba3655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f97370",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1e3b6",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### The same abve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cc388",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned = clean_noise(df_copy_scanned_1, filter_keywords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129198df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a48a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_sc_2 = df_scanned_cleaned[df_scanned_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_sc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0b03f",
   "metadata": {},
   "source": [
    "# Limpieza global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset = pd.read_csv(\"raw_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = clean_noise(raw_text_dataset, filter_keywords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85e839-342c-48ff-92f7-7f163f272df5",
   "metadata": {},
   "source": [
    "## Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83de11-21f5-4602-b8c1-984e33cc7352",
   "metadata": {},
   "source": [
    "(exact, fuzzy, and semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87b60-edbb-4add-94dd-68aa32dca553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4e3de-00f6-45e6-9f98-e46bbe05e731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6bc72-7b82-4a6c-8a27-d4b0d3f4d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f0611-274f-4fa2-a51e-5a1dd0c3c708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d142ba-ab58-41fc-b795-01b6c22d92e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c281b-dcdb-44e1-ac54-178f4f09961a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d0f2ac4-b938-4804-9971-7c57a3bbf698",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeba45",
   "metadata": {},
   "source": [
    "# Limpieza para estad√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea36f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "input_text_dataset = pd.read_csv(\"input_text_dataset_true.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68515fb",
   "metadata": {},
   "source": [
    "## Order by year and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_type\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_year_and_date(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'date' a datetime (si es necesario) y ordena el DataFrame por 'year' y 'date'.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame que contiene las columnas 'year' y 'date'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame ordenado correctamente por 'year' y 'date'.\n",
    "    \"\"\"\n",
    "    required_cols = {'year', 'date'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'year' y 'date'.\")\n",
    "\n",
    "    # Convertir 'date' a datetime si no lo es ya\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"date\"]):\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors='coerce')\n",
    "\n",
    "    return df.sort_values(by=[\"year\", \"date\", \"page\", \"paragraph_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = sort_by_year_and_date(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_doc_type_from_title(df):\n",
    "    \"\"\"\n",
    "    Completa los valores vac√≠os en la columna 'doc_type' a partir del contenido de la columna 'title'.\n",
    "    Solo extrae 'Comunicado' o 'Informe' si est√°n al inicio del t√≠tulo.\n",
    "    \n",
    "    Par√°metros:\n",
    "        df (pd.DataFrame): DataFrame con las columnas 'title' y 'doc_type'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_type' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas requeridas existan\n",
    "    required_cols = {'title', 'doc_type'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'title' y 'doc_type'.\")\n",
    "\n",
    "    # Crear m√°scara de filas donde doc_type est√° vac√≠o\n",
    "    mask = df[\"doc_type\"].isna()\n",
    "\n",
    "    # Expresi√≥n regular para capturar solo \"Comunicado\" o \"Informe\" al inicio del t√≠tulo\n",
    "    regex = r\"^(Comunicado|Informe)\\b\"\n",
    "\n",
    "    # Extraer y llenar solo en filas vac√≠as\n",
    "    df.loc[mask, \"doc_type\"] = df.loc[mask, \"title\"].str.extract(regex)[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7787bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_doc_type_from_title(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd665d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_number\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_doc_number(input_text_dataset):\n",
    "    \"\"\"\n",
    "    Llena los valores vac√≠os en 'doc_number' con n√∫meros flotantes secuenciales por a√±o, \n",
    "    basados en el orden de 'date', empezando en 1.0.\n",
    "\n",
    "    Par√°metros:\n",
    "        input_text_dataset (pd.DataFrame): DataFrame que debe contener las columnas 'doc_number', 'year', y 'date'.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_number' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas necesarias existan\n",
    "    required_cols = {'doc_number', 'year', 'date'}\n",
    "    if not required_cols.issubset(input_text_dataset.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'doc_number', 'year' y 'date'.\")\n",
    "\n",
    "    # Ordenar por year y date\n",
    "    df = input_text_dataset.sort_values(by=[\"year\", \"date\"]).copy()\n",
    "\n",
    "    # Crear m√°scara de valores vac√≠os en doc_number\n",
    "    mask = df[\"doc_number\"].isna()\n",
    "\n",
    "    # Para cada a√±o, generar un contador incremental para fechas √∫nicas\n",
    "    def assign_ids(group):\n",
    "        # Solo para filas con doc_number nulo\n",
    "        missing = group[\"doc_number\"].isna()\n",
    "        # Contador secuencial por fecha\n",
    "        group.loc[missing, \"doc_number\"] = (\n",
    "            group.loc[missing]\n",
    "                 .groupby(\"date\", sort=False)\n",
    "                 .ngroup() + 1\n",
    "        ).astype(float)\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(assign_ids)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_missing_doc_number(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368141b",
   "metadata": {},
   "source": [
    "# TO csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"llm_input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20813007",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Estad√≠sticos de p√°rrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c872a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Carga del DataFrame (si no est√° cargado) ===\n",
    "# input_text_dataset = pd.read_csv(\"input_text_dataset.csv\")\n",
    "\n",
    "# === Total de documentos √∫nicos, por tipo ===\n",
    "total_docs = input_text_dataset[['title', 'doc_number', 'date']].apply(tuple, axis=1).nunique()\n",
    "docs_por_tipo = input_text_dataset.drop_duplicates('title')['doc_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_por_tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c849ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A√±os con m√°s y menos opiniones ===\n",
    "opiniones_por_a√±o = input_text_dataset.groupby('year')['doc_number'].nunique()\n",
    "a√±o_mas_opiniones = opiniones_por_a√±o.idxmax()\n",
    "a√±o_menos_opiniones = opiniones_por_a√±o.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiniones_por_a√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_mas_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a√±o_menos_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Documento con m√°s/menos p√°rrafos ===\n",
    "parrafos_por_doc = input_text_dataset.groupby('title')['paragraph_id'].nunique()\n",
    "doc_mas_parrafos = parrafos_por_doc.idxmax()\n",
    "doc_menos_parrafos = parrafos_por_doc.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mas_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_menos_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5266a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmin()]\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmax()]\n",
    "tamano_promedio_parrafo = input_text_dataset[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a418",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4689",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a89ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968737a",
   "metadata": {},
   "source": [
    "# palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Conteo de palabras por p√°rrafo ===\n",
    "input_text_dataset[\"word_count\"] = input_text_dataset[\"text\"].str.split().str.len()\n",
    "\n",
    "# P√°rrafo con menor cantidad de palabras\n",
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmin()]\n",
    "\n",
    "# P√°rrafo con mayor cantidad de palabras\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmax()]\n",
    "\n",
    "# Tama√±o promedio de p√°rrafo en n√∫mero de palabras\n",
    "tamano_promedio_parrafo = input_text_dataset[\"word_count\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8886fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d15d9a2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc27b1",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9207",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python (Fiscal_Tone)",
   "language": "python",
   "name": "fiscal_tone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
