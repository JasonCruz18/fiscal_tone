{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b952faed",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"background:#634C44; color:white; padding:12px; box-sizing:border-box; border-radius:4px;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22631b14",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497e7d2",
   "metadata": {},
   "source": [
    "> **Author:** Jason Cruz  \n",
    "  **Last updated:** 11/16/2025  \n",
    "  **Python version:** 3.12  \n",
    "  **Project:** Fiscal Tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916311",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Summary\n",
    "Welcome to the **Fiscal Tone** Text Preprocessing notebook! This notebook will guide you through the **step-by-step process** of \n",
    "\n",
    "\n",
    "### What will this notebook help you achieve?\n",
    "1. **Downloading PDFs** from the BCRP Weekly Reports (WR).\n",
    "2. **Generating PDF inputs** by shortening them to focus on key pages containing GDP growth rate tables.\n",
    "3. **Cleaning-up extracted data** to ensure it's usable and building RTD.\n",
    "4. **Concatenating RTD** from different years and frequencies (monthly, quarterly, annual).\n",
    "5. **Updating metadata** for storing base years changes and other revisions-based information.\n",
    "6. **Converting RTD** to releases dataset for econometric analysis.\n",
    "\n",
    "ğŸŒ **Main Data Source:** [BCRP Weekly Report](https://www.bcrp.gob.pe/publicaciones/nota-semanal.html) (ğŸ“° WR, from here on)  \n",
    "For any questions or issues, feel free to reach out via email: [Jason ğŸ“¨](mailto:jj.cruza@up.edu.pe)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a909438",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### âš™ï¸ Initial Set-up\n",
    "\n",
    "Before preprocessing the new GDP releases data, we need to perform some initial set-up steps:\n",
    "\n",
    "1. ğŸ§° **Import helper functions** from `gdp_rtd_pipeline.py` that are required for this notebook.\n",
    "2. ğŸ›¢ï¸ **Connect to the PostgreSQL database** that will contain GDP revisions datasets. _(This step is pending: direct access will be provided via ODBC or other methods, allowing users to connect from any software or programming language.)_\n",
    "3. ğŸ“‚ **Create necessary folders** to store inputs, outputs, logs, and screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f754c24",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "#from cf_mef_functions.py import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f17343",
   "metadata": {},
   "source": [
    "> ğŸš§ Although the second step (database connection) is pending, the notebook currently works using **flat files (CSV)**. These CSV files will **not be saved in GitHub** as they are included in the `.gitignore` to ensure no data is stored publicly. Users can be confident that no data will be stored on GitHub. The notebook **automatically generates the CSV files**, giving users direct access to the dataset on their own systems. The data is created on the fly and can be saved locally for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8c42b",
   "metadata": {},
   "source": [
    "### ğŸ§° Import helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d50a",
   "metadata": {},
   "source": [
    "This notebook relies on a set of helper functions found in the script `gdp_rtd_pipeline.py`. These functions will be used throughout the notebook, so please ensure you have them ready by running the line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing_pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b6199",
   "metadata": {},
   "source": [
    "> ğŸ› ï¸ **Libraries:** Before you begin, please ensure that you have the required libraries installed and imported. See all the libraries you need section by section in `gdp_rtd_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os # Comment this code with \"#\" if you have already installed this library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660bfd8",
   "metadata": {},
   "source": [
    "**Check out Python information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addf904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ Python Information\n",
      "  Version  : 3.12.11\n",
      "  Compiler : MSC v.1929 64 bit (AMD64)\n",
      "  Build    : ('main', 'Jun  5 2025 12:58:53')\n",
      "  OS       : Windows 10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"ğŸ Python Information\")\n",
    "print(f\"  Version  : {sys.version.split()[0]}\")\n",
    "print(f\"  Compiler : {platform.python_compiler()}\")\n",
    "print(f\"  Build    : {platform.python_build()}\")\n",
    "print(f\"  OS       : {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ace86",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Create necessary folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb2655",
   "metadata": {},
   "source": [
    "We will start by creating the necessary folders to store the data at various stages of processing. The following code ensures all required directories exist, and if not, it creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6916c1-e725-4eba-8199-4aa480f7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a78251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter relative path (default='.'):  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: C:\\Users\\Jason Cruz\\OneDrive\\Documentos\\RA\\CIUP\\GitHub\\FiscalTone\n",
      "ğŸ“‚ data created\n",
      "ğŸ“‚ data\\raw created\n",
      "ğŸ“‚ data\\input created\n",
      "ğŸ“‚ data\\output created\n",
      "ğŸ“‚ metadata created\n",
      "ğŸ“‚ record created\n",
      "ğŸ“‚ _debugging created\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # Importing Path module from pathlib to handle file and directory paths in a cross-platform way.\n",
    "\n",
    "# Get current working directory\n",
    "PROJECT_ROOT = Path.cwd()  # Get the current working directory where the notebook is being executed.\n",
    "\n",
    "# User input for folder location\n",
    "user_input = input(\"Enter relative path (default='.'): \").strip() or \".\"  # Prompt user to input the folder path or use the default value \".\"\n",
    "target_path = (PROJECT_ROOT / user_input).resolve()  # Combine the project root directory with user input to get the full target path.\n",
    "\n",
    "# Create the necessary directories if they don't already exist\n",
    "target_path.mkdir(parents=True, exist_ok=True)  # Creates the target folder and any necessary parent directories.\n",
    "print(f\"Using path: {target_path}\")  # Print out the path being used for confirmation.\n",
    "\n",
    "# Define paths for saving data and PDFs\n",
    "data_folder = 'data'  # This folder will store the new Weekly Reports (post-2013), which are in PDF format.\n",
    "raw_data_subfolder = os.path.join(data_folder, 'raw')  # Subfolder for saving the raw PDFs exactly as downloaded from the BCRP website.\n",
    "input_data_subfolder = os.path.join(data_folder, 'input')  # Subfolder for saving reduced PDFs that contain only the selected pages with GDP growth tables.\n",
    "output_data_subfolder = os.path.join(data_folder, 'output')\n",
    "\n",
    "# Additional folders for metadata, records, and alert tracking\n",
    "metadata_folder = 'metadata'  # Folder for storing metadata files like wr_metadata.csv.\n",
    "record_folder = 'record'  # Folder for storing .txt files that track the files already processed to avoid reprocessing them.\n",
    "debugging_folder = '_debugging'  # Folder for storing .html files that perform debugging.\n",
    "\n",
    "# Create additional required folders\n",
    "for folder in [data_folder, raw_data_subfolder, input_data_subfolder, output_data_subfolder, metadata_folder, record_folder, debugging_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)  # Create the additional folders if they don't exist.\n",
    "    print(f\"ğŸ“‚ {folder} created\")  # Print confirmation for each of these additional folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc9c1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1f74",
   "metadata": {},
   "source": [
    "## 1. Downloading Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75589e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f6410e-3fdf-4b83-864c-29b6da530698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: classify PPT / presentation documents\n",
    "# ======================================================\n",
    "\n",
    "def is_presentation_pdf(url_or_text):\n",
    "    if not url_or_text:\n",
    "        return False\n",
    "    s = url_or_text.lower()\n",
    "    ppt_keywords = [\n",
    "        \"ppt\", \"presentacion\", \"presentaciÃ³n\", \"diapositiva\",\n",
    "        \"slides\", \"conferencia\", \"powerpoint\"\n",
    "    ]\n",
    "    return any(kw in s for kw in ppt_keywords)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: extract all PDF links\n",
    "# ======================================================\n",
    "\n",
    "def extract_pdf_links(dsoup):\n",
    "    pdf_links = []\n",
    "    for a in dsoup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \".pdf\" in href.lower():\n",
    "            txt = a.text.strip() if a.text else \"\"\n",
    "            pdf_links.append((href, txt))\n",
    "    return pdf_links\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Utility: select the best PDF from multiple candidates\n",
    "# ======================================================\n",
    "\n",
    "def select_appropriate_pdf(pdf_links):\n",
    "    if not pdf_links:\n",
    "        return None\n",
    "\n",
    "    filtered = [\n",
    "        (href, txt) for href, txt in pdf_links\n",
    "        if not is_presentation_pdf(href) and not is_presentation_pdf(txt)\n",
    "    ]\n",
    "\n",
    "    candidates = filtered if filtered else pdf_links\n",
    "\n",
    "    priority_keywords = [\n",
    "        \"comunicado\", \"informe\", \"nota\", \"reporte\",\n",
    "        \"documento\", \"pronunciamiento\"\n",
    "    ]\n",
    "\n",
    "    def score(x):\n",
    "        href, _ = x\n",
    "        h = href.lower()\n",
    "        return sum(kw in h for kw in priority_keywords)\n",
    "\n",
    "    return max(candidates, key=score)[0]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# SCRAPER â€“ Incremental: only visits detail pages not seen before\n",
    "# ======================================================\n",
    "\n",
    "def scrape_cf(url, already_scraped_pages):\n",
    "    t0 = timer()\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    rows = soup.select(\"table.table tbody tr\")\n",
    "    new_records = []\n",
    "    list_records = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            date = row.select_one(\"td.size100 p\").text.strip()\n",
    "            link_tag = row.select_one(\"td a\")\n",
    "            title = link_tag.text.strip()\n",
    "            page_url = link_tag[\"href\"]\n",
    "\n",
    "            list_records.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"page_url\": page_url\n",
    "            })\n",
    "\n",
    "            if page_url in already_scraped_pages:\n",
    "                continue\n",
    "\n",
    "            detail = requests.get(page_url, timeout=15)\n",
    "            dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "\n",
    "            pdf_url = None\n",
    "\n",
    "            # A) <a> PDF\n",
    "            pdf_links = extract_pdf_links(dsoup)\n",
    "            if pdf_links:\n",
    "                pdf_url = select_appropriate_pdf(pdf_links)\n",
    "\n",
    "            # B) iframe\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if iframe:\n",
    "                    src = iframe[\"src\"]\n",
    "                    if src.startswith(\"//\"):\n",
    "                        src = \"https:\" + src\n",
    "                    pdf_url = src\n",
    "\n",
    "            # C) Google Docs viewer\n",
    "            if not pdf_url:\n",
    "                iframe = dsoup.find(\"iframe\", src=lambda x: x and \"docs.google.com\" in x.lower())\n",
    "                if iframe:\n",
    "                    parsed = urlparse(iframe[\"src\"])\n",
    "                    q = parse_qs(parsed.query)\n",
    "                    if \"url\" in q:\n",
    "                        pdf_url = unquote(q[\"url\"][0])\n",
    "\n",
    "            # D) PDF viewer \"Guardar\"\n",
    "            if not pdf_url:\n",
    "                button = dsoup.find(\"button\", id=\"downloadButton\") or dsoup.find(\"span\", string=\"Guardar\")\n",
    "                if button:\n",
    "                    parent = button.find_parent(\"a\")\n",
    "                    if parent and parent.has_attr(\"href\"):\n",
    "                        pdf_url = parent[\"href\"]\n",
    "\n",
    "            filename = pdf_url.split(\"/\")[-1] if pdf_url else None\n",
    "\n",
    "            new_records.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"page_url\": page_url,\n",
    "                \"final_pdf_url\": pdf_url,\n",
    "                \"final_pdf_filename\": filename\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing row: {e}\")\n",
    "\n",
    "    print(f\"âŒ› scrape_cf_expanded executed in {timer() - t0:.2f} sec\")\n",
    "    return list_records, new_records\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# DOWNLOADER â€” with incremental CSV-safe saving (even if interrupted)\n",
    "# ======================================================\n",
    "\n",
    "def pdf_downloader(cf_urls, raw_pdf_folder, metadata_folder, metadata_csv):\n",
    "\n",
    "    t0 = timer()\n",
    "    os.makedirs(raw_pdf_folder, exist_ok=True)\n",
    "    os.makedirs(metadata_folder, exist_ok=True)\n",
    "\n",
    "    metadata_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load previous metadata safely\n",
    "    if os.path.exists(metadata_path):\n",
    "        old_df = pd.read_csv(metadata_path, dtype=str)\n",
    "        old_urls = set(old_df[\"final_pdf_url\"].dropna())\n",
    "        old_pages = set(old_df[\"page_url\"].dropna())\n",
    "    else:\n",
    "        old_df = pd.DataFrame()\n",
    "        old_urls = set()\n",
    "        old_pages = set()\n",
    "\n",
    "    all_new_records = []\n",
    "\n",
    "    # SCRAPE incremental\n",
    "    for url in cf_urls:\n",
    "        print(f\"\\nğŸŒ Scraping list page: {url}\")\n",
    "        list_records, new_page_records = scrape_cf(\n",
    "            url, already_scraped_pages=old_pages\n",
    "        )\n",
    "        all_new_records.extend(new_page_records)\n",
    "\n",
    "    if not all_new_records:\n",
    "        print(\"\\nğŸ” No new pages: skipping download.\")\n",
    "        print(f\"ğŸ“ Metadata unchanged: {metadata_path}\")\n",
    "        # â†’ return existing CSV as DataFrame\n",
    "        return pd.read_csv(metadata_path, dtype=str)\n",
    "\n",
    "    new_df = pd.DataFrame(all_new_records).dropna(subset=[\"final_pdf_url\"])\n",
    "    mask_new = ~new_df[\"final_pdf_url\"].isin(old_urls)\n",
    "    df_to_download = new_df[mask_new].copy()\n",
    "\n",
    "    # Sort oldest â†’ newest\n",
    "    df_to_download[\"date\"] = pd.to_datetime(df_to_download[\"date\"], dayfirst=True)\n",
    "    df_to_download = df_to_download.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nğŸ” Found {len(df_to_download)} new PDFs to download\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/pdf\"}\n",
    "\n",
    "    # ---- Incremental download + incremental CSV writing ----\n",
    "    temp_df = old_df.copy()\n",
    "\n",
    "    for i, row in df_to_download.iterrows():\n",
    "        pdf_url = row[\"final_pdf_url\"]\n",
    "        filename = row[\"final_pdf_filename\"]\n",
    "        page_url = row[\"page_url\"]\n",
    "        filepath = os.path.join(raw_pdf_folder, filename)\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(df_to_download)}] ğŸ“„ {filename}\")\n",
    "        print(f\"ğŸ”— {pdf_url}\")\n",
    "        \n",
    "        success = False\n",
    "\n",
    "        # Primary download attempt\n",
    "        try:\n",
    "            r = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"âœ… Saved {filename}\")\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Primary failed: {e}\")\n",
    "        \n",
    "        # Fallback attempt\n",
    "        if not success:\n",
    "            try:\n",
    "                print(\"ğŸ” Trying extended fallbackâ€¦\")\n",
    "                detail = requests.get(page_url, timeout=15)\n",
    "                dsoup = BeautifulSoup(detail.text, \"html.parser\")\n",
    "        \n",
    "                iframe_url = None\n",
    "        \n",
    "                # 1) <embed src=\"...pdf\">\n",
    "                embed = dsoup.find(\"embed\", src=lambda x: x and \".pdf\" in x.lower())\n",
    "                if embed:\n",
    "                    iframe_url = embed[\"src\"]\n",
    "        \n",
    "                # 2) <div data-pdf-src=\"...pdf\">\n",
    "                if not iframe_url:\n",
    "                    divpdf = dsoup.find(\"div\", attrs={\"data-pdf-src\": True})\n",
    "                    if divpdf and \".pdf\" in divpdf[\"data-pdf-src\"].lower():\n",
    "                        iframe_url = divpdf[\"data-pdf-src\"]\n",
    "        \n",
    "                # Normalize URL\n",
    "                if iframe_url and iframe_url.startswith(\"//\"):\n",
    "                    iframe_url = \"https:\" + iframe_url\n",
    "        \n",
    "                if iframe_url:\n",
    "                    print(f\"   â‡¢ fallback PDF URL: {iframe_url}\")\n",
    "        \n",
    "                    r2 = requests.get(iframe_url, headers=headers, timeout=20)\n",
    "                    r2.raise_for_status()\n",
    "        \n",
    "                    if \"pdf\" not in r2.headers.get(\"Content-Type\", \"\").lower():\n",
    "                        raise ValueError(\"Server returned HTML instead of PDF\")\n",
    "        \n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        f.write(r2.content)\n",
    "        \n",
    "                    print(f\"âœ… Saved via embed/data-pdf-src fallback: {filename}\")\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(\"âŒ No embed/data-pdf-src found\")\n",
    "        \n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Extended fallback failed: {e2}\")\n",
    "\n",
    "        # === Incremental update to metadata (even if interrupted later) ===\n",
    "        temp_df = pd.concat([temp_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        temp_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "        # avoid rate limit\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"ğŸ“ Metadata saved incrementally: {metadata_path}\")\n",
    "    print(f\"â±ï¸ Done in {round(timer() - t0, 2)} sec\")\n",
    "\n",
    "    # === RETURN THE CSV AS DATAFRAME ===\n",
    "    final_df = pd.read_csv(metadata_path, dtype=str)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a6dff3-a6a6-44b6-99a9-87f3be0a3fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ Scraping list page: https://cf.gob.pe/p/informes/\n",
      "âŒ› scrape_cf_expanded executed in 52.03 sec\n",
      "\n",
      "ğŸŒ Scraping list page: https://cf.gob.pe/p/comunicados/\n",
      "âŒ› scrape_cf_expanded executed in 25.52 sec\n",
      "\n",
      "ğŸ” Found 79 new PDFs to download\n",
      "\n",
      "[1/79] ğŸ“„ Informe_CF_N_001-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf\n",
      "âœ… Saved Informe_CF_N_001-2016.pdf\n",
      "\n",
      "[2/79] ğŸ“„ Informe_CF_N_002-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf\n",
      "âœ… Saved Informe_CF_N_002-2016.pdf\n",
      "\n",
      "[3/79] ğŸ“„ Informe_CF_N_003-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf\n",
      "âœ… Saved Informe_CF_N_003-2016.pdf\n",
      "\n",
      "[4/79] ğŸ“„ Informe_CF_N_005-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf\n",
      "âœ… Saved Informe_CF_N_005-2016.pdf\n",
      "\n",
      "[5/79] ğŸ“„ Informe_CF_N_004-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf\n",
      "âœ… Saved Informe_CF_N_004-2016.pdf\n",
      "\n",
      "[6/79] ğŸ“„ Informe_CF_N_006-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf\n",
      "âœ… Saved Informe_CF_N_006-2016.pdf\n",
      "\n",
      "[7/79] ğŸ“„ Informe_CF_N_007-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf\n",
      "âœ… Saved Informe_CF_N_007-2016.pdf\n",
      "\n",
      "[8/79] ğŸ“„ Informe_CF_N_008-2016.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf\n",
      "âœ… Saved Informe_CF_N_008-2016.pdf\n",
      "\n",
      "[9/79] ğŸ“„ INFORME_N_001-2017-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf\n",
      "âœ… Saved INFORME_N_001-2017-CF.pdf\n",
      "\n",
      "[10/79] ğŸ“„ INFORME_N_002-2017-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf\n",
      "âœ… Saved INFORME_N_002-2017-CF.pdf\n",
      "\n",
      "[11/79] ğŸ“„ INFORME_N_003-2017-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/INFORME_N_003-2017-CF.pdf\n",
      "âœ… Saved INFORME_N_003-2017-CF.pdf\n",
      "\n",
      "[12/79] ğŸ“„ INFORME_N_004-2017-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/pub/INFORME_N_004-2017-CF.pdf\n",
      "âœ… Saved INFORME_N_004-2017-CF.pdf\n",
      "\n",
      "[13/79] ğŸ“„ Informe-NÂ°-001-2018-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2018/01/Informe-NÂ°-001-2018-CF.pdf\n",
      "âœ… Saved Informe-NÂ°-001-2018-CF.pdf\n",
      "\n",
      "[14/79] ğŸ“„ Informe-anual-2017_CF_vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2018/04/Informe-anual-2017_CF_vf.pdf\n",
      "âœ… Saved Informe-anual-2017_CF_vf.pdf\n",
      "\n",
      "[15/79] ğŸ“„ CF-Pronunciamiento.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2018/05/CF-Pronunciamiento.pdf\n",
      "âœ… Saved CF-Pronunciamiento.pdf\n",
      "\n",
      "[16/79] ğŸ“„ Pronunciamiento-CF-DCRF-2017-12julio-enviada-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2018/07/Pronunciamiento-CF-DCRF-2017-12julio-enviada-1.pdf\n",
      "âœ… Saved Pronunciamiento-CF-DCRF-2017-12julio-enviada-1.pdf\n",
      "\n",
      "[17/79] ğŸ“„ CF-Pronunciamiento-MMM-2019-2022_15_8_2018-enviada-al-MEF-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2018/08/CF-Pronunciamiento-MMM-2019-2022_15_8_2018-enviada-al-MEF-1.pdf\n",
      "âœ… Saved CF-Pronunciamiento-MMM-2019-2022_15_8_2018-enviada-al-MEF-1.pdf\n",
      "\n",
      "[18/79] ğŸ“„ Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2019/04/Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\n",
      "âœ… Saved Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\n",
      "\n",
      "[19/79] ğŸ“„ InformeCF-IAPM-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2019/05/InformeCF-IAPM-VF.pdf\n",
      "âœ… Saved InformeCF-IAPM-VF.pdf\n",
      "\n",
      "[20/79] ğŸ“„ Pronunciamiento-CF-DCRF-2018-VF-publicada.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2019/06/Pronunciamiento-CF-DCRF-2018-VF-publicada.pdf\n",
      "âœ… Saved Pronunciamiento-CF-DCRF-2018-VF-publicada.pdf\n",
      "\n",
      "[21/79] ğŸ“„ CF-Pronunciamiento-MMM-2020-2023-VersiÃ³n-Final-9pm.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2019/08/CF-Pronunciamiento-MMM-2020-2023-VersiÃ³n-Final-9pm.pdf\n",
      "âœ… Saved CF-Pronunciamiento-MMM-2020-2023-VersiÃ³n-Final-9pm.pdf\n",
      "\n",
      "[22/79] ğŸ“„ CF-Pronunciamiento-DU-032-2019-VERSIÃ“N-FINAL.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/01/CF-Pronunciamiento-DU-032-2019-VERSIÃ“N-FINAL.pdf\n",
      "âœ… Saved CF-Pronunciamiento-DU-032-2019-VERSIÃ“N-FINAL.pdf\n",
      "\n",
      "[23/79] ğŸ“„ Pronunciamiento-DU-031-subnacionalvf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/01/Pronunciamiento-DU-031-subnacionalvf.pdf\n",
      "âœ… Saved Pronunciamiento-DU-031-subnacionalvf.pdf\n",
      "\n",
      "[24/79] ğŸ“„ Pronunciamiento-COVID-CF-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/04/Pronunciamiento-COVID-CF-VF.pdf\n",
      "âœ… Saved Pronunciamiento-COVID-CF-VF.pdf\n",
      "\n",
      "[25/79] ğŸ“„ Pronunciamiento-suspension-de-reglas-fiscales-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/04/Pronunciamiento-suspension-de-reglas-fiscales-VF.pdf\n",
      "âœ… Saved Pronunciamiento-suspension-de-reglas-fiscales-VF.pdf\n",
      "\n",
      "[26/79] ğŸ“„ Informe-Escenarios-_8.6.2020_FINAL.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/06/Informe-Escenarios-_8.6.2020_FINAL.pdf\n",
      "âœ… Saved Informe-Escenarios-_8.6.2020_FINAL.pdf\n",
      "\n",
      "[27/79] ğŸ“„ Informe-NÂ°-005-2020-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/06/Informe-NÂ°-005-2020-CF.pdf\n",
      "âœ… Saved Informe-NÂ°-005-2020-CF.pdf\n",
      "\n",
      "[28/79] ğŸ“„ Informe-ReglasGRsLs-VF-publicado.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/06/Informe-ReglasGRsLs-VF-publicado.pdf\n",
      "âœ… Saved Informe-ReglasGRsLs-VF-publicado.pdf\n",
      "\n",
      "[29/79] ğŸ“„ CF-Informe-MMM2124-cNotaAclaratoria-28-de-agosto-VF-publicada.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/08/CF-Informe-MMM2124-cNotaAclaratoria-28-de-agosto-VF-publicada.pdf\n",
      "âœ… Saved CF-Informe-MMM2124-cNotaAclaratoria-28-de-agosto-VF-publicada.pdf\n",
      "\n",
      "[30/79] ğŸ“„ CF-Informe-MMM2124-publicado-enviado-CF-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/09/CF-Informe-MMM2124-publicado-enviado-CF-VF.pdf\n",
      "âœ… Saved CF-Informe-MMM2124-publicado-enviado-CF-VF.pdf\n",
      "\n",
      "[31/79] ğŸ“„ Comunicado-Responsabilida-Fiscal-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/11/Comunicado-Responsabilida-Fiscal-VF.pdf\n",
      "âœ… Saved Comunicado-Responsabilida-Fiscal-VF.pdf\n",
      "\n",
      "[32/79] ğŸ“„ Comunicado-LeyONP-VersioÌn-Final-Publicada-convertido.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/12/Comunicado-LeyONP-VersioÌn-Final-Publicada-convertido.pdf\n",
      "âš ï¸ Primary failed: 404 Client Error: Not Found for url: https://cf.gob.pe/wp-content/uploads/2020/12/Comunicado-LeyONP-Versio%CC%81n-Final-Publicada-convertido.pdf\n",
      "ğŸ” Trying extended fallbackâ€¦\n",
      "   â‡¢ fallback PDF URL: https://cf.gob.pe/wp-content/uploads/2020/12/Comunicado-LeyONP-Version-Final-Publicada-convertido.pdf\n",
      "âœ… Saved via embed/data-pdf-src fallback: Comunicado-LeyONP-VersioÌn-Final-Publicada-convertido.pdf\n",
      "\n",
      "[33/79] ğŸ“„ Comunicado-CF-161220.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2020/12/Comunicado-CF-161220.pdf\n",
      "âœ… Saved Comunicado-CF-161220.pdf\n",
      "\n",
      "[34/79] ğŸ“„ CF-Informe-IP21-vF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/03/CF-Informe-IP21-vF.pdf\n",
      "âœ… Saved CF-Informe-IP21-vF.pdf\n",
      "\n",
      "[35/79] ğŸ“„ Comunicado-CAS-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/03/Comunicado-CAS-vf.pdf\n",
      "âœ… Saved Comunicado-CAS-vf.pdf\n",
      "\n",
      "[36/79] ğŸ“„ Comunicado-PIPs-G2G-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/04/Comunicado-PIPs-G2G-vf.pdf\n",
      "âœ… Saved Comunicado-PIPs-G2G-vf.pdf\n",
      "\n",
      "[37/79] ğŸ“„ Comunicado-N-03-2021-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/05/Comunicado-N-03-2021-CF.pdf\n",
      "âœ… Saved Comunicado-N-03-2021-CF.pdf\n",
      "\n",
      "[38/79] ğŸ“„ CF-Informe-IAPM21-vF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/05/CF-Informe-IAPM21-vF.pdf\n",
      "âœ… Saved CF-Informe-IAPM21-vF.pdf\n",
      "\n",
      "[39/79] ğŸ“„ Pronunciamiento-DCRF-2020-publicar.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/07/Pronunciamiento-DCRF-2020-publicar.pdf\n",
      "âœ… Saved Pronunciamiento-DCRF-2020-publicar.pdf\n",
      "\n",
      "[40/79] ğŸ“„ Pronunciamiento-DU-079-ReglasFiscales-2022-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/08/Pronunciamiento-DU-079-ReglasFiscales-2022-vf.pdf\n",
      "âœ… Saved Pronunciamiento-DU-079-ReglasFiscales-2022-vf.pdf\n",
      "\n",
      "[41/79] ğŸ“„ Pronunciamiento-MMM2022-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/08/Pronunciamiento-MMM2022-vf.pdf\n",
      "âœ… Saved Pronunciamiento-MMM2022-vf.pdf\n",
      "\n",
      "[42/79] ğŸ“„ Comunicado-CSsxDUs-v9_.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/10/Comunicado-CSsxDUs-v9_.pdf\n",
      "âœ… Saved Comunicado-CSsxDUs-v9_.pdf\n",
      "\n",
      "[43/79] ğŸ“„ Comunicado-CSsxDUs100-v4.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/10/Comunicado-CSsxDUs100-v4.pdf\n",
      "âœ… Saved Comunicado-CSsxDUs100-v4.pdf\n",
      "\n",
      "[44/79] ğŸ“„ Informe-GobSubnacionales-2021-vF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/12/Informe-GobSubnacionales-2021-vF.pdf\n",
      "âœ… Saved Informe-GobSubnacionales-2021-vF.pdf\n",
      "\n",
      "[45/79] ğŸ“„ Comunicado-DU-112.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2021/12/Comunicado-DU-112.pdf\n",
      "âœ… Saved Comunicado-DU-112.pdf\n",
      "\n",
      "[46/79] ğŸ“„ Comunicado-NÂ°-01-2022-CF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/01/Comunicado-NÂ°-01-2022-CF.pdf\n",
      "âœ… Saved Comunicado-NÂ°-01-2022-CF.pdf\n",
      "\n",
      "[47/79] ğŸ“„ Informe-PLReglasFiscales-2022.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/04/Informe-PLReglasFiscales-2022.pdf\n",
      "âœ… Saved Informe-PLReglasFiscales-2022.pdf\n",
      "\n",
      "[48/79] ğŸ“„ Pronunciamiento-IAPM2022-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/05/Pronunciamiento-IAPM2022-vf.pdf\n",
      "âœ… Saved Pronunciamiento-IAPM2022-vf.pdf\n",
      "\n",
      "[49/79] ğŸ“„ 2-ComunicadoCF-RetiroAFP-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/05/2-ComunicadoCF-RetiroAFP-1.pdf\n",
      "âœ… Saved 2-ComunicadoCF-RetiroAFP-1.pdf\n",
      "\n",
      "[50/79] ğŸ“„ Comunicado-3DeudaMagisterio-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/05/Comunicado-3DeudaMagisterio-1.pdf\n",
      "âœ… Saved Comunicado-3DeudaMagisterio-1.pdf\n",
      "\n",
      "[51/79] ğŸ“„ PronunciamientoDCRF-RFSN-2021-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/07/PronunciamientoDCRF-RFSN-2021-vf.pdf\n",
      "âœ… Saved PronunciamientoDCRF-RFSN-2021-vf.pdf\n",
      "\n",
      "[52/79] ğŸ“„ Opinion-MMM2023-2026-cNotaAclaratoria.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/08/Opinion-MMM2023-2026-cNotaAclaratoria.pdf\n",
      "âœ… Saved Opinion-MMM2023-2026-cNotaAclaratoria.pdf\n",
      "\n",
      "[53/79] ğŸ“„ Comunicado-del-Consejo-Fiscal-sobre-el-DU-023-2022-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/10/Comunicado-del-Consejo-Fiscal-sobre-el-DU-023-2022-vf.pdf\n",
      "âœ… Saved Comunicado-del-Consejo-Fiscal-sobre-el-DU-023-2022-vf.pdf\n",
      "\n",
      "[54/79] ğŸ“„ Comunicado-05-2022-Art79CPP-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2022/11/Comunicado-05-2022-Art79CPP-vf.pdf\n",
      "âœ… Saved Comunicado-05-2022-Art79CPP-vf.pdf\n",
      "\n",
      "[55/79] ğŸ“„ Comunicado-01_2023-TC-intereses-moratorios-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2023/03/Comunicado-01_2023-TC-intereses-moratorios-vf.pdf\n",
      "âœ… Saved Comunicado-01_2023-TC-intereses-moratorios-vf.pdf\n",
      "\n",
      "[56/79] ğŸ“„ Informe01-OpinionCFIAPM2023.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2023/05/Informe01-OpinionCFIAPM2023.pdf\n",
      "âœ… Saved Informe01-OpinionCFIAPM2023.pdf\n",
      "\n",
      "[57/79] ğŸ“„ Pronunciamiento-FinanzasPublicas2022-vF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2023/06/Pronunciamiento-FinanzasPublicas2022-vF.pdf\n",
      "âœ… Saved Pronunciamiento-FinanzasPublicas2022-vF.pdf\n",
      "\n",
      "[58/79] ğŸ“„ Pronunciamiento-MMM-24-27-VF-nota-aclaratoria-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2023/08/Pronunciamiento-MMM-24-27-VF-nota-aclaratoria-VF.pdf\n",
      "âœ… Saved Pronunciamiento-MMM-24-27-VF-nota-aclaratoria-VF.pdf\n",
      "\n",
      "[59/79] ğŸ“„ Comunicado-022023-CF-v4.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2023/12/Comunicado-022023-CF-v4.pdf\n",
      "âœ… Saved Comunicado-022023-CF-v4.pdf\n",
      "\n",
      "[60/79] ğŸ“„ Comunicado-012024-RiesgosFiscales-v4-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/02/Comunicado-012024-RiesgosFiscales-v4-1.pdf\n",
      "âœ… Saved Comunicado-012024-RiesgosFiscales-v4-1.pdf\n",
      "\n",
      "[61/79] ğŸ“„ Comunicado-022024-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/04/Comunicado-022024-vf.pdf\n",
      "âœ… Saved Comunicado-022024-vf.pdf\n",
      "\n",
      "[62/79] ğŸ“„ Pronunciamiento-IAPM24-27-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/05/Pronunciamiento-IAPM24-27-vf.pdf\n",
      "âœ… Saved Pronunciamiento-IAPM24-27-vf.pdf\n",
      "\n",
      "[63/79] ğŸ“„ Comunicado-03-2024-Reformapensiones-vf-2.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/06/Comunicado-03-2024-Reformapensiones-vf-2.pdf\n",
      "âœ… Saved Comunicado-03-2024-Reformapensiones-vf-2.pdf\n",
      "\n",
      "[64/79] ğŸ“„ Informe-DCRF2023-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/07/Informe-DCRF2023-vf.pdf\n",
      "âœ… Saved Informe-DCRF2023-vf.pdf\n",
      "\n",
      "[65/79] ğŸ“„ Pronunciamiento_Subnacional_2024_240624.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/07/Pronunciamiento_Subnacional_2024_240624.pdf\n",
      "âœ… Saved Pronunciamiento_Subnacional_2024_240624.pdf\n",
      "\n",
      "[66/79] ğŸ“„ Informe-DL1621-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/07/Informe-DL1621-vf.pdf\n",
      "âœ… Saved Informe-DL1621-vf.pdf\n",
      "\n",
      "[67/79] ğŸ“„ Pronunciamiento-pMMM-25-28-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/08/Pronunciamiento-pMMM-25-28-vf.pdf\n",
      "âœ… Saved Pronunciamiento-pMMM-25-28-vf.pdf\n",
      "\n",
      "[68/79] ğŸ“„ Comunicado042024-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/10/Comunicado042024-VF.pdf\n",
      "âœ… Saved Comunicado042024-VF.pdf\n",
      "\n",
      "[69/79] ğŸ“„ Comunicado-Presupuesto-y-Otros-vf-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2024/12/Comunicado-Presupuesto-y-Otros-vf-1.pdf\n",
      "âœ… Saved Comunicado-Presupuesto-y-Otros-vf-1.pdf\n",
      "\n",
      "[70/79] ğŸ“„ Comunicado-01-2025-ReglasFiscales-vf-1.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/01/Comunicado-01-2025-ReglasFiscales-vf-1.pdf\n",
      "âœ… Saved Comunicado-01-2025-ReglasFiscales-vf-1.pdf\n",
      "\n",
      "[71/79] ğŸ“„ Comunicado-02-InciativasLegislativas-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/04/Comunicado-02-InciativasLegislativas-vf.pdf\n",
      "âœ… Saved Comunicado-02-InciativasLegislativas-vf.pdf\n",
      "\n",
      "[72/79] ğŸ“„ Comunicado-03-IAPM-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/05/Comunicado-03-IAPM-vf.pdf\n",
      "âœ… Saved Comunicado-03-IAPM-vf.pdf\n",
      "\n",
      "[73/79] ğŸ“„ Pronunciamiento-IAPM25-28-VF.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/05/Pronunciamiento-IAPM25-28-VF.pdf\n",
      "âœ… Saved Pronunciamiento-IAPM25-28-VF.pdf\n",
      "\n",
      "[74/79] ğŸ“„ Comunicado-04Infraestructura-v6.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/05/Comunicado-04Infraestructura-v6.pdf\n",
      "âœ… Saved Comunicado-04Infraestructura-v6.pdf\n",
      "\n",
      "[75/79] ğŸ“„ Comunicado-05-2025CF-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/06/Comunicado-05-2025CF-vf.pdf\n",
      "âœ… Saved Comunicado-05-2025CF-vf.pdf\n",
      "\n",
      "[76/79] ğŸ“„ Pronunciamiento-Subnacional2024-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/06/Pronunciamiento-Subnacional2024-vf.pdf\n",
      "âœ… Saved Pronunciamiento-Subnacional2024-vf.pdf\n",
      "\n",
      "[77/79] ğŸ“„ Informe-DCRF2024-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/07/Informe-DCRF2024-vf.pdf\n",
      "âœ… Saved Informe-DCRF2024-vf.pdf\n",
      "\n",
      "[78/79] ğŸ“„ Informe-sobre-MMM-26-29-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/08/Informe-sobre-MMM-26-29-vf.pdf\n",
      "âœ… Saved Informe-sobre-MMM-26-29-vf.pdf\n",
      "\n",
      "[79/79] ğŸ“„ Comunicado-Congreso-vf.pdf\n",
      "ğŸ”— https://cf.gob.pe/wp-content/uploads/2025/10/Comunicado-Congreso-vf.pdf\n",
      "âœ… Saved Comunicado-Congreso-vf.pdf\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "ğŸ“ Metadata saved incrementally: metadata\\cf_metadata.csv\n",
      "â±ï¸ Done in 253.42 sec\n"
     ]
    }
   ],
   "source": [
    "cf_urls = [\n",
    "    \"https://cf.gob.pe/p/informes/\",\n",
    "    \"https://cf.gob.pe/p/comunicados/\"\n",
    "]\n",
    "\n",
    "metadata_df = pdf_downloader(\n",
    "    cf_urls=cf_urls,\n",
    "    raw_pdf_folder=raw_data_subfolder,\n",
    "    metadata_folder=metadata_folder,\n",
    "    metadata_csv=\"cf_metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d174521-b611-4a90-b242-18e39551b61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>page_url</th>\n",
       "      <th>final_pdf_url</th>\n",
       "      <th>final_pdf_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>Informe CF NÂ° 001-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>Informe_CF_N_001-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>Informe CF NÂ° 002-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>Informe_CF_N_002-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>Informe CF NÂ° 003-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>Informe_CF_N_003-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF NÂ° 005-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>Informe_CF_N_005-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF NÂ° 004-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>Informe_CF_N_004-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>Informe CF NÂ° 006-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf</td>\n",
       "      <td>Informe_CF_N_006-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>Informe CF NÂ° 007-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf</td>\n",
       "      <td>Informe_CF_N_007-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>Informe CF NÂ° 008-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf</td>\n",
       "      <td>Informe_CF_N_008-2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>Informe CF NÂ° 001-2017 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_001-2017-CF.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>Informe CF NÂ° 002-2017 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf</td>\n",
       "      <td>INFORME_N_002-2017-CF.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2016-01-28  Informe CF NÂ° 001-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "1  2016-04-15  Informe CF NÂ° 002-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "2  2016-06-23  Informe CF NÂ° 003-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "3  2016-08-18  Informe CF NÂ° 005-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "4  2016-08-18  Informe CF NÂ° 004-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "5  2016-08-26  Informe CF NÂ° 006-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "6  2016-10-04  Informe CF NÂ° 007-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "7  2016-12-26  Informe CF NÂ° 008-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "8  2017-05-03  Informe CF NÂ° 001-2017 â€“ OpiniÃ³n del Consejo F...   \n",
       "9  2017-05-09  Informe CF NÂ° 002-2017 â€“ OpiniÃ³n del Consejo F...   \n",
       "\n",
       "                                            page_url  \\\n",
       "0  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "1  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "2  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "3  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "4  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "5  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "6  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "7  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "8  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "9  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "\n",
       "                                     final_pdf_url         final_pdf_filename  \n",
       "0  https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf  Informe_CF_N_001-2016.pdf  \n",
       "1  https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf  Informe_CF_N_002-2016.pdf  \n",
       "2  https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf  Informe_CF_N_003-2016.pdf  \n",
       "3  https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf  Informe_CF_N_005-2016.pdf  \n",
       "4  https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf  Informe_CF_N_004-2016.pdf  \n",
       "5  https://cf.gob.pe/pub/Informe_CF_N_006-2016.pdf  Informe_CF_N_006-2016.pdf  \n",
       "6  https://cf.gob.pe/pub/Informe_CF_N_007-2016.pdf  Informe_CF_N_007-2016.pdf  \n",
       "7  https://cf.gob.pe/pub/Informe_CF_N_008-2016.pdf  Informe_CF_N_008-2016.pdf  \n",
       "8  https://cf.gob.pe/pub/INFORME_N_001-2017-CF.pdf  INFORME_N_001-2017-CF.pdf  \n",
       "9  https://cf.gob.pe/pub/INFORME_N_002-2017-CF.pdf  INFORME_N_002-2017-CF.pdf  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c597d21",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### Delete selected documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7254f2a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Lista de archivos a eliminar\n",
    "pdfs_to_remove = [\n",
    "    \"Informe-anual-2017_CF_vf.pdf\", # This is a way too long document containing statistical analaysis. We're focus only on text as comunicados from CF.\n",
    "    \"Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafdf3a0",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_pdfs(folder_path, filenames_to_remove):\n",
    "    \"\"\"\n",
    "    Deletes specific unwanted PDF files from a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, the directory containing the PDFs\n",
    "    - filenames_to_remove: list of str, filenames to delete\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    removed_count = 0\n",
    "\n",
    "    print(f\"ğŸ§¹ Removing in: {folder_path}\")\n",
    "    for filename in filenames_to_remove:\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            os.remove(full_path)\n",
    "            print(f\"ğŸ—‘ï¸ Deleted: {filename}\")\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            print(f\"âš ï¸ File not found: {filename}\")\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    print(\"\\nğŸ“Š Summary:\")\n",
    "\n",
    "    print(f\"\\nğŸ§¹ Cleanup complete. Total files removed: {removed_count}\")\n",
    "    print(f\"â±ï¸ Time taken: {t1 - t0:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caca1ebf",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Removing in: data\\raw\n",
      "ğŸ—‘ï¸ Deleted: Informe-anual-2017_CF_vf.pdf\n",
      "ğŸ—‘ï¸ Deleted: Informe-anual-del-Consejo-Fiscal-2018-version-final1.pdf\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "\n",
      "ğŸ§¹ Cleanup complete. Total files removed: 2\n",
      "â±ï¸ Time taken: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "remove_unwanted_pdfs(raw_data_subfolder, pdfs_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433e30b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Clasificando entre scanned and editable PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a037",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Please, compruebe que la carpeta \"\" contenga scaneados y que la carpeta \"\" ediatables. En caso de que no se haya clasificado correctamente los pdfs, agreguelos manualemnte. Esto es vital para los prÃ³ximos cÃ³digos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5ef1b9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF, used to extract text from PDFs\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def is_editable_pdf(file_path, min_text_length=20):\n",
    "    \"\"\"Check if a PDF contains extractable text (editable).\"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            return len(\"\".join(page.get_text() for page in doc).strip()) >= min_text_length\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def classify_pdfs_by_type(classification_folder, metadata_folder, metadata_csv=\"cf_metadata\"):\n",
    "    \"\"\"\n",
    "    Classifies PDF files from a given directory into 'editable' and 'scanned' subfolders, \n",
    "    and updates a metadata CSV file with the type of each document (editable or scanned).\n",
    "\n",
    "    Parameters:\n",
    "    - classification_folder: str, the directory where 'editable' and 'scanned' subfolders will be created.\n",
    "    - metadata_folder: str, the directory where the metadata CSV file is located.\n",
    "    - metadata_csv: str, the name of the CSV file containing metadata (without the '.csv' extension).\n",
    "    \"\"\"\n",
    "    # Ensure classification_folder is a list, even if a single folder is passed\n",
    "    if isinstance(classification_folder, str):\n",
    "        classification_folder = [classification_folder]\n",
    "\n",
    "    # Create the 'editable' and 'scanned' subfolders within the classification_folder\n",
    "    output_dir_editable = os.path.join(classification_folder[0], \"editable\")\n",
    "    output_dir_scanned = os.path.join(classification_folder[0], \"scanned\")\n",
    "    os.makedirs(output_dir_editable, exist_ok=True)\n",
    "    os.makedirs(output_dir_scanned, exist_ok=True)\n",
    "\n",
    "    total_files = 0\n",
    "    scanned_count = 0\n",
    "    editable_count = 0\n",
    "\n",
    "    # Add .csv extension to metadata_csv if not provided\n",
    "    metadata_csv_path = os.path.join(metadata_folder, f\"{metadata_csv}.csv\")\n",
    "\n",
    "    # Load the metadata CSV file\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # Add a new \"type\" column if it doesn't exist\n",
    "    if 'type' not in metadata_df.columns:\n",
    "        metadata_df['type'] = ''\n",
    "\n",
    "    t0 = timer()\n",
    "    print(\"ğŸ” Starting PDF classification...\\n\")\n",
    "\n",
    "    # Iterate through the provided folders\n",
    "    for folder in classification_folder:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                total_files += 1\n",
    "                pdf_path = os.path.join(folder, filename)\n",
    "\n",
    "                # Classify and move the PDF to the appropriate folder\n",
    "                if is_editable_pdf(pdf_path):\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_editable, filename))\n",
    "                    editable_count += 1\n",
    "                    file_type = \"editable\"\n",
    "                else:\n",
    "                    shutil.move(pdf_path, os.path.join(output_dir_scanned, filename))\n",
    "                    scanned_count += 1\n",
    "                    file_type = \"scanned\"\n",
    "\n",
    "                # Update the metadata CSV with the file type\n",
    "                metadata_df.loc[metadata_df['final_pdf_filename'] == filename, 'type'] = file_type\n",
    "\n",
    "    # Save the updated CSV back to the metadata folder\n",
    "    metadata_df.to_csv(metadata_csv_path, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "\n",
    "    # Print a summary\n",
    "    print(\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"ğŸ“„ Total PDFs processed: {total_files}\")\n",
    "    print(f\"ğŸ’» Editable PDFs: {editable_count}\")\n",
    "    print(f\"ğŸ–¨ï¸ Scanned PDFs: {scanned_count}\")\n",
    "    print(f\"ğŸ“ Saved editable PDFs in: '{output_dir_editable}'\")\n",
    "    print(f\"ğŸ“ Saved scanned PDFs in: '{output_dir_scanned}'\")\n",
    "    print(f\"â±ï¸ Time taken: {t1 - t0:.2f} seconds\")\n",
    "    print(f\"ğŸ“‘ Metadata CSV updated: '{metadata_csv_path}'\")\n",
    "\n",
    "    # Return the updated CSV as a DataFrame for inspection\n",
    "    return metadata_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508d2041",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting PDF classification...\n",
      "\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "ğŸ“„ Total PDFs processed: 77\n",
      "ğŸ’» Editable PDFs: 64\n",
      "ğŸ–¨ï¸ Scanned PDFs: 13\n",
      "ğŸ“ Saved editable PDFs in: 'data\\raw\\editable'\n",
      "ğŸ“ Saved scanned PDFs in: 'data\\raw\\scanned'\n",
      "â±ï¸ Time taken: 2.24 seconds\n",
      "ğŸ“‘ Metadata CSV updated: 'metadata\\cf_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the folder paths\n",
    "classification_folder = raw_data_subfolder\n",
    "metadata_folder = metadata_folder\n",
    "metadata_csv = \"cf_metadata\"  # Note: without the \".csv\" extension\n",
    "\n",
    "# Call the function to classify PDFs, update the CSV, and return the updated DataFrame\n",
    "updated_metadata_df = classify_pdfs_by_type(classification_folder, metadata_folder, metadata_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50629e11-9424-4721-b548-3b35b4b00add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>page_url</th>\n",
       "      <th>final_pdf_url</th>\n",
       "      <th>final_pdf_filename</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>Informe CF NÂ° 001-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>Informe_CF_N_001-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>Informe CF NÂ° 002-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>Informe_CF_N_002-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>Informe CF NÂ° 003-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>Informe_CF_N_003-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF NÂ° 005-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>Informe_CF_N_005-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>Informe CF NÂ° 004-2016 â€“ OpiniÃ³n del Consejo F...</td>\n",
       "      <td>https://cf.gob.pe/documentos/informes/informe-...</td>\n",
       "      <td>https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>Informe_CF_N_004-2016.pdf</td>\n",
       "      <td>scanned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2016-01-28  Informe CF NÂ° 001-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "1  2016-04-15  Informe CF NÂ° 002-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "2  2016-06-23  Informe CF NÂ° 003-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "3  2016-08-18  Informe CF NÂ° 005-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "4  2016-08-18  Informe CF NÂ° 004-2016 â€“ OpiniÃ³n del Consejo F...   \n",
       "\n",
       "                                            page_url  \\\n",
       "0  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "1  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "2  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "3  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "4  https://cf.gob.pe/documentos/informes/informe-...   \n",
       "\n",
       "                                     final_pdf_url         final_pdf_filename  \\\n",
       "0  https://cf.gob.pe/pub/Informe_CF_N_001-2016.pdf  Informe_CF_N_001-2016.pdf   \n",
       "1  https://cf.gob.pe/pub/Informe_CF_N_002-2016.pdf  Informe_CF_N_002-2016.pdf   \n",
       "2  https://cf.gob.pe/pub/Informe_CF_N_003-2016.pdf  Informe_CF_N_003-2016.pdf   \n",
       "3  https://cf.gob.pe/pub/Informe_CF_N_005-2016.pdf  Informe_CF_N_005-2016.pdf   \n",
       "4  https://cf.gob.pe/pub/Informe_CF_N_004-2016.pdf  Informe_CF_N_004-2016.pdf   \n",
       "\n",
       "      type  \n",
       "0  scanned  \n",
       "1  scanned  \n",
       "2  scanned  \n",
       "3  scanned  \n",
       "4  scanned  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the updated DataFrame in Jupyter Notebook or JupyterLab\n",
    "updated_metadata_df.head()  # Display the first few rows of the updated CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c27811",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def combine_metadata(csv_reports=\"cf_reports.csv\", csv_announcements=\"cf_announcements.csv\", output_csv=\"cf_pdfs_metadata.csv\"):\n",
    "    \"\"\"\n",
    "    Combines metadata from two separate CSV files (reports and announcements) into one.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_reports: str, path to the reports metadata CSV\n",
    "    - csv_announcements: str, path to the announcements metadata CSV\n",
    "    - output_csv: str, path for the combined output CSV\n",
    "\n",
    "    Saves:\n",
    "    - A single CSV file containing all metadata under the name specified in output_csv\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    print(\"ğŸ“¥ Loading metadata...\")\n",
    "\n",
    "    df_reports = pd.read_csv(csv_reports)\n",
    "    df_announcements = pd.read_csv(csv_announcements)\n",
    "\n",
    "    df_combined = pd.concat([df_reports, df_announcements], ignore_index=True)\n",
    "    df_combined.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"âœ… Combined metadata saved as '{output_csv}'\")\n",
    "    print(f\"ğŸ§¾ Total records: {len(df_combined)}\")\n",
    "    print(f\"â±ï¸ Time taken: {t1 - t0:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ba3d9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "combine_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293849d3",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-3\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.3 Split into paragraphs\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a0469-4b91-4b9a-aab7-7054986addd1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73aeab-af66-47bc-88f1-33400c1bf941",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb714-1ab9-4f90-a3b9-56c6f8a6c717",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17329117",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Editable PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b607c22",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def process_editable_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", output_csv=\"raw_editable_pdfs_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Processes editable PDF documents, extracting structured paragraphs and saving to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, path to the folder containing editable PDFs\n",
    "    - metadata_csv_path: str, path to the CSV file with document metadata\n",
    "    - output_csv: str, output CSV filename to save extracted paragraphs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted structured text and metadata\n",
    "    \"\"\"\n",
    "    t0 = timer()\n",
    "    print(\"ğŸ“¥ Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    # --- Extract doc_type, doc_id, year from title ---\n",
    "    def extract_doc_info(row):\n",
    "        title = row[\"title\"]\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[Â°Âºo]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "\n",
    "    print(\"ğŸ§  Metadata enriched. Starting paragraph extraction...\\n\")\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            print(f\"ğŸ“„ Processing: {idx}. {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                paragraph_counter = 1\n",
    "                anexo_found = False\n",
    "\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    if anexo_found:\n",
    "                        break\n",
    "\n",
    "                    # Extract words with their size and vertical position\n",
    "                    words = page.extract_words(extra_attrs=[\"size\", \"top\"])\n",
    "                    FONT_MIN = 11.0\n",
    "                    FONT_MAX = 11.9\n",
    "                    clean_words = [w for w in words if FONT_MIN <= w[\"size\"] <= FONT_MAX]\n",
    "                    if not clean_words:\n",
    "                        continue\n",
    "\n",
    "                    # Group words by their vertical position to form lines\n",
    "                    lines_dict = {}\n",
    "                    for word in clean_words:\n",
    "                        line_top = round(word[\"top\"], 1)\n",
    "                        lines_dict.setdefault(line_top, []).append(word[\"text\"])\n",
    "\n",
    "                    lines = [\n",
    "                        \" \".join(words).strip()\n",
    "                        for _, words in sorted(lines_dict.items())\n",
    "                        if words\n",
    "                    ]\n",
    "\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    page_text = \"\\n\".join(lines)\n",
    "\n",
    "                    # ğŸš« Stop extraction at \"Anexo\"\n",
    "                    match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                    if match:\n",
    "                        page_text = page_text[:match.start()].strip()\n",
    "                        anexo_found = True\n",
    "                        print(f\"ğŸ›‘ 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                    # Paragraph segmentation\n",
    "                    lines = page_text.strip().split(\"\\n\")\n",
    "                    lines = [line.strip() for line in lines if line.strip()]\n",
    "                    paragraph_lines = []\n",
    "\n",
    "                    for i, line in enumerate(lines):\n",
    "                        is_new_paragraph = (\n",
    "                            line.startswith(\"â€¢\")\n",
    "                            or line.startswith(\"â¢\")\n",
    "                            or (i > 0 and lines[i - 1].strip().endswith(\".\"))\n",
    "                            or (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                        )\n",
    "\n",
    "                        if is_new_paragraph:\n",
    "                            if paragraph_lines:\n",
    "                                all_records.append({\n",
    "                                    \"filename\": filename,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"paragraph_id\": paragraph_counter,\n",
    "                                    \"text\": \" \".join(paragraph_lines).strip()\n",
    "                                })\n",
    "                                paragraph_counter += 1\n",
    "                            paragraph_lines = [line]\n",
    "                        else:\n",
    "                            paragraph_lines.append(line)\n",
    "\n",
    "                    if paragraph_lines:\n",
    "                        all_records.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"page\": page_num,\n",
    "                            \"paragraph_id\": paragraph_counter,\n",
    "                            \"text\": \" \".join(paragraph_lines).strip()\n",
    "                        })\n",
    "                        paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    # Merge with metadata\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    t1 = timer()\n",
    "    print(f\"\\nâœ… Extraction complete. Total paragraphs: {len(df)}\")\n",
    "    print(f\"ğŸ’¾ Saved to: {output_csv}\")\n",
    "    print(f\"â±ï¸ Time taken: {t1 - t0:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a354e5d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable = process_editable_pdfs(\"editable_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f3936",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbd2f",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af8df4a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd8030",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "\"\"\"\n",
    "Procesa PDFs escaneados usando OCR para extraer pÃ¡rrafos por pÃ¡gina y metadatos.\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Ruta a la carpeta con archivos PDF escaneados.\n",
    "    dpi (int): ResoluciÃ³n al convertir PDF a imagen.\n",
    "    lang (str): Idioma para el OCR ('spa' para espaÃ±ol, 'eng' para inglÃ©s, etc.).\n",
    "\n",
    "Returns:\n",
    "    pd.DataFrame: DataFrame con columnas:\n",
    "        ['filename', 'year', 'date', 'announcement', 'page', 'paragraph_id', 'text']\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eec680",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Puede tardar un poco mÃ¡s. Se excluye pies de pagina con detecciÃ³n visual de lineas que marcan el inicio. Se pÃ¡ginas de anexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e58f5",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.2, y_range=(0.5, 0.85), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line that likely marks the beginning of the footer in a scanned PDF page.\n",
    "    \n",
    "    Parameters:\n",
    "        image (PIL.Image): Page image to analyze.\n",
    "        min_length_ratio (float): Minimum line length relative to image width.\n",
    "        y_range (tuple): Vertical range (as a proportion of image height) where footer lines are expected.\n",
    "        debug_path (str): Optional path to save debug image with the detected line.\n",
    "    \n",
    "    Returns:\n",
    "        int: Y-coordinate to crop the image above the footer line, or image height if no line is found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=100,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 3 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # Default: no footer line detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c36df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a635301",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Major function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac228",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Utilities ===\n",
    "\n",
    "def detect_cut_line_y(image, min_length_ratio=0.17, y_range=(0.55, 0.90), debug_path=None):\n",
    "    \"\"\"\n",
    "    Detects a horizontal line likely indicating the beginning of the footer in scanned PDFs.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Page image.\n",
    "        min_length_ratio (float): Minimum length of line relative to image width.\n",
    "        y_range (tuple): Vertical range to search (proportional to height).\n",
    "        debug_path (str): Optional file path to save a debug image with detected line.\n",
    "\n",
    "    Returns:\n",
    "        int: Y-coordinate of the detected line, or image height if none found.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh, 1, np.pi / 180, threshold=80,\n",
    "        minLineLength=int(image.width * min_length_ratio), maxLineGap=5\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        height = image.height\n",
    "        min_y, max_y = int(height * y_range[0]), int(height * y_range[1])\n",
    "\n",
    "        horizontal_lines = [\n",
    "            (x1, y1, x2, y2) for x1, y1, x2, y2 in lines[:, 0]\n",
    "            if abs(y1 - y2) <= 5 and min_y <= y1 <= max_y\n",
    "        ]\n",
    "\n",
    "        if horizontal_lines:\n",
    "            best_line = min(horizontal_lines, key=lambda l: l[1])\n",
    "            if debug_path:\n",
    "                img_dbg = image.copy()\n",
    "                dbg_np = np.array(img_dbg)\n",
    "                cv2.line(dbg_np, (best_line[0], best_line[1]), (best_line[2], best_line[3]), (0, 0, 255), 2)\n",
    "                cv2.imwrite(debug_path, cv2.cvtColor(dbg_np, cv2.COLOR_RGB2BGR))\n",
    "            return best_line[1]\n",
    "\n",
    "    return image.height  # No line detected â†’ return full height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a24cd2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === OCR Processing for Scanned PDFs ===\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from scanned PDFs using OCR, excluding footers and annexes.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Folder with scanned PDFs.\n",
    "        metadata_csv_path (str): Path to metadata CSV file.\n",
    "        dpi (int): Resolution used to convert PDFs to images.\n",
    "        lang (str): OCR language code.\n",
    "        debug (bool): Whether to save debug images with detected lines.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Paragraph-level extracted data.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¥ Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[Â°Âºo]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"ğŸ§  Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"ğŸ–¨ï¸ OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                debug_path = None\n",
    "                if debug:\n",
    "                    os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                    debug_path = f\"debug_lines/{filename}_page_{page_num}.png\"\n",
    "\n",
    "                cut_y = detect_cut_line_y(image, debug_path=debug_path)\n",
    "                cropped_img = image.crop((0, 0, image.width, cut_y))\n",
    "\n",
    "                page_text = pytesseract.image_to_string(cropped_img, lang=lang)\n",
    "\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # Stop at 'Anexo'\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"ğŸ›‘ 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"â€¢\") or line.startswith(\"â¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\nâœ… OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51a39",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# === Run and Save ===\n",
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"scanned_pdf\",\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"ğŸ’¾ Saved as 'raw_scanned_pdfs_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deef5c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93432a87",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# NUEVA UTILIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6963",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def filter_text_body_dynamic(\n",
    "    image,\n",
    "    size_threshold_percentile=40,\n",
    "    merge_dist=20,\n",
    "    expand_margin=10,\n",
    "    debug_path=None,\n",
    "    audit_csv_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Filtra regiones de texto fuera del cuerpo principal del documento mediante detecciÃ³n dinÃ¡mica.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Imagen de entrada.\n",
    "        size_threshold_percentile (int): Percentil mÃ­nimo de altura para considerar caja vÃ¡lida.\n",
    "        merge_dist (int): Distancia mÃ¡xima entre cajas para considerar agrupaciÃ³n.\n",
    "        expand_margin (int): ExpansiÃ³n del marco del cuerpo principal.\n",
    "        debug_path (str): Ruta para guardar imagen de depuraciÃ³n.\n",
    "        audit_csv_path (str): Ruta para guardar CSV con info de cajas.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Imagen filtrada.\n",
    "    \"\"\"\n",
    "    img_rgb = np.array(image.convert(\"RGB\"))\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    if not boxes:\n",
    "        return image\n",
    "\n",
    "    heights = [h for (_, _, _, h) in boxes]\n",
    "    height_threshold = np.percentile(heights, size_threshold_percentile)\n",
    "\n",
    "    filtered_boxes = [(x, y, w, h) for (x, y, w, h) in boxes if h >= height_threshold]\n",
    "    if not filtered_boxes:\n",
    "        return image\n",
    "\n",
    "    centers = np.array([[x + w//2, y + h//2] for (x, y, w, h) in filtered_boxes])\n",
    "    clustering = DBSCAN(eps=merge_dist, min_samples=3).fit(centers)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    largest_label = pd.Series(labels).value_counts().idxmax()\n",
    "    main_group_boxes = [box for i, box in enumerate(filtered_boxes) if labels[i] == largest_label]\n",
    "\n",
    "    x_min = max(0, min(x for x, _, _, _ in main_group_boxes) - expand_margin)\n",
    "    x_max = max(x + w for x, _, w, _ in main_group_boxes) + expand_margin\n",
    "    y_min = max(0, min(y for _, y, _, _ in main_group_boxes) - expand_margin)\n",
    "    y_max = max(y + h for _, y, _, h in main_group_boxes) + expand_margin\n",
    "\n",
    "    mask = np.zeros_like(gray)\n",
    "    accepted, rejected = [], []\n",
    "\n",
    "    for x, y, w, h in boxes:\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "        if x_min <= cx <= x_max and y_min <= cy <= y_max:\n",
    "            accepted.append((x, y, w, h))\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "        else:\n",
    "            rejected.append((x, y, w, h))\n",
    "\n",
    "    result = cv2.bitwise_and(img_rgb, img_rgb, mask=mask)\n",
    "    filtered_image = Image.fromarray(result)\n",
    "\n",
    "    if debug_path:\n",
    "        debug_img = img_rgb.copy()\n",
    "        for x, y, w, h in accepted:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        for x, y, w, h in rejected:\n",
    "            cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "        cv2.rectangle(debug_img, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "        cv2.imwrite(debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    if audit_csv_path:\n",
    "        records = []\n",
    "        for x, y, w, h in accepted:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"accepted\"})\n",
    "        for x, y, w, h in rejected:\n",
    "            records.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h, \"status\": \"rejected\"})\n",
    "        pd.DataFrame(records).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def process_scanned_pdfs(folder_path, metadata_csv_path=\"cf_pdfs_metadata.csv\", dpi=300, lang='spa', debug=True):\n",
    "    print(\"ğŸ“¥ Loading metadata...\")\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "    def extract_doc_info(row):\n",
    "        title = row.get(\"title\", \"\")\n",
    "        match = re.search(r\"\\b(Informe|Comunicado)\\b(?:\\s+CF)?(?:\\s+(?:N[Â°Âºo]|No))?\\s*(\\d{2,4})\", title, re.IGNORECASE)\n",
    "        doc_type = match.group(1).capitalize() if match else None\n",
    "        doc_id = match.group(2) if match and match.lastindex >= 2 else None\n",
    "        year_match = re.search(r\"\\b(\\d{4})\\b\", str(row.get(\"date\", \"\")))\n",
    "        year = year_match.group(1) if year_match else None\n",
    "        return pd.Series({\"doc_type\": doc_type, \"doc_id\": doc_id, \"year\": year})\n",
    "\n",
    "    metadata_df[[\"doc_type\", \"doc_id\", \"year\"]] = metadata_df.apply(extract_doc_info, axis=1)\n",
    "    print(\"ğŸ§  Metadata enriched. Starting OCR paragraph extraction...\\n\")\n",
    "\n",
    "    all_records = []\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx, filename in enumerate(sorted(filenames), start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"ğŸ–¨ï¸ OCR Processing ({idx}/{total_files}): {filename}\")\n",
    "        try:\n",
    "            images = convert_from_path(file_path, dpi=dpi)\n",
    "            paragraph_counter = 1\n",
    "            anexo_found = False\n",
    "\n",
    "            for page_num, image in enumerate(images, start=1):\n",
    "                if anexo_found:\n",
    "                    break\n",
    "\n",
    "                os.makedirs(\"debug_lines\", exist_ok=True)\n",
    "                debug_path = f\"debug_lines/{filename}_page_{page_num}.png\" if debug else None\n",
    "                audit_csv_path = f\"debug_lines/{filename}_page_{page_num}_boxes.csv\" if debug else None\n",
    "\n",
    "                # ğŸ§  NUEVA funciÃ³n dinÃ¡mica\n",
    "                filtered_img = filter_text_body_dynamic(\n",
    "                    image,\n",
    "                    size_threshold_percentile=40,\n",
    "                    merge_dist=20,\n",
    "                    expand_margin=10,\n",
    "                    debug_path=debug_path,\n",
    "                    audit_csv_path=audit_csv_path\n",
    "                )\n",
    "\n",
    "                page_text = pytesseract.image_to_string(filtered_img, lang=lang)\n",
    "                if not page_text.strip():\n",
    "                    continue\n",
    "\n",
    "                # ğŸ›‘ Stop at Anexos\n",
    "                match = re.search(r\"(?mi)^ *Anexos?\\b[\\s\\w]*:?\", page_text)\n",
    "                if match:\n",
    "                    page_text = page_text[:match.start()].strip()\n",
    "                    anexo_found = True\n",
    "                    print(f\"ğŸ›‘ 'Anexo' detected on page {page_num}. Truncating content.\")\n",
    "\n",
    "                lines = [line.strip() for line in page_text.split(\"\\n\") if line.strip()]\n",
    "                paragraph_lines = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    is_new_paragraph = (\n",
    "                        line.startswith(\"â€¢\") or line.startswith(\"â¢\") or\n",
    "                        (i > 0 and lines[i - 1].strip().endswith(\".\")) or\n",
    "                        (i > 0 and len(lines[i - 1].split()) <= 3)\n",
    "                    )\n",
    "                    if is_new_paragraph:\n",
    "                        if paragraph_lines:\n",
    "                            all_records.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"page\": page_num,\n",
    "                                \"paragraph_id\": paragraph_counter,\n",
    "                                \"text\": \" \".join(paragraph_lines).strip()\n",
    "                            })\n",
    "                            paragraph_counter += 1\n",
    "                        paragraph_lines = [line]\n",
    "                    else:\n",
    "                        paragraph_lines.append(line)\n",
    "\n",
    "                if paragraph_lines:\n",
    "                    all_records.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"page\": page_num,\n",
    "                        \"paragraph_id\": paragraph_counter,\n",
    "                        \"text\": \" \".join(paragraph_lines).strip()\n",
    "                    })\n",
    "                    paragraph_counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {filename}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No text extracted.\")\n",
    "        return df\n",
    "\n",
    "    df = df.merge(\n",
    "        metadata_df[[\"pdf_filename\", \"title\", \"doc_type\", \"doc_id\", \"year\", \"date\"]],\n",
    "        left_on=\"filename\", right_on=\"pdf_filename\", how=\"left\"\n",
    "    )\n",
    "    df = df[[\"title\", \"doc_type\", \"doc_id\", \"year\", \"date\", \"page\", \"paragraph_id\", \"text\"]]\n",
    "    print(f\"\\nâœ… OCR extraction complete. Total paragraphs: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scanned = process_scanned_pdfs(\n",
    "    folder_path=\"sc\",  # Cambia por tu carpeta de PDFs\n",
    "    metadata_csv_path=\"cf_pdfs_metadata.csv\",\n",
    "    dpi=400,\n",
    "    lang='spa',\n",
    "    debug=True\n",
    ")\n",
    "df_scanned.to_csv(\"raw_scanned_pdfs_dataset.csv\", index=False)\n",
    "print(\"ğŸ’¾ Saved as 'raw_scanned_pdfs_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d6f369",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11bf8b",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc1b583",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00801fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732a622",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenar los dos dataframes\n",
    "text_df = pd.concat([df_editable, df_scanned], ignore_index=True)\n",
    "\n",
    "# Ordenar por la columna 'date'\n",
    "text_df = text_df.sort_values(by='date')\n",
    "\n",
    "# Si deseas resetear los Ã­ndices despuÃ©s de la concatenaciÃ³n\n",
    "text_df = text_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "text_df.to_csv(\"raw_text_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f7447",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f091eb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905873",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas donde \"text\" empieza con una letra minÃºscula\n",
    "text_df[text_df[\"text\"].str.match(r\"^[a-z]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a964d",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99986295",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2a300",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4495a5e",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div id=\"1-4\"; style=\"background-color: #292929; padding: 10px; line-height: 1.5; font-family: 'PT Serif Pro Book';\">\n",
    "    <h2 style=\"text-align: left; color: #ff8575;\">\n",
    "        1.4 Noise reduction\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad352b2c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# BOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4509a5",
   "metadata": {},
   "source": [
    "\n",
    "# Testing on scanned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1 = df_editable.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb181b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_editable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1 = df_scanned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327efd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_scanned_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474c213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87487",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a35c",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_noise(df, filter_keywords=True):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # === COMPILAR PATRONES DE DIRECCIONES A ELIMINAR ===\n",
    "    pattern_direccion_completa = re.compile(\n",
    "        r\"(?:Av\\.|Jr\\.|Calle)\\s+[\\w\\sÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³Ãº]+\\d{1,5}\\s+â€”\\s+Oficina\\s+\\d{1,4}\\s+â€”\\s+[\\w\\sÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³Ãº]+.*\",\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # === PATRÃ“N DE DIRECCIÃ“N ESCANEADA O CORRUPTA (completo o incrustado) ===\n",
    "    pattern_direccion_ruido = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "            \\b(?:Av|Jr|Calle|Psj|ProlongaciÃ³n)\\.?\\s+            # VÃ­a\n",
    "            [\\wÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ‘Ã±\\s\\.\\-]{3,30}                        # Nombre de calle\n",
    "            \\s+\\d{3,6}                                          # NÃºmero\n",
    "            (?:\\s+[^\\s]{3,15})?                                 # CÃ³digo corrupto: \"OfIKi981\", etc.\n",
    "            \\s*[-â€“â€”]?\\s*\n",
    "            (?:San\\s+Isidro|Miraflores|Magdalena|Surco|Jesus\\s+MarÃ­a|Lince|Barranco)?\n",
    "            (?:\\s+e\\s+)?                                        # Posible \"e\"\n",
    "            (?:\\.pe|cf\\.gob\\.pe)?                               # Dominio\n",
    "        )\n",
    "        \"\"\",\n",
    "        flags=re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # === CONVERTIR A UNICODE SEGURO ===\n",
    "    def to_unicode(x):\n",
    "        try:\n",
    "            if isinstance(x, bytes):\n",
    "                return x.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            return str(x).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(to_unicode)\n",
    "    \n",
    "    # === ELIMINAR FILAS CON DIRECCIONES REPETITIVAS (ANTES DE LIMPIEZAS) ===\n",
    "    df_clean = df_clean[~df_clean[\"text\"].str.contains(pattern_direccion_completa)].reset_index(drop=True)\n",
    "    \n",
    "    # === LIMPIAR O ELIMINAR DIRECCIONES ESCANEADAS ===\n",
    "    def limpiar_o_eliminar_direccion(text):\n",
    "        if re.fullmatch(pattern_direccion_ruido, text.strip()):\n",
    "            return None  # Eliminar pÃ¡rrafo completo\n",
    "        return re.sub(pattern_direccion_ruido, \"\", text)  # Reemplazar si estÃ¡ embebido\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].apply(limpiar_o_eliminar_direccion)\n",
    "    df_clean = df_clean[df_clean[\"text\"].notnull() & df_clean[\"text\"].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "    # === LIMPIEZAS BÃSICAS VECTORIAL ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\b\\d{1,3}/\\d{1,3}\\b\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+([,;:.!?])\", r\"\\1\", regex=True)\n",
    "\n",
    "    # === LIMPIEZA DE URLS (normales y corruptas) ===\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"(https?://|htps://|htttp://|htpss://|www\\.|wwww\\.)[\\w\\-\\.]*\\.[a-z]{2,6}(\\/[\\w\\-\\.%]*)*\", \n",
    "        \"\", regex=True\n",
    "    )\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(htps?|htttp|htpss):\\/\\/[^\\s]*\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[â€¢â¢Ã˜*ï€ªÂ°Â¡!?Â¿\\\"]\", \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\":\\s*$\", \".\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"Lima[,]?\\s+\\d{1,2}\\s+de\\s+[a-zÃ¡Ã©Ã­Ã³Ãº]+\\s+de\\s+\\d{4}\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\"p.p.\", \"puntos porcentuales\")\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r'^\\s*((?:[ivxlcdm]+|[a-zA-Z]|\\d+)[\\.\\)]\\s*)+', '', regex=True, flags=re.IGNORECASE)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(\n",
    "        r\"\\b(?:a|al|de|del|con|por|para|y|o|en|sin|sobre|ante|tras|entre|hacia|hasta|durante|mediante|excepto|salvo|segÃºn)\\.$\",\n",
    "        lambda m: m.group(0)[:-1], regex=True)\n",
    "    \n",
    "    # === REEMPLAZAR EXPRESIONES CORRUPTAS ===\n",
    "    pattern_minist = re.compile(r\"MINIST\\s+ERIO[\\w%-Âº|\\\"â€]+|8N%IC%A\\s+Y\\s+FIN\\s+em.*?Usuaro\", flags=re.IGNORECASE)\n",
    "    pattern_garbage_symbols = re.compile(r\"[^\\w\\sÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ‘Ã±]{5,}\")\n",
    "    \n",
    "    # PatrÃ³n para capturar bloques muy corruptos (inicio o intermedios)\n",
    "    pattern_noise_block = re.compile(\n",
    "        r\"(?:[â€”\\[\\]_=\\]\\|%#@*<>~Â«Â»]{2,}|[\\w]*â€”[\\w]*|â€”\\s*[A-Z]{1}\\s*â€”)+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    \n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_minist, \"\", regex=True)\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_garbage_symbols, \"\", regex=True)\n",
    "    \n",
    "    # Aplica reemplazo a esos bloques basura\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_noise_block, \"\", regex=True)\n",
    "    \n",
    "    # PatrÃ³n para frases de cortesÃ­a y cierre\n",
    "    pattern_cortesia = re.compile(\n",
    "        r\"(aprovecho\\s+la\\s+oportunidad\\s+para\\s+expresar(le)?\\s*(a usted)?\\s*las?\\s*muestras?\\s+de\\s+mi\\s+especial\\s+consideraciÃ³n( y estima)?[\\.]?)|\"  # Variante directa\n",
    "        r\"(le\\s+reitero\\s+(las?\\s+)?muestras?\\s+de\\s+(mi\\s+)?especial\\s+consideraciÃ³n[\\.]?)|\"  # Otra variante frecuente\n",
    "        r\"(hago\\s+propicia\\s+la\\s+ocasiÃ³n\\s+para\\s+saludar(lo|la)?\\s*(muy)?\\s*atentamente[\\.]?)|\"  # Variante \"hago propicia...\"\n",
    "        r\"(sin\\s+otro\\s+particular.*?me\\s+despido[^\\n\\.]*[\\.]?)|\"  # Frase de cierre clÃ¡sica\n",
    "        r\"(con\\s+(mi\\s+)?especial\\s+consideraciÃ³n[\\.]?)\",  # Frase final tÃ­pica\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Aplicar limpieza al texto\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].str.replace(pattern_cortesia, \"\", regex=True)\n",
    "\n",
    "    # === FILTROS DE TEXTO VÃLIDO ===\n",
    "    def is_valid(text):\n",
    "        txt = text.strip()\n",
    "        letters = [c for c in txt if c.isalpha()]\n",
    "        if letters and sum(c.isupper() for c in letters) / len(letters) > 0.7:\n",
    "            return False\n",
    "        if txt.lower().startswith(\"fuente:\") or re.search(r\"\\b(Fuente:|ElaboraciÃ³n:)\", txt, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        if re.search(r\"(?:\\b\\w\\s){3,}\\w\", txt):\n",
    "            return False\n",
    "        patterns = [\n",
    "            r'PCA\\s*(Inicial|1er\\s*Trim\\.|2do\\s*Trim\\.|3er\\s*Trim\\.)',\n",
    "            r'entre\\s+\\d{4}\\sy\\s+\\d{4}\\s*,?\\s*\\d+\\s*de\\s+cada\\s+100\\s*(leyes?|insistencia|implicancia\\s*fiscal)',\n",
    "            r'â€œ[^â€]*\\(pÃ¡g\\.\\s*\\d+\\)[^â€]*â€|\\(pÃ¡g\\.\\s*\\d+\\)',\n",
    "            r\"VÃ©ase informes\\s+(NÂ°\\s*\\d{2}-\\d{4}-CF[, y]*){2,}\",  # puedes mantenerla si gustas\n",
    "            r\"\\b(VÃ©ase|Ver|Consultar|RemÃ­tase|RevÃ­sese)\\s+(el\\s+)?(informe|artÃ­culo|documento|reporte|anÃ¡lisis|dictamen|comunicado\\s+oficial|nota)\\b.*?(N[Â°Âº]?\\s*\\d{1,3})?.*?\\b(del\\s+)?\\d{4}\",\n",
    "            r\"Â¢\\s*US?|US?\\s*Â¢\", r\"Â¢\"\n",
    "        ]\n",
    "        if any(re.search(p, txt, flags=re.IGNORECASE) for p in patterns):\n",
    "            return False\n",
    "        if len(txt.split()) < 6:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_clean = df_clean[df_clean[\"text\"].apply(is_valid)].reset_index(drop=True)\n",
    "\n",
    "    # === FUSIÃ“N DE PÃRRAFOS CONSECUTIVOS ===\n",
    "    rows = []\n",
    "    i = 0\n",
    "    while i < len(df_clean):\n",
    "        current_row = df_clean.iloc[i].copy()\n",
    "        current_text = current_row[\"text\"]\n",
    "        current_title = current_row[\"title\"]\n",
    "\n",
    "        while i + 1 < len(df_clean):\n",
    "            next_row = df_clean.iloc[i + 1]\n",
    "            next_text = next_row[\"text\"]\n",
    "            next_title = next_row[\"title\"]\n",
    "\n",
    "            if next_title != current_title:\n",
    "                break\n",
    "\n",
    "            if not current_text.endswith(\".\") and next_text and next_text[0].isalpha() and next_text.endswith(\".\"):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{4}\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if re.match(r\"^[\\(,;:\\s]*[a-z\\(]\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not current_text.endswith(\".\") and re.match(r\"^\\d{1,3}(?:,\\d+)?\", next_text):\n",
    "                current_text += \" \" + next_text\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        current_row[\"text\"] = current_text\n",
    "        rows.append(current_row)\n",
    "        i += 1\n",
    "\n",
    "    df_result = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    df_result[\"paragraph_id\"] = df_result.groupby(\"title\").cumcount() + 1\n",
    "\n",
    "    # === LIMPIEZAS ADICIONALES ===\n",
    "    cleaned_rows = []\n",
    "    for idx, row in df_result.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        starts_with_special = re.match(r'^[*%\\\"\\'\\â€œ\\\\\\[\\]\\{\\}\\(\\)\\^\\-+=<>~#@|]', text)\n",
    "        starts_with_number = re.match(r\"^\\d+\\s+\", text)\n",
    "        prev_text = df_result.iloc[idx - 1][\"text\"].strip() if idx > 0 else \"\"\n",
    "        prev_ends_with_dot = prev_text.endswith(\".\")\n",
    "\n",
    "        if starts_with_special:\n",
    "            continue\n",
    "        if starts_with_number and prev_ends_with_dot:\n",
    "            continue\n",
    "\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    df_result = pd.DataFrame(cleaned_rows).reset_index(drop=True)\n",
    "\n",
    "    # Eliminar nÃºmeros al inicio del texto aunque estÃ©n pegados\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+\\s*\", \"\", regex=True)\n",
    "    df_result[\"text\"] = df_result[\"text\"].str.replace(r\"^\\d+(?=\\w)\", \"\", regex=True)\n",
    "    \n",
    "    # Eliminar observaciones con menos de 75 caracteres\n",
    "    df_result = df_result[df_result[\"text\"].str.len() >= 75].reset_index(drop=True)\n",
    "\n",
    "    # === FILTRO OPCIONAL CON KEYWORDS DE ALERTA FISCAL ===\n",
    "    if filter_keywords:\n",
    "        keywords = [\n",
    "            \"Incumplimiento de reglas fiscales\", \"PreocupaciÃ³n\", \"Advertencia\", \"Alerta\",\n",
    "            \"Riesgos fiscales\", \"DesvÃ­o del dÃ©ficit fiscal\", \"No cumplimiento\", \"Desviaciones significativas\",\n",
    "            \"Margen significativo\", \"Problema de credibilidad fiscal\", \"Credibilidad fiscal\",\n",
    "            \"Sostenibilidad fiscal\", \"ConsolidaciÃ³n fiscal\", \"Medidas correctivas\", \"RecomendaciÃ³n\",\n",
    "            \"Necesidad de tomar medidas\", \"Control del gasto pÃºblico\", \"Presiones fiscales\",\n",
    "            \"Exceso de optimismo en proyecciones\", \"Exceso de gasto\", \"Aumento de gasto\",\n",
    "            \"ReducciÃ³n de dÃ©ficit fiscal\", \"Incremento de ingresos permanentes\",\n",
    "            \"Falta de compromiso con la responsabilidad fiscal\", \"Medidas de consolidaciÃ³n\",\n",
    "            \"Deficiencia en la ejecuciÃ³n del gasto\", \"Aumento de la deuda pÃºblica\",\n",
    "            \"Iniciativas legislativas que afectan las finanzas pÃºblicas\", \"Incremento del gasto pÃºblico\",\n",
    "            \"Beneficios tributarios sin justificaciÃ³n\", \"Tratamientos tributarios preferenciales\",\n",
    "            \"ErosiÃ³n de la base tributaria\", \"ElusiÃ³n y evasiÃ³n tributaria\",\n",
    "            \"Aumento de gastos no previstos\", \"Aumento de gastos extraordinarios\",\n",
    "            \"Aumento de gastos en remuneraciones\", \"Crecimiento del gasto no financiero\",\n",
    "            \"Problema de sostenibilidad\", \"Riesgos de sostenibilidad fiscal\", \"Aumento de deuda neta\",\n",
    "            \"Desajuste fiscal\", \"Falta de transparencia en el gasto\", \"Riesgos de sobreendeudamiento\",\n",
    "            \"Excepciones a las reglas fiscales\", \"Riesgo de incumplimiento de metas fiscales\",\n",
    "            \"Aumento de los compromisos de deuda\", \"Riesgo de insolvencia\",\n",
    "            \"Falta de flexibilidad fiscal\", \"Desajuste entre el presupuesto y el MMM\",\n",
    "            \"Riesgo de incumplimiento debido a presiones de gasto\", \"ErosiÃ³n de la capacidad recaudatoria\",\n",
    "            \"Incremento de la deuda pÃºblica\", \"Falta de control de gastos extraordinarios\",\n",
    "            \"Necesidad de ajustar el gasto\", \"Inestabilidad macroeconÃ³mica\",\n",
    "            \"Problemas fiscales derivados de iniciativas legislativas\",\n",
    "            \"Riesgo de desajustes fiscales por reformas\",\n",
    "            \"Falta de capacidad de generar ingresos fiscales\", \"Riesgo de gasto excesivo\",\n",
    "            \"Incremento del gasto pÃºblico no controlado\", \"Medidas de ajuste fiscal\",\n",
    "            \"Inestabilidad presupuestaria\", \"Riesgo de inestabilidad fiscal\",\n",
    "            \"Falta de sostenibilidad de la deuda\", \"Compromiso con la disciplina fiscal\",\n",
    "            \"Necesidad de mejorar la disciplina fiscal\", \"Riesgos derivados de la crisis financiera\",\n",
    "            \"Emergencia fiscal\", \"No cumplimiento de los lÃ­mites de deuda\",\n",
    "            \"Riesgo de presiÃ³n sobre las finanzas pÃºblicas\", \"Riesgos de sostenibilidad a largo plazo\",\n",
    "            \"Inconsistencia en las proyecciones fiscales\", \"Proyecciones fiscales no realistas\",\n",
    "            \"Implicaciones fiscales de la situaciÃ³n de PetroperÃº\",\n",
    "            \"Desajuste en las proyecciones fiscales\", \"Necesidad de consolidaciÃ³n fiscal\",\n",
    "            \"Riesgos de desequilibrio fiscal\", \"Amenaza a la estabilidad fiscal\",\n",
    "            \"Inseguridad fiscal\", \"Inconsistencias fiscales\", \"Falta de previsiÃ³n en el gasto\",\n",
    "            \"Riesgo de pÃ©rdida de control fiscal\", \"Impacto fiscal no anticipado\",\n",
    "            \"PresiÃ³n de gastos adicionales\", \"Aumento en la presiÃ³n fiscal\",\n",
    "            \"ErosiÃ³n de las finanzas pÃºblicas\", \"Riesgo de dÃ©ficit fiscal no controlado\",\n",
    "            \"Aumento de la carga fiscal\", \"Riesgo de crisis fiscal\",\n",
    "            \"Propuestas legislativas que generan gasto\",\n",
    "            \"Propuestas que limitan la recaudaciÃ³n fiscal\",\n",
    "            \"Iniciativas fiscales con implicaciones negativas\",\n",
    "            \"Aumento de los gastos sociales no previstos\",\n",
    "            \"Riesgo de incumplimiento de los lÃ­mites fiscales\",\n",
    "            \"Propuestas legislativas que no cumplen con las reglas fiscales\",\n",
    "            \"Desviaciones fiscales no justificadas\",\n",
    "            \"Proyecciones de dÃ©ficit fiscal no alcanzables\",\n",
    "            \"Riesgos derivados de iniciativas legislativas excesivas\",\n",
    "            \"Crecimiento de la deuda pÃºblica sin control\",\n",
    "            \"Necesidad de polÃ­ticas fiscales mÃ¡s estrictas\"\n",
    "        ]\n",
    "        keywords_lower = [k.lower() for k in keywords]\n",
    "        df_result = df_result[\n",
    "            df_result[\"text\"].str.lower().apply(lambda txt: any(k in txt for k in keywords_lower))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alertas = clean_noise(df, filter_keywords=True)   # Solo alertas fiscales\n",
    "df_editable_cleaned = clean_noise(df_copy_editable_1, filter_keywords=False) # Todo texto Ãºtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff0fb",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_editable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_ed_2 = df_editable_cleaned[df_editable_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_ed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba3655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f97370",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1e3b6",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### The same abve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cc388",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned = clean_noise(df_copy_scanned_1, filter_keywords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129198df",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_scanned_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a48a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_sc_2 = df_scanned_cleaned[df_scanned_cleaned[\"text\"].str.len() <= 75].reset_index(drop=True)\n",
    "df_copy_sc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0b03f",
   "metadata": {},
   "source": [
    "# Limpieza global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset = pd.read_csv(\"raw_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = clean_noise(raw_text_dataset, filter_keywords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d98569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeba45",
   "metadata": {},
   "source": [
    "# Limpieza para estadÃ­sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea36f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Carga del DataFrame (si no estÃ¡ cargado) ===\n",
    "input_text_dataset = pd.read_csv(\"input_text_dataset_true.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68515fb",
   "metadata": {},
   "source": [
    "## Order by year and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_type\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_year_and_date(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'date' a datetime (si es necesario) y ordena el DataFrame por 'year' y 'date'.\n",
    "    \n",
    "    ParÃ¡metros:\n",
    "        df (pd.DataFrame): DataFrame que contiene las columnas 'year' y 'date'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame ordenado correctamente por 'year' y 'date'.\n",
    "    \"\"\"\n",
    "    required_cols = {'year', 'date'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'year' y 'date'.\")\n",
    "\n",
    "    # Convertir 'date' a datetime si no lo es ya\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"date\"]):\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors='coerce')\n",
    "\n",
    "    return df.sort_values(by=[\"year\", \"date\", \"page\", \"paragraph_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = sort_by_year_and_date(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_doc_type_from_title(df):\n",
    "    \"\"\"\n",
    "    Completa los valores vacÃ­os en la columna 'doc_type' a partir del contenido de la columna 'title'.\n",
    "    Solo extrae 'Comunicado' o 'Informe' si estÃ¡n al inicio del tÃ­tulo.\n",
    "    \n",
    "    ParÃ¡metros:\n",
    "        df (pd.DataFrame): DataFrame con las columnas 'title' y 'doc_type'.\n",
    "    \n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_type' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas requeridas existan\n",
    "    required_cols = {'title', 'doc_type'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'title' y 'doc_type'.\")\n",
    "\n",
    "    # Crear mÃ¡scara de filas donde doc_type estÃ¡ vacÃ­o\n",
    "    mask = df[\"doc_type\"].isna()\n",
    "\n",
    "    # ExpresiÃ³n regular para capturar solo \"Comunicado\" o \"Informe\" al inicio del tÃ­tulo\n",
    "    regex = r\"^(Comunicado|Informe)\\b\"\n",
    "\n",
    "    # Extraer y llenar solo en filas vacÃ­as\n",
    "    df.loc[mask, \"doc_type\"] = df.loc[mask, \"title\"].str.extract(regex)[0]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7787bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_doc_type_from_title(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd665d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = input_text_dataset[input_text_dataset[\"doc_id\"].isna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_doc_id(input_text_dataset):\n",
    "    \"\"\"\n",
    "    Llena los valores vacÃ­os en 'doc_id' con nÃºmeros flotantes secuenciales por aÃ±o, \n",
    "    basados en el orden de 'date', empezando en 1.0.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "        input_text_dataset (pd.DataFrame): DataFrame que debe contener las columnas 'doc_id', 'year', y 'date'.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El DataFrame con 'doc_id' actualizado donde era nulo.\n",
    "    \"\"\"\n",
    "    # Verificar que las columnas necesarias existan\n",
    "    required_cols = {'doc_id', 'year', 'date'}\n",
    "    if not required_cols.issubset(input_text_dataset.columns):\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'doc_id', 'year' y 'date'.\")\n",
    "\n",
    "    # Ordenar por year y date\n",
    "    df = input_text_dataset.sort_values(by=[\"year\", \"date\"]).copy()\n",
    "\n",
    "    # Crear mÃ¡scara de valores vacÃ­os en doc_id\n",
    "    mask = df[\"doc_id\"].isna()\n",
    "\n",
    "    # Para cada aÃ±o, generar un contador incremental para fechas Ãºnicas\n",
    "    def assign_ids(group):\n",
    "        # Solo para filas con doc_id nulo\n",
    "        missing = group[\"doc_id\"].isna()\n",
    "        # Contador secuencial por fecha\n",
    "        group.loc[missing, \"doc_id\"] = (\n",
    "            group.loc[missing]\n",
    "                 .groupby(\"date\", sort=False)\n",
    "                 .ngroup() + 1\n",
    "        ).astype(float)\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(assign_ids)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_dataset = fill_missing_doc_id(input_text_dataset)\n",
    "input_text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368141b",
   "metadata": {},
   "source": [
    "# TO csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV con el mismo nombre\n",
    "input_text_dataset.to_csv(\"llm_input_text_dataset_true.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20813007",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## EstadÃ­sticos de pÃ¡rrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c872a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Carga del DataFrame (si no estÃ¡ cargado) ===\n",
    "# input_text_dataset = pd.read_csv(\"input_text_dataset.csv\")\n",
    "\n",
    "# === Total de documentos Ãºnicos, por tipo ===\n",
    "total_docs = input_text_dataset[['title', 'doc_id', 'date']].apply(tuple, axis=1).nunique()\n",
    "docs_por_tipo = input_text_dataset.drop_duplicates('title')['doc_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_por_tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c849ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AÃ±os con mÃ¡s y menos opiniones ===\n",
    "opiniones_por_aÃ±o = input_text_dataset.groupby('year')['doc_id'].nunique()\n",
    "aÃ±o_mas_opiniones = opiniones_por_aÃ±o.idxmax()\n",
    "aÃ±o_menos_opiniones = opiniones_por_aÃ±o.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiniones_por_aÃ±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "aÃ±o_mas_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aÃ±o_menos_opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Documento con mÃ¡s/menos pÃ¡rrafos ===\n",
    "parrafos_por_doc = input_text_dataset.groupby('title')['paragraph_id'].nunique()\n",
    "doc_mas_parrafos = parrafos_por_doc.idxmax()\n",
    "doc_menos_parrafos = parrafos_por_doc.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mas_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_menos_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5266a",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmin()]\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"text_length\"].idxmax()]\n",
    "tamano_promedio_parrafo = input_text_dataset[\"text_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a418",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4689",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a89ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968737a",
   "metadata": {},
   "source": [
    "# palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Conteo de palabras por pÃ¡rrafo ===\n",
    "input_text_dataset[\"word_count\"] = input_text_dataset[\"text\"].str.split().str.len()\n",
    "\n",
    "# PÃ¡rrafo con menor cantidad de palabras\n",
    "parrafo_mas_pequeno = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmin()]\n",
    "\n",
    "# PÃ¡rrafo con mayor cantidad de palabras\n",
    "parrafo_mas_grande = input_text_dataset.loc[input_text_dataset[\"word_count\"].idxmax()]\n",
    "\n",
    "# TamaÃ±o promedio de pÃ¡rrafo en nÃºmero de palabras\n",
    "tamano_promedio_parrafo = input_text_dataset[\"word_count\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_mas_grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_promedio_parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8886fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d15d9a2",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "<div style=\"font-family: PT Serif Pro Book; text-align: left; color: dark; font-size: 16px;\">\n",
    "    <span style=\"font-size: 30px; color: #033280; font-weight: bold;\">\n",
    "        <a href=\"#outilne\" style=\"color: #033280; text-decoration: none;\">&#11180;</a>\n",
    "    </span> \n",
    "    <a href=\"#outilne\" style=\"color: #cd301b; text-decoration: none;\">Back to the outline.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc27b1",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9207",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python (Fiscal_Tone)",
   "language": "python",
   "name": "fiscal_tone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
