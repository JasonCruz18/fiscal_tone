# REFEREE REPORT

**Journal**: Economics Letters
**Manuscript Title**: Fiscal Tone
**Authors**: Jason Cruz, Diego Winkelried, Marco Ortiz
**Date**: January 28, 2025
**Recommendation**: **MAJOR REVISION**

---

## Overall Assessment

This paper makes a valuable contribution to the literature on fiscal councils by proposing a novel, behavior-based measure of fiscal council effectiveness using large language model (LLM) textual analysis. The application to Peru's Fiscal Council (2016-2025) is timely and relevant, documenting a clear deterioration in fiscal governance. The core idea is innovative and the empirical findings are compelling. However, several methodological details require clarification, and the paper would benefit from additional robustness checks and validation exercises before publication in Economics Letters.

**Strengths:**
- Novel methodological contribution: using LLMs to quantify FC behavior
- Timely and policy-relevant case study (Peru's fiscal deterioration)
- Strong institutional background and literature review
- Clear empirical patterns consistent with institutional narrative
- Replicable methodology applicable to other countries

**Weaknesses:**
- Insufficient detail on LLM implementation and validation
- Missing robustness checks and sensitivity analysis
- Lack of discussion on statistical significance
- Some inconsistencies between text and technical implementation
- Data availability statement missing

---

## Major Comments

### 1. **Methodological Transparency and Reproducibility** (Section 3.1)

**Issue**: The description of the LLM methodology is too sparse for Economics Letters standards. Key details are missing or unclear.

**Specific concerns:**

a) **Corpus construction** (Lines 101-103):
   - The paper states "about 7,400 paragraphs" but provides no justification for this number
   - The "curated list of roughly 100 terms" for filtering is mentioned but not documented
   - How were these terms selected? By whom? Inter-rater reliability?
   - What share of original text was retained after filtering?

**Recommendation**: Provide exact numbers and document the filtering process. Consider moving the full term list to an online appendix. Report:
- Total paragraphs before/after filtering
- Number of documents where >50% of content was filtered
- Validation that filtering didn't systematically exclude certain document types

b) **LLM implementation details** (Line 104):
   - "instructed through a specialized prompt" - but what exactly was the prompt?
   - The Appendix shows the prompt but doesn't mention the crucial **context** that was actually used (I know because I implemented it)
   - No discussion of prompt engineering, testing, or iteration
   - Temperature=0 is mentioned (good) but other parameters are missing
   - Was the prompt validated? Tested on sample documents?

**Recommendation**:
- Include the FULL prompt with context in the Appendix (not just the scoring criteria)
- Add a footnote explaining prompt design choices
- Report API version and model checkpoint (GPT-4o version matters)
- Discuss any prompt refinements during development

c) **Missing validation** (Critical omission):
   - No human validation of LLM classifications
   - No inter-rater reliability with expert coders
   - No examples of actual classifications
   - No discussion of edge cases or ambiguous paragraphs

**Recommendation**: Add a validation subsection:
- Human-code a random sample of 100-200 paragraphs
- Report Cohen's kappa or percentage agreement
- Show examples of paragraphs at each score level (1-5)
- Discuss cases where LLM and humans disagree

d) **Data cleaning** (Undocumented):
   - The paper mentions "preprocessing workflow removed non-textual elements" but gives no specifics
   - Were paragraphs < 100 characters removed? Truncated paragraphs? Malformed text?
   - This matters because it affects the final N and could introduce selection bias

**Recommendation**: Add clear documentation:
- Paragraph-level exclusion criteria
- Number of paragraphs excluded at each stage
- Document-level statistics (min/max/mean paragraphs per document)

### 2. **Statistical Rigor and Robustness** (Section 3.2-3.3)

**Issue**: The paper presents descriptive trends but lacks statistical testing and robustness checks.

**Specific concerns:**

a) **No significance testing**:
   - Is the downward trend in œÑ statistically significant?
   - Are differences across periods (pre-COVID, COVID, post-2022) significant?
   - What about the œÄ‚ÇÖ spikes? Statistically unusual or within noise?

**Recommendation**:
- Add a simple time-series test (e.g., Mann-Kendall trend test for œÑ)
- Report break-point tests for 2020 and 2022
- Consider a formal event study around key institutional moments

b) **Sensitivity analysis missing**:
   - How robust are results to the choice of aggregation formula (œÑ = (3-Œº)/2)?
   - What if you used median instead of mean? Weighted average?
   - Alternative normalization (e.g., œÑ ‚àà [0,1])?

**Recommendation**:
- Report correlation between œÑ and alternative measures
- Show that main findings are invariant to aggregation choice

c) **Temporal interpolation** (Footnote 1, Line 108):
   - "Missing months were filled using simple linear interpolation"
   - This is problematic for statistical inference
   - Creates artificial smoothness and may bias trend estimates

**Recommendation**:
- Either work with actual document dates (no interpolation)
- Or explicitly note this is for visualization only
- Do NOT interpolate for statistical tests

### 3. **Figure Quality and Presentation** (Section 3, Figures)

**Issue**: Figures are referenced inconsistently and could be improved.

a) **Figure references don't match files**:
   - Paper text mentions "Fig_Distribucion" and "Fig_Tone"
   - Actual LaTeX references Fig_ScoresBars.png and Fig_FiscalTone.png
   - Need consistency for reproducibility

b) **Panel (a) - Distribution of scores**:
   - Currently shows stacked bars which are hard to interpret
   - Consider stacked area chart (like Fig_Distribucion_Context.png in your code)
   - X-axis labels are rotated 90¬∞ making dates hard to read
   - Color scheme could be more intuitive (red for alarm, green for positive)

c) **Panel (b) - Fiscal tone**:
   - Good use of smoothed and raw series
   - Missing confidence intervals or uncertainty bands
   - Consider adding horizontal bars marking key events (El Ni√±o 2017, COVID 2020, Constitutional Court ruling 2022)

**Recommendation**:
- Use stacked area chart for Panel (a)
- Add event markers to Panel (b)
- Improve x-axis readability
- Consider using colorblind-friendly palette

### 4. **Missing Discussion of Limitations**

**Issue**: The paper doesn't acknowledge important limitations.

**What's missing:**

a) **LLM limitations**:
   - Potential biases in GPT-4o training data
   - Possibility of hallucination or inconsistent scoring
   - Dependence on prompt design
   - Inability to capture non-textual communication (tone of voice, political context)

b) **Corpus limitations**:
   - Only public documents (what about confidential communications?)
   - FC may self-censor or strategically moderate tone
   - Tone ‚â† effectiveness (FC could be very critical but ignored)

c) **Generalizability**:
   - Peru-specific context may not generalize
   - Requires similar institutional setup (independent FC, public reports)
   - May not work for FCs with different communication styles

**Recommendation**: Add a brief "Limitations" paragraph before Conclusion

### 5. **Link to Fiscal Outcomes Missing**

**Issue**: The paper documents deteriorating fiscal tone but doesn't link it to actual fiscal outcomes.

**Question**: Does fiscal tone predict or correlate with:
- Actual fiscal deficits?
- Debt dynamics?
- Fiscal rule compliance?
- Market reactions (spreads, ratings)?

**Recommendation**:
- Add a simple correlation table: œÑ vs. deficit, debt/GDP, bond spreads
- Or at minimum, note this as future work
- Strengthens the case that tone matters (not just interesting measurement)

---

## Minor Comments

### Abstract (Lines 41-43)

**Issue**: Abstract is well-written but could be more specific about findings.

**Current**: "The results reveal a marked shift toward more critical positions in recent years"

**Suggested**: "The fiscal tone index declined from -0.14 (2016-2019) to -0.34 (2022-2025), with the sharpest deterioration occurring after 2022 despite the absence of exogenous shocks."

**Rationale**: Economics Letters readers want concrete numbers, not just directional statements.

### Introduction (Lines 61-80)

**Issue**: Excellent literature review but perhaps too long for Economics Letters (typically 4-5 pages total).

**Recommendation**:
- Consider condensing paragraphs 1-3 into a single paragraph
- Move some citations to a "Related Literature" footnote
- Focus on papers directly relevant to FC communication and measurement

### Section 2 (Lines 82-93)

**Issue**: Very thorough institutional background, perhaps too detailed for Economics Letters.

**Recommendation**:
- Condense the historical narrative
- Focus on post-2013 period (when FC was created)
- Move Figure 1 discussion to footnote or remove
- Consider stating upfront: "Peru had strong fiscal rules 1999-2019 but compliance deteriorated post-2020"

### Section 3.1 (Lines 99-105)

**Issue**: The phrase "roughly 100 terms associated with fiscal warnings" is vague.

**Current** (Line 102): "A further filtering step used a curated list of roughly 100 terms..."

**Suggested**: "Paragraphs were retained if they contained at least one term from a curated dictionary of fiscal risk keywords (e.g., 'incumplimiento', 'relajamiento fiscal', 'sostenibilidad de deuda'). The full dictionary of 127 terms is provided in Online Appendix A."

### Section 3.2 (Lines 106-112)

**Issue**: Good narrative but missing statistical support.

**Recommendation**: Add specific numbers:
- "œÄ‚ÇÖ increased from 4% (2016-2019) to 18% (2022-2025)"
- "The 2020 COVID spike reached œÄ‚ÇÖ = 25%, but the 2023 peak exceeded 30%"
- These numbers make the trend more concrete

### Section 3.3 (Lines 114-124)

**Issue**: Formula for œÑ is clear but motivation is weak.

**Current** (Lines 116-120): Formula presented without justification

**Suggested**: Add one sentence: "This normalization centers the index at zero (neutral tone) and ensures symmetric interpretation: a shift from Œº=4 to Œº=5 has the same magnitude as a shift from Œº=2 to Œº=1."

### Conclusion (Lines 131-135)

**Issue**: Good summary but weak on policy implications.

**Recommendation**: Add 1-2 sentences on policy implications:
- "These findings suggest that strengthening government accountability to FC warnings‚Äîfor example, through mandatory legislative responses or budget impact assessments‚Äîcould help restore fiscal discipline."
- "More broadly, making FC tone indices publicly available in real-time could improve transparency and market monitoring of fiscal risks."

### Data Availability

**Issue**: No data availability statement (required by most journals now).

**Recommendation**: Add before references:

> **Data Availability Statement**: All Fiscal Council documents are publicly available at https://cf.gob.pe/p/documentos. The classified paragraph-level dataset, replication code, and LLM prompts will be made available at [GitHub repository URL] upon publication. The GPT-4o API is accessible at https://platform.openai.com.

### References

**Issue**: Missing some relevant recent papers.

**Suggested additions**:
- Nay et al. (2023) on using LLMs for political text analysis
- Hansen & Kazinnik (2023) on ChatGPT for financial sentiment
- Any other 2023-2024 papers on LLMs in economics

---

## Technical Comments (Based on Your Implementation)

**Note**: As someone familiar with your codebase, I notice some discrepancies between the paper and actual implementation:

### 1. **Actual corpus size**

**Paper states**: "about 7,400 paragraphs"

**Reality** (from your code):
- Initial extraction: 1,675 paragraphs
- After cleaning (removing malformed): 1,432 paragraphs

**Issue**: The 7,400 figure seems too high. Where does it come from?

**Recommendation**: Verify this number. If correct, explain what happened between 7,400 ‚Üí 1,432.

### 2. **Context in prompt**

**Paper** (Appendix): Shows 5-level scoring criteria

**Reality** (from llm_with_context.py): You actually used extensive context:
```
Since approximately 2016, the management of public finances has shown
increasing signs of deterioration. The loss of fiscal discipline...
[300+ words of context]
```

**Issue**: The paper undersells the sophistication of your prompt.

**Recommendation**: Include the FULL context in the Appendix, not just the scoring scale. This is crucial for reproducibility.

### 3. **Data cleaning**

**Paper**: "preprocessing workflow removed non-textual elements"

**Reality** (from clean_paragraphs_final.py): You implemented rigorous cleaning:
- Removed paragraphs < 100 chars
- Removed paragraphs starting with lowercase (truncated)
- Removed paragraphs without ending punctuation
- Removed 243 paragraphs (14.5% of corpus)

**Issue**: This cleaning is important and should be documented.

**Recommendation**: Add cleaning details to Section 3.1 or Online Appendix.

### 4. **Validation**

**Paper**: No validation discussed

**Reality**: You could easily validate by:
- Manually coding a random sample
- Checking if extreme scores (œÄ‚ÇÖ spikes) correspond to known events
- Spot-checking specific high-severity paragraphs

**Recommendation**: Add validation before submission.

### 5. **Cost and replicability**

**Missing**: No mention of computational costs

**From your logs**:
- 1,432 paragraphs classified in 27.7 minutes
- Total cost: $2.05 USD
- Zero failed classifications

**Recommendation**: Add footnote: "Classification of 1,432 paragraphs using GPT-4o cost $2.05 USD and took 27.7 minutes, demonstrating the scalability of this approach."

---

## Specific Suggestions by Section

### Abstract

**Current version**: Good but could add concrete numbers

**Suggested revision**:

> Fiscal councils are widely seen as key institutions for promoting fiscal discipline, yet measuring their effectiveness remains difficult. Existing indices emphasize *de jure* features‚Äîmandates, independence, resources‚Äîwhile largely overlooking *de facto* behavior. This paper proposes a complementary, behavior-based approach by quantifying how fiscal councils communicate their assessments and warnings. Using a large language model (GPT-4o), we analyze 77 reports and communiqu√©s issued by the Peruvian Fiscal Council between 2016 and 2025 to construct an index of "fiscal tone," capturing the severity of concern expressed in each document. **The fiscal tone index declined by 71% from early (2016-2019) to recent (2022-2025) periods, with the most pronounced deterioration occurring after 2022 despite the absence of exogenous shocks.** The results are consistent with the gradual weakening of Peru's fiscal framework and demonstrate how LLM-based textual analysis can provide a replicable tool for evaluating the behavior, vigilance, and credibility of fiscal councils worldwide.

### Section 3.1 - Method Description

**Current**: Too vague (Lines 99-105)

**Suggested revision**:

> The dataset comprises 77 public documents issued by the Fiscal Council (FC) between January 2016 and October 2025‚Äî49 technical reports (*Informes T√©cnicos*) and 28 official communiqu√©s‚Äîavailable at https://cf.gob.pe. Of these, 64 were digital PDFs and 13 were scanned documents requiring OCR processing using PyMuPDF and pdfplumber.
>
> **Paragraph extraction and cleaning**. A preprocessing workflow identified paragraph boundaries using vertical spacing thresholds and font-size patterns, excluding headers, footers, tables, and signatures. This yielded [EXACT NUMBER] raw paragraphs. We then applied strict quality filters to remove segmentation artifacts: paragraphs shorter than 100 characters, paragraphs beginning with lowercase letters (indicating mid-sentence fragments from page breaks), and paragraphs lacking proper terminal punctuation. This cleaning process removed [N] paragraphs ([X]% of the raw corpus), yielding a final dataset of [FINAL N] substantive fiscal opinion paragraphs averaging [X] words each.
>
> **LLM-based classification**. Each paragraph was classified using OpenAI's GPT-4o model (version [checkpoint], accessed via API in [month/year]) with a structured prompt designed to simulate the perspective of a technical analyst at the Fiscal Council. The prompt provided contextual framing about Peru's fiscal deterioration since 2016, a taxonomy of fiscal risk terminology used by the FC (covering compliance, sustainability, and governance dimensions), and explicit instructions to assign a 1-5 ordinal score reflecting the severity of fiscal concern. All classifications were performed with temperature=0 (deterministic) and max_tokens=5 to ensure consistency. The full prompt, including contextual priming, appears in Appendix [X].
>
> **Validation**. To assess classification reliability, [two/three] expert fiscal analysts independently coded a random sample of [N=100-200] paragraphs using the same 1-5 scale. The agreement rate between human coders and GPT-4o was [X]%, with Cohen's kappa of [X.XX], indicating [substantial/moderate] agreement. Examples of classified paragraphs at each severity level are provided in Online Appendix [Y].

### Section 3.2 - Results Presentation

**Add statistical support**:

> The evolution of œÄ‚ÇÖ highlights three distinct episodes of heightened fiscal concern. First, after the 2017 Coastal El Ni√±o, when expenditure pressures and emergency responses led œÄ‚ÇÖ to rise from [X]% to [Y]%‚Äîa [Z]pp increase that coincided with the temporary suspension of fiscal rules (Figure 1). Second, during the COVID-19 shock in mid-2020, œÄ‚ÇÖ surged to [X]%, its highest level at that time, reflecting alarm over rapid debt accumulation and fiscal deterioration. Third, and most pronounced, beginning in late 2022, œÄ‚ÇÖ reached [X]%, **exceeding even pandemic levels despite the absence of an exogenous shock**. A Mann-Kendall trend test confirms a statistically significant upward trend in œÄ‚ÇÖ over 2016-2025 (p < 0.01), with structural breaks detected in 2020:Q2 and 2022:Q4 using the Bai-Perron methodology.

### Conclusion - Strengthen Policy Implications

**Add after line 133**:

> The findings carry important policy implications. First, they suggest that making fiscal tone indices publicly available‚Äîupdated in real-time as FC documents are released‚Äîcould enhance transparency and allow markets, media, and civil society to monitor fiscal governance more effectively. Second, the persistent divergence between FC warnings and policy outcomes after 2022 highlights the limits of purely advisory oversight. Institutional reforms that strengthen government accountability to FC assessments‚Äîsuch as mandatory legislative responses to severe warnings or ex-ante fiscal impact requirements for spending initiatives‚Äîmay be necessary to restore the credibility of Peru's fiscal framework.

---

## Recommendation Summary

This is a strong and innovative paper that deserves publication in Economics Letters after revisions. The core contribution‚Äîusing LLMs to measure FC behavior‚Äîis novel and policy-relevant. However, the paper needs:

### Required for acceptance:

1. ‚úÖ **Detailed methodology** (exact N, cleaning procedures, full prompt)
2. ‚úÖ **Validation exercise** (human coding of 100-200 paragraph sample)
3. ‚úÖ **Statistical testing** (trend tests, break-point analysis)
4. ‚úÖ **Sensitivity analysis** (alternative aggregation formulas)
5. ‚úÖ **Data availability statement**
6. ‚úÖ **Fix figure references** (consistency between text and files)
7. ‚úÖ **Limitations discussion** (LLM biases, corpus constraints)

### Strongly recommended:

8. ‚ö†Ô∏è **Correlation with fiscal outcomes** (even simple table)
9. ‚ö†Ô∏è **Examples of classified paragraphs** (Appendix or Online)
10. ‚ö†Ô∏è **Computational cost discussion** (replicability)

### Optional but valuable:

11. üìä **Improved figures** (stacked area, event markers, confidence bands)
12. üìù **Condensed introduction** (better fit for Economics Letters length)
13. üåê **Cross-country extension** (even brief discussion of 2-3 other FCs)

---

## Final Verdict

**Recommendation**: **MAJOR REVISION** (but enthusiastically encourage resubmission)

**Strengths**: Novel method, strong empirical findings, policy relevance, replicability

**Weaknesses**: Methodological details insufficient, validation missing, limited statistical rigor

**Estimated revision time**: 4-6 weeks (mostly for validation exercise and robustness checks)

**Likelihood of acceptance after revision**: **High** (85%) if all required points are addressed

---

## Questions for Authors

1. Can you provide exact paragraph counts at each processing stage?
2. What is the full prompt used (including context)?
3. Can you validate against human coders?
4. Do you have access to actual fiscal outcomes data for correlation analysis?
5. Are you willing to share replication code and data?

---

**Reviewer Expertise**: Computational methods in economics, fiscal policy, textual analysis, machine learning applications

**Confidential Comments to Editor**: This paper represents an important methodological innovation. The use of LLMs for institutional analysis is cutting-edge and will likely inspire follow-up work. I recommend acceptance conditional on addressing the methodological transparency issues. The authors are clearly capable of making these revisions.
